<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MongoDB 副本集与分片]]></title>
    <url>%2F2020%2F09%2F16%2FMongoDB-%E5%89%AF%E6%9C%AC%E9%9B%86%E4%B8%8E%E5%88%86%E7%89%87%2F</url>
    <content type="text"><![CDATA[MongoDB 副本集与分片副本集 (Replica sets) 提供数据的冗余备份, 在多个服务器上存储数据副本, 提高了数据的可用性， 并可以保证数据的安全性 副本集包含多个数据节点和一个仲裁节点（可选） 在数据节点中，只有一个成员被视为主要节点 (primary)，而其他节点则被视为次要节点 (secondary) 仲裁者 (Arbiter) 不保存数据, 当 primary 不可用时投票选举切换 primary 一个副本集最多可以有 50 个成员，但只有 7 个投票成员 选举时避免出现平票的情况，部署一般采用基数节点 Primary 主节点是副本集中接收写操作的唯一成员 MongoDB 在主数据库上应用写操作，然后在主数据库的操作日志 (oplog) 上记录该操作 次要成员复制此日志，并将操作应用于其数据集 副本集的所有成员都可以接受读取操作 默认情况下，应用程序将其读取操作定向到 Primary Secondary 次要节点数据的复制过程是异步的 Arbiter 仲裁节点使用最小的资源并且不要求硬件设备 不能将 Arbiter 部署在同一个数据集节点中 分片 (Sharding) 分片集群由以下三个组件构成 mongos 数据库集群请求的入口，负责把对应的数据请求请求转发到对应的 shard 服务器上 在生产环境通常有多 mongos 作为请求的入口，防止其中一个挂掉所有的 mongodb 请求都没有办法操作 config server 存储所有数据库元信息（路由、分片）的配置 mongos 本身没有物理存储分片服务器和数据路由信息，只是缓存在内存里，配置服务器则实际存储这些数据 MongoDB 3.4 开始必须将配置服务器部署为副本集 (CSRS, Config Servers Replica Set)，防止数据丢失 shard 分片是指将数据库拆分，将其分散在不同的机器上的过程，每个分片是整体数据的子集，且都可以部署为副本集 MongoDB 3.6 开始必须将分片部署为副本集 将数据分散到不同的机器上，不需要功能强大的服务器就可以存储更多的数据和处理更大的负载 基本思想就是将集合切成小块，这些块分散到若干片里，每个片只负责总数据的一部分，最后通过一个均衡器来对各个分片进行均衡（数据迁移） 如果集合没有分片，那么该集合数据都存储在数据库的 Primary Shard 中 分片机制分片键 (Shared Key) MongoDB 通过定义分片键从而对整个集合进行分片，分片键的好坏直接影响到整个集群的性能 一个集合只有且只能有一个分片键，一旦分片键确定好之后就不能更改 分片方式 基于范围 优势：分片键范围查询性能较好，读性能较好 劣势：数据分布可能不均匀，存在热点 基于 Hash 优势：数据分布均匀，写性能较好，适用于日志、物联网等高并发场景 劣势：范围查询效率较低 基于 zone/tag 若数据具备一些天然的区分，如基于地域、时间等标签，数据可以基于标签来做区分 优势：数据分布较为合理 分片键的选择, 选择合适的片键对 sharding 效率影响很大 取值基数 取值基数建议尽可能大，如果用小基数的片键，因为备选值有限，那么块的总数量就有限，随着数据增多，块的大小会越来越大，导致水平扩展时移动块会非常困难 取值分布 取值分布建议尽量均匀，分布不均匀的片键会造成某些块的数据量非常大，同样有上面数据分布不均匀，性能瓶颈的问题 查询带分片 查询时建议带上分片，使用分片键进行条件查询时，mongos 可以直接定位到具体分片，否则 mongos 需要将查询分发到所有分片，再等待响应返回 避免单调底层或递减 单调递增的 sharding key，数据文件挪动小，但写入会集中，导致最后一篇的数据量持续增大，不断发生迁移，递减同理 Chunk 块 chunk（块）是均衡器迁移数据的最小单元，默认大小为 64MB，取值范围为 1-1024MB 一个 chunk 只存在于一个分片，每个 chunk 由片键特定范围内的文档组成，范围为 [start,end) 一个文档属于且只属于一个 chunk，当一个 chunk 增加到特定大小的时候，会通过拆分点（split point）被拆分成 2 个较小的块 在有些情况下，chunk 会持续增长，超过 ChunkSize，官方称为 jumbo chunk，该块无法被拆分和迁移，所以会导致 chunk 在分片服务器上分布不均匀，从而成为性能瓶颈 Chunk 的拆分 mongos 会记录每个块中有多少数据，一旦达到了阈值就会检查是否需要对其进行拆分，如果确实需要拆分则可以在配置服务器上更新这个块的相关元信息 chunk 的拆分过程如下： mongos 接收到客户端发起的写请求后会检查当前块的拆分阈值点 如果需要拆分，mongos 则会像分片服务器发起一个拆分请求 分片服务器会做拆分工作，然后将信息返回 mongos Chunk 的迁移 均衡器进程发送 moveChunk 命令到源分片 源分片使用内部 moveChunk 命令，在迁移过程，对该块的操作还是会路由到源分片 目标分片构建索引 目标分片开始进行数据复制 复制完成后会同步在迁移过程中该块的更改 同步完成后源分片会连接到配置服务器，使用块的新位置更新集群元数据 源分片完成元数据更新后，一旦块上没有打开的游标，源分片将删除其文档副本 迁移过程可确保一致性，并在平衡期间最大化块的可用性 迁移的阈值 对于数据的不均衡是根据两个分片上的 Chunk 个数差异来判定的，阈值对应表如下： Number of Chunks Migration Threshold Fewer than 20 2 20-79 4 80 and greater 8 修改 Chunk Size 的注意事项 chunk 的自动拆分操作仅发生在插入或更新的时候。 如果减少 chunk size，将会耗费一些时间将原有的 chunk 拆分到新 chunk，并且此操作不可逆 如果新增 chunk size，已存在的 chunk 只会等到新的插入或更新操作将其扩充至新的大小 chunk size 的可调整范围为 1-1024MB Balancer 均衡器 MongoDB 的 balancer（均衡器）是监视每个分片的 chunk 数的一个后台进程 当分片上的 chunk 数达到特定迁移阈值时，均衡器会尝试在分片之间自动迁移块，使得每个分片的块的数量达到平衡 分片群集的平衡过程对用户和应用程序层完全透明，但在执行过程时可能会对性能产生一些影响 从 MongoDB 3.4 开始，balancer 在配置服务器副本集（CSRS）的主服务器上运行 在 3.4 版本中，当平衡器进程处于活动状态时，主配置服务器的的 locks 集合通过修改 _id: &quot;balancer&quot; 文档会获取一个 balancer lock，该 balancer lock 不会被释放，是为了保证只有一个 mongos 实例能够在分片集群中执行管理任务 从 3.6 版本开始，均衡器不再需要 balancer lock 均衡器可以动态的开启和关闭，也可以针对指定的集合开启和关闭，还可以手动控制均衡器迁移 chunk 的时间，避免在业务高峰期的时候迁移 chunk 从而影响集群性能 事务 MongoDB 很早就有事务的概念，但是这个事务只能是针对单文档的，即单个文档的操作是有原子性保证的 在 4.0 版本之后，MongoDB 开始支持多文档的事务： 4.0 版本支持副本集范围的多文档事务 4.2 版本支持跨分片的多文档事务 (基于两阶段提交) 在事务的隔离性上，MongoDB 支持快照(snapshot)的隔离级别，可以避免脏读、不可重复读和幻读 尽管有了真正意义上的事务功能，但多文档事务对于性能有一定的影响，应用应该在充分评估后再做选用 相关操作 (Mongo shell)添加分片键12345678910111213141516171819202122232425262728293031323334353637383940mongos&gt; sh.enableSharding("test");&#123; "ok" : 1, "operationTime" : Timestamp(1556535000, 8), "$clusterTime" : &#123; "clusterTime" : Timestamp(1556535000, 8), "signature" : &#123; "hash" : BinData(0,"84KefOzN8tKmsPfr6IrnBUxF9NM="), "keyId" : NumberLong("6685242219722440730") &#125; &#125;&#125;mongos&gt; sh.shardCollection("test.testcoll", &#123;"myfield": 1&#125;);&#123; "collectionsharded" : "test.testcoll", "collectionUUID" : UUID("68ff9452-40bb-41a2-b35a-405132f90cd3"), "ok" : 1, "operationTime" : Timestamp(1556535010, 8), "$clusterTime" : &#123; "clusterTime" : Timestamp(1556535010, 8), "signature" : &#123; "hash" : BinData(0,"IgVzMa8qE4UBzjc2gOZJX5kZ3T4="), "keyId" : NumberLong("6685242219722440730") &#125; &#125;&#125;mongos&gt; use test;switched to db testmongos&gt; db.testcoll.insert(&#123;"myfield": "a", "otherfield": "b"&#125;);WriteResult(&#123; "nInserted" : 1 &#125;)mongos&gt; db.testcoll.insert(&#123;"myfield": "c", "otherfield": "d", "kube" : "db" &#125;);WriteResult(&#123; "nInserted" : 1 &#125;)mongos&gt; db.testcoll.find();&#123; "_id" : ObjectId("5cc6d6f656a9ddd30be2c12a"), "myfield" : "a", "otherfield" : "b" &#125;&#123; "_id" : ObjectId("5cc6d71e56a9ddd30be2c12b"), "myfield" : "c", "otherfield" : "d", "kube" : "db" &#125; 这里设置 test 数据库的 testcoll 表需要分片，根据 myfield 自动分片 查看集群状态 sh.status()123456789101112131415161718192021222324252627282930313233343536373839mongos&gt; sh.status();--- Sharding Status --- sharding version: &#123; "_id" : 1, "minCompatibleVersion" : 5, "currentVersion" : 6, "clusterId" : ObjectId("5cc6c061f439d076e04d737b") &#125; shards: &#123; "_id" : "shard0", "host" : "shard0/mongo-sh-shard0-0.mongo-sh-shard0-gvr.demo.svc.cluster.local:27017,mongo-sh-shard0-1.mongo-sh-shard0-gvr.demo.svc.cluster.local:27017,mongo-sh-shard0-2.mongo-sh-shard0-gvr.demo.svc.cluster.local:27017", "state" : 1 &#125; &#123; "_id" : "shard1", "host" : "shard1/mongo-sh-shard1-0.mongo-sh-shard1-gvr.demo.svc.cluster.local:27017,mongo-sh-shard1-1.mongo-sh-shard1-gvr.demo.svc.cluster.local:27017,mongo-sh-shard1-2.mongo-sh-shard1-gvr.demo.svc.cluster.local:27017", "state" : 1 &#125; &#123; "_id" : "shard2", "host" : "shard2/mongo-sh-shard2-0.mongo-sh-shard2-gvr.demo.svc.cluster.local:27017,mongo-sh-shard2-1.mongo-sh-shard2-gvr.demo.svc.cluster.local:27017,mongo-sh-shard2-2.mongo-sh-shard2-gvr.demo.svc.cluster.local:27017", "state" : 1 &#125; active mongoses: "3.6.12" : 2 autosplit: Currently enabled: yes balancer: Currently enabled: yes Currently running: no Failed balancer rounds in last 5 attempts: 0 Migration Results for the last 24 hours: No recent migrations databases: &#123; "_id" : "config", "primary" : "config", "partitioned" : true &#125; config.system.sessions shard key: &#123; "_id" : 1 &#125; unique: false balancing: true chunks: shard0 1 &#123; "_id" : &#123; "$minKey" : 1 &#125; &#125; --&gt;&gt; &#123; "_id" : &#123; "$maxKey" : 1 &#125; &#125; on : shard0 Timestamp(1, 0) &#123; "_id" : "test", "primary" : "shard1", "partitioned" : true &#125; test.testcoll shard key: &#123; "myfield" : 1 &#125; unique: false balancing: true chunks: shard1 1 &#123; "myfield" : &#123; "$minKey" : 1 &#125; &#125; --&gt;&gt; &#123; "myfield" : &#123; "$maxKey" : 1 &#125; &#125; on : shard1 Timestamp(1, 0) 删除片键12345678910mongos&gt; db.collections.remove(&#123;_id:"test.testcoll"&#125;)WriteResult(&#123; "nRemoved" : 1 &#125;)mongos&gt; db.chunks.remove(&#123;ns:"test.testcoll"&#125;)WriteResult(&#123; "nRemoved" : 38 &#125;)mongos&gt; db.locks.remove(&#123;_id:"test.testcoll"&#125;)WriteResult(&#123; "nRemoved" : 1 &#125;)mongos&gt; use adminswitched to db adminmongos&gt; db.adminCommand("flushRouterConfig") ##刷新路由配置&#123; "flushed" : true, "ok" : 1 &#125; 查看 mongo 集群是否开启了 balance12mongos&gt; sh.getBalancerState()true 也可通过执行 sh.status() 查看 balance 状态。 查看是否正在有数据的迁移12mongos&gt; sh.isBalancerRunning()false 设置 balance 窗口 将均衡器的迁移 chunk 时间控制在凌晨 02 点至凌晨 06 点： 123456use configdb.settings.update( &#123; _id: "balancer" &#125;, &#123; $set: &#123; activeWindow : &#123; start : "02:00", stop : "06:00" &#125; &#125; &#125;, &#123; upsert: true &#125;) 删除 balance 窗口： 12use configdb.settings.update(&#123; _id : "balancer" &#125;, &#123; $unset : &#123; activeWindow : true &#125; &#125;) 关闭 balance 默认 balance 的运行可以在任何时间，迁移只需要迁移的 chunk，如需关闭 balance，可执行下列命令： 12sh.stopBalancer()sh.getBalancerState() 停止 balace 后，查看是否有迁移进程正在执行，可执行下列命令： 12345use configwhile( sh.isBalancerRunning() ) &#123; print("waiting..."); sleep(1000);&#125; 打开 balance 如您需要准备重新打开 balance，可执行下列命令： 1sh.setBalancerState(true) 当驱动版本不支持 sh.startBalancer() 时，可执行下列命令来重新打开 balance： 12use configdb.settings.update( &#123; _id: "balancer" &#125;, &#123; $set : &#123; stopped: false &#125; &#125; , &#123; upsert: true &#125; ) 集合的 balance 关闭某个集合的 balance： 1sh.disableBalancing("students.grades") 打开某个集合的 balance： 1sh.enableBalancing("students.grades") 查看某个集合是否开启了 balance： 1db.getSiblingDB("config").collections.findOne(&#123;_id : "students.grades"&#125;).noBalance 相关问题 分片建的值不允许更新 在有分片键的存在的集合中,必须根据分片键或者是主键这种 mongos 可以确定唯一的方式来进行单条更新 如果设置 {upsert:true} 在进行单条更新的时候, 必须根据分片键进行更新 因为如果没有 shard key，mongos 既不能在所有 shard 实例上执行这条语句 (可能会导致每个shard都插入数据), 也无法选择在某个 shard 上执行这条语句 虽然 mongos 知道 _id 是唯一的, 但是 _id 不是分片键, mongos 不清楚 _id 落在哪个分片上 可以使用批量更新操作解决此问题]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 并发编程实战 笔记]]></title>
    <url>%2F2020%2F08%2F14%2FJava-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98-%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Java 并发编程实战 笔记二. 线程安全2.2 原子性 原子操作指不可分割的操作 竞态条件 (Race Condition) 当某个计算的正确性取决于多个线程交替执行的时序时, 那就会发生竞态条件 最常见的竞态条件类型就是”先检查后执行“ 例如在延迟加载的实现中, 需要先判断对象是否已初始化, 否则需要先进行初始化 两个线程同时进入时可能会造成初始化两次 可以使用 JUC 包中的 Atomic 类保证变量状态的原子性 2.3 加锁机制 Atomic 类并不能保证竞态条件下的线程安全 内置锁 synchronized 可重入 获取锁的操作粒度是”线程”, 而不是”调用” 避免了子类重写父类 synchronized 方法时调用父类方法会死锁的情况 2.4 用锁来保护状态 对于被多线程访问的变量, 在访问它时都需要持有同一个锁 2.5 活跃性与性能 要判断同步代码块的合理大小, 需要在安全性(必须满足), 简单性和性能等需求之间进行权衡 三. 对象的共享3.1 可见性 多线程下存在可见性和重排序的情况 3.1.1 失效数据 在每次访问变量时都使用同步可以避免失效数据 3.1.2 非原子的 64 为操作 在 32 位操作系统下, JVM 允许将 64 位数值 (double, long) 的读写操作分解为两个 32 位的操作 3.1.3 加锁与可见性 加锁的含义不仅仅局限于互斥行为, 还包括内存可见性 3.1.4 volatile 变量 volatile 可以确保变量的更新通知到其他线程 并不能保证原子性 3.2 发布与逸出 发布一个对象是指使其能够在当前作用于之外的代码中使用 某个不该被发布的对象被发布时, 被称为逸出 避免在构造函数中将还未完成初始化的对象 this 指针逸出 3.3 线程封闭 仅在单线程内访问数据, 被称为线程封闭 (Thread Confinement) 栈封闭指只能通过局部变量才能访问对象, 因为局部变量位于执行线程的栈中 ThreadLocal 能使每个使用变量的线程都有一份独立的副本 避免了传递执行上下文信息 但降低了代码的可重用性, 并在类之间引入隐含的耦合 3.4 不变性 不可变对象一定是线程安全的 3.5 安全发布 多线程间共享数据需要同步, 否则因可见性等问题会出现未知现象 Java 内存模型为不可变对象提供了特殊的初始化安全性保证, 从而不需要同步也能共享该对象 发布一个静态对象最简单安全的方式是使用静态初始化器 安全发布只能确保发布时状态的可见性, 后续对该对象的访问修改还是需要使用同步机制 四. 对象的组合4.1 设计线程安全的类 设计线程安全类的过程需要包含三个基本要素 找出构成对象状态的所有变量 找出约束状态变量的不变性条件 建立对象状态的并发访问管理策略 收集同步的需求, 了解对象在状态变量上的各种约束条件以及依赖状态的操作, 需要借助同步与封装 4.2 实例封闭 将数据封装在对象内部, 可以将数据的访问限制在对象的方法上, 从而更容易确保持有锁 4.3 线程安全的委托4.4 在现有的线程安全类中添加功能 使用组合而非其他的方式给某个对象添加线程安全的方法 4.5 将同步策略文档化 在文档中说明使用者需要了解的线程安全性保证, 以及代码维护人员需要了解的同步策略 五. 基础构建模块5.1 同步容器类5.1.1 同步容器类的问题 同步容器类是线程安全的, 但还是需要额外加锁来保护复合操作, 如遍历删除 5.1.2 迭代器与 ConcurrentModificationException 在设计同步容器类的迭代器时并没有考虑并发修改的问题, 当发现在迭代过程中容器被修改时, 会抛出 ConcurrentModificationException 实现方式为将计数器的变化与容器关联, 在迭代期间计数器被修改, 则 hasNext 或 next 会抛出 ConcurrentModificationException 在单线程中也可能抛出 ConcurrentModificationException, 当对象直接从容器删除而不是通过 Iterator.remove 来删除时 5.1.3 隐藏迭代器 在某些情况下, 迭代器会隐藏起来, 比如打印容器时会迭代调用每个元素的 toString 方法 容器的 hashCode 和 equals 等方法也会间接的执行迭代操作, 所以都可能抛出 ConcurrentModificationException 5.2 并发容器5.2.1 ConcurrentHashMap 同步容器类在执行每个操作期间都持有一个锁 对于基于散列的容器, 如果 hashCode 不能很均匀的分布, 会影响散列表的性能, 特别是还持有锁的情况 ConcurrentHashMap 采用分段锁 (Lock Striping) 的机制, 实现了更高的吞吐量, 而在单线程环境只损失较小的性能 ConcurrentHashMap 和其他并发容器一起增强了同步容器类 迭代器不会抛出 ConcurrentModificationException, 因此迭代过程不需要加锁 ConcurrentHashMap 的迭代器具有弱一致性 (weakly consistent) 而非 “及时失败” (fail-fast) 弱一致性的迭代器允许并发的修改, 并可以 (但是不保证) 在迭代器被构造后将修改操作反映给容器 对于需要在整个 Map 上进行计算的方法, 如 size 和 isEmpty , 这些方法的语义被略微减弱了以反映容器的并发特性 因为在计算的过程中可能已经过期, 实际上只是一个估计值 事实上在并发环境下的用处很小 ConcurrentHashMap 中没有实现对 Map 加锁以提供独占访问 在 HashTable 和 synchronizedMap 中, 获得 Map 的锁能防止其他线程访问这个 Map ConcurrentHashMap 还实现一些原子的复合操作, 如 putIfAbsent 5.2.3 CopyOnWriteArrayList Copy-On-Write 容器的线程安全性在于, 只要正确的发布一个不可变的对象, 则在访问的时候就不需要进行同步 在每次修改时都会创建并重新发布一个新的容器副本, 从而实现可变性 Copy-On-Write 容器的迭代器保留一个指向底层基础数组的引用, 且数组不会被修改 不会抛出 ConcurrentModificationException 并且返回的元素与迭代器创建时的元素完全一致 每次修改都会复制底层数组, 开销很大, 所以当迭代操作大于修改操作时才应该使用 5.3 阻塞队列和生产者-消费者 阻塞队列提供了可阻塞的 put 和 take 方法, 以及支持定时的 offer 和 poll 方法 阻塞队列支持生产者-消费者模式, 此模式解耦了生产数据与使用数据的过程 类库中包含了 BlockingQueue 的多种实现 LinkedBlockingQueue 和 ArrayBlockingQueue 是 FIFO 队列 两者分别与 LinkedList 和 ArrayList 类似但拥有更好的并发性能 PriorityBlockingQueue 是一个按优先级排序的队列 SynchronousQueue 是一个不存储元素的阻塞队列, 每个插入操作必须等到另一个线程调用移除操作, 否则插入操作一直处于阻塞状态 Java 6 加入了 Deque 和 BlockingDeque 分别对 Queue 和 BlockingQueue 进行扩展 Deque 是一个双端队列, 可以在队列的头和尾高效的插入和移除 具体实现包括 ArrayDeque 和 LinkedBlockingDeque 适用于工作密取 (Work Stealing) 模式 每个消费者持有各自的 Deque 如果一个消费者消费完了自己的 Deque, 它会去其他消费者 Deque 尾部消费 极大的减少了竞争 5.4 阻塞方法与中断方法 线程可能会阻塞或暂停执行, 原因有多种: 等待 I/O 操作结束 等待获得锁 等待从 Thread.sleep 中醒来 等待另外一个线程的计算结果 大多数阻塞的操作被中断时会抛出 InterruptedException 中断是一种协助机制 调用一个线程的 interrupt() 方法中断一个线程，并不是强行关闭这个线程，只是通知线程停止，将线程的中断标志位置为 true，线程是否中断，由线程本身决定, 线程可以进行停止前的释放资源, 完成必要的处理任务 处理中断的两种选择 传递中断 对于底层的方法, 在方法的签名上标注异常 (throws InterruptedException) 抛出异常，而异常的真正处理，应该交给调用它的那个函数 因为标注了异常, 调用者必须对 InterruptedException 异常进行处理 恢复中断 在底层方法中 catch 处理异常 处理完成后手动调用 Thread.currentThread().interrupt() 恢复中断 5.5 同步工具类5.5.1 闭锁 闭锁可以延迟线程的进度直到其到达终止状态 CountDownLatch 是一种闭锁的实现 有初始计数值 计数值大于 0 时, 获取锁的线程会被阻塞 计数值被减到 0 时, 所有被阻塞的线程同时被释放 5.5.2 FutureTask FutureTask 也可以用做闭锁 实现了 Future 语义, 表示一种抽象的可生成结果的计算 FutureTask 表示的计算是通过 Callable 实现的, 可以处于以下三种状态 等待运行 (Waiting to run) 正在运行 (Running) 运行完成 (Completed) Future.get 的行为取决于任务的状态 如果任务已完成, 会立即返回结果 否则将阻塞直到任务完成 返回结果或者抛出异常 5.5.3 信号量 Semaphore 用来控制同时访问某个资源的操作数量 信号量 S, 整型变量, 需要初始化值大于0 P 操作, 原子减少 S, 如果 S &lt; 0, 则阻塞当前线程 对应方法 Semaphore.acquire V 操作, 原子增加 S, 如果 S &lt;= 0, 则唤醒一个阻塞的线程 对应方法 Semaphore.release 可以利用信号量将任何容器变成有界阻塞容器 5.5.4 栅栏 栅栏 (Barrier) 类似于闭锁, 能阻塞一组线程直到某个事件发生 主要区别在于必须所有线程到达栅栏位置才能继续执行 闭锁用于等待事件, 栅栏用于等待其他线程 CyclicBarrier 1public CyclicBarrier(int parties, Runnable barrierAction) parties 表示线程个数 barrierAction 表示线程都达到栅栏后先执行的一个方法 1private int dowait(boolean timed, long nanos) timed 表示是否需要超时 nanos 表示超时的具体时间 超时会抛出 BrokenBarrierException 1public void reset() 重置栅栏, 原先在等待的线程会收到 BrokenBarrierException 六. 任务执行6.1 在线程中执行任务 串行处理机制无法提供高吞吐率或快速响应性 为每个任务分配一个线程会造成很高的开销和资源消耗 6.2 Executor 框架 java.util.concurrent 包中提供了一种灵活的线程池实现作为 Executor 框架的一部分 Executor 基于生产者 - 消费者模式 6.2.2 执行策略 在什么线程中执行任务 任务按照什么顺序执行 (FIFO, LIFO, 优先级) 有多少个任务能并发执行 在队列中有多少任务在等待执行 系统过载时的拒绝策略以及相关通知 执行任务的前后 6.2.3 线程池 线程池通过重用现有的线程而不是创建新线程而节省开销和响应性 Executors 中创建线程池的静态方法 newFixedThreadPool 创建一个核心和最大线程数量都固定的线程池, 等待队列的长度不受限制 newCachedThreadPool 创建一个可缓存的线程池, 会回收空闲线程, 最大线程数不受限制 newSingleThreadExecutor 创建一个单线程的 Executor, 等待队列的长度不受限制 newScheduledThreadPool 创建一个核心线程数量固定的线程池, 以延迟或定时的方式执行任务, 最大线程数不受限制 6.2.4 Executor 的生命周期 如果无法正确的关闭 Executor , JVM 将无法结束 Executor 扩展了 ExecutorService 接口用于解决生命周期 6.3 找出可利用的并行性 Executor 框架使用 Runnable 作为其基本的任务表示形式 Runnable 有很大的局限的抽象, 不能返回值或抛出受检查的异常 Callable 是一种更好的抽象 Future 表示一个任务的生命周期 CompletionService 将 Executor 和 BlockingQueue 融合在一起, 可以从阻塞队列里取出完成的 Future 七. 取消与关闭 通常中断是实现取消最合理的方式 最合理的中断策略是某种形式的线程级取消操作或服务级取消操作 尽快退出, 在必要时进行清理, 通知所有者该线程已退出 大多数可阻塞的库函数都是抛出 InterruptException 作为中断响应 处理不可中断的阻塞 Java.io 包中的同步 Socket I/O InputStream 和 OutputStream 中的 read, write 等方法都不会响应中断 通过关闭底层的套接字, 可以使上述方法抛出 SocketException Java.io 包中的同步 I/O 大多数标准的 Channel 都实现了 InterruptibleChannel 中断一个正在 InterruptibleChannel 上等待的线程会使所有在这条链路上阻塞的线程抛出 CloseByInterruptException 并关闭链路 关闭一个 InterruptibleChannel 时会使所有在这条链路上阻塞的线程抛出 AsynchronousCloseException Selector 的异步 I/O 如果一个线程在调用 Selector.select 方法 (在 java.nio. channels 中) 时阻塞了, 调用 close 或 wakeup 方法会使线程抛出 CloseSelectorException 获取某个锁 如果一个线程由于等待内置锁而阻塞, 那将无法响应中断 在 Lock 类中提供了 lockInterruptibly 方法允许线程响应中断 避免使用终结器 八. 线程池的使用8.1 在任务与执行策略之间的隐性耦合 有些任务需要明确的指定执行策略 依赖性任务 可能产生死锁 使用线程封闭机制的任务 如果将 Executor 从单线程环境改为线程池环境, 会失去线程安全性 对响应时间敏感的任务 执行时间长的任务会影响时间短的任务 使用 ThreadLocal 的任务 8.2 设置线程池的大小 8.3 配置 ThreadPoolExcutor12345678910public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; ......&#125; corePoolSize 表示线程池的常驻核心线程数 如果设置为 0, 则表示在没有任何任务时, 销毁线程池 如果大于 0, 即使没有任务时也会保证线程池的线程数量等于此值 此值如果设置的比较小, 则会频繁的创建和销毁线程 如果设置的比较大, 则会浪费系统资源, 所以开发者需要根据自己的实际业务来调整此值 maximumPoolSize 表示线程池在任务最多时, 最大可以创建的线程数 此值必须大于 0, 也必须大于等于 corePoolSize, 此值只有在任务比较多, 且不能存放在任务队列时才会用到 keepAliveTime 表示线程的存活时间 当线程池空闲时并且超过了此时间, 多余的线程就会销毁, 直到线程池中的线程数量销毁的等于 corePoolSize 为止 如果 maximumPoolSize 等于 corePoolSize, 那么线程池在空闲的时候也不会销毁任何线程 unit 表示存活时间的单位, 它是配合 keepAliveTime 参数共同使用的 workQueue 表示线程池执行的任务队列 当线程池的所有线程都在处理任务时, 如果来了新任务就会缓存到此任务队列中排队等待执行 threadFactory 表示线程的创建工厂, 此参数一般用的比较少, 我们通常在创建线程池时不指定此参数, 它会使用默认的线程创建工厂的方法来创建线程: 123456789101112131415161718192021222324252627// 默认的线程创建工厂, 需要实现 ThreadFactory 接口static class DefaultThreadFactory implements ThreadFactory &#123; private static final AtomicInteger poolNumber = new AtomicInteger(1); private final ThreadGroup group; private final AtomicInteger threadNumber = new AtomicInteger(1); private final String namePrefix; DefaultThreadFactory() &#123; SecurityManager s = System.getSecurityManager(); group = (s != null) ? s.getThreadGroup() : Thread.currentThread().getThreadGroup(); namePrefix = "pool-" + poolNumber.getAndIncrement() + "-thread-"; &#125; // 创建线程 public Thread newThread(Runnable r) &#123; Thread t = new Thread(group, r, namePrefix + threadNumber.getAndIncrement(), 0); if (t.isDaemon()) t.setDaemon(false); // 创建一个非守护线程 if (t.getPriority() != Thread.NORM_PRIORITY) t.setPriority(Thread.NORM_PRIORITY); // 线程优先级设置为默认值 return t; &#125;&#125; 我们也可以自定义一个线程工厂, 通过实现 ThreadFactory 接口来完成, 这样就可以自定义线程的名称或线程执行的优先级了 RejectedExecutionHandler 表示指定线程池的拒绝策略 当线程池的任务已经在缓存队列 workQueue 中存储满了之后, 并且不能创建新的线程来执行此任务时, 就会用到此拒绝策略, 它属于一种限流保护的机制 8.4 扩展 ThreadPoolExcutor 通过重写 beforeExecute() 和 afterExecute() 方法, 我们可以在扩展方法中添加日志或者实现数据统计, 比如统计线程的执行时间 十. 避免活跃性危险10.1 死锁 抱死 Deadly Embrace 线程 A 持有锁 L 并想获得锁 R 时, 线程 B 持有锁 R 并尝试获得锁 L 数据库系统的设计中考虑了监测死锁以及从死锁中恢复 当检测到一组事务 (Transaction) 发生了死锁时 (通过在表示等待关系的有向图中搜索循环) 选择一个牺牲者并放弃这个事务, 释放它所持有的资源 锁顺序死锁 如果按照相同循序来请求锁, 就不会出现死锁 业务中根据一个唯一不可变, 且具备可比性的值 (如数据库中的 id) 来排序加锁 在协作对象之间发生的死锁 如果在持有锁时调用某个外部方法, 将会出现活跃性问题 在这个外部方法中可能会获取其他锁, 从而产生死锁 或者阻塞时间过长, 导致其他线程无法及时获得当前被持有的锁 在调用某个方法时不需要持有锁, 那这种调用称为开放调用 10.2 死锁的避免和诊断 使用支持定时的锁 通过 Thread Dump 分析死锁 10.3 其他活跃性危险 饥饿 (Starvation) 线程由于不能访问所需要的资源而无法继续执行时, 就发生了饥饿 引发饥饿的最常见资源就是 CPU 时钟周期 线程优先级可能会导致低优先级的线程饥饿 糟糕的响应性 活锁 (Livelock) 错误的将不可修复的错误作为可修复的错误, 不断尝试修复导致活锁 十一. 性能与可伸缩性11.1 对性能的思考 可伸缩性: 当增加计算资源时 (如 cpu, 内存, 存储容量或带宽), 程序的吞吐量或者处理能力能相应地增加 避免不成熟的优化, 首先正确运行, 然后再考虑提高速度 11.2 Amdahl 定律 11.3 线程引入的开销 上下文切换 内存同步 非竞争同步会被 JVM 进行优化, 如锁消除和锁粒度粗化 阻塞 11.4 减少锁竞争 降低锁竞争程度的方式 减少持有锁的时间 缩小锁的范围 主要考虑大量计算或阻塞操作的代码 锁分解, 锁分段 避免热点域 降低请求锁的频率 使用带有协调机制的独占锁 并发容器 读写锁 不可变对象 原子变量 对象分配操作比同步的开销低, 对于性能优化来说, 对象池的用途有限 11.5 比较 Map 的性能 当竞争变的激烈时, 每个操作消耗的时间大部分用于上下文切换和调度延迟, 再加入更多线程也不会提高太多吞吐量 十二. 并发程序的测试 并发测试大致分为两类 安全性测试 不发生任何错误的行为 要找出容易检查的属性, 这些属性在发生错误时极可能失败, 同时检查的代码不会限制并发性 活跃性测试 性能测试 吞吐量: 一组并发任务中已经完成任务所占的比例 响应性: 请求从发出到完成之间的时间 (延迟) 可伸缩性: 在增加更多资源的情况下, 吞吐量提升的情况 可用通过 Thread.yield 或 Thread.sleep(0) 产生更多交替操作, 提高出错的概率 12.3 避免性能测试的陷阱 垃圾回收 确保测试运行期间能执行多次垃圾回收 动态编译 当某个类第一次被加载时, HotSpot JVM 会通过解译字节码的方式来执行它, 当某个方法运行次数足够多, 动态编译器会将其编译为机器码, 后面代码的执行就从解释执行变为直接执行 代码还可能被反编译已经重新编译 确保测试运行足够长, 或先预运行一段时间 对代码路径的不真实采样 JVM 可能会基于测试情况下临时有效的假设进行优化 测试程序不仅要大致判断典型的使用模式, 还需要尽量覆盖执行的代码路径集合 无用代码的消除 避免测试时的没覆盖到的代码被编译器优化消除 可以在相关代码中打印空白字符串 十三. 显式锁]]></content>
      <categories>
        <category>programming</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>multithread</tag>
        <tag>concurrency</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s-国内环境集群搭建-通过sealos]]></title>
    <url>%2F2020%2F08%2F06%2Fk8s-%E5%9B%BD%E5%86%85%E7%8E%AF%E5%A2%83%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA-%E9%80%9A%E8%BF%87sealos%2F</url>
    <content type="text"><![CDATA[k8s-国内环境集群搭建-通过sealos准备工作 获得 Kubernetes 最新版本离线包, 除默认的 1.14.1 版本外, 需要付费下载 准备服务器 主机名不可重复 12# 修改 hostname$ hostnamectl set-hostname --static k8s-x 支持 root 用户远程 ssh 登录 1234567$ vim /etc/ssh/sshd_config修改 PasswordAuthentication yes若无法使用 root 用户进行 SSH 登录, 还需修改 PermitRootLogin yes$ service sshd restart 安装 sealos 123# 下载并安装sealos, sealos是个golang的二进制工具，直接下载拷贝到bin目录即可, release页面也可下载wget -c https://sealyun.oss-cn-beijing.aliyuncs.com/latest/sealos &amp;&amp; \ chmod +x sealos &amp;&amp; mv sealos /usr/bin 提前下载离线资源包, 也可以 init 时直接指定下载地址 12# 下载离线资源包wget -c https://sealyun.oss-cn-beijing.aliyuncs.com/054f15a19f3c943bf387b3da25ef3de1-1.18.6/kube1.18.6.tar.gz 执行命令安装 需要 root 用户执行 123456# 安装 kubernetes 集群sealos init --passwd 123456 \--master 192.168.0.2 --master 192.168.0.3 --master 192.168.0.4 \--node 192.168.0.5 --node 192.168.0.6 --node 192.168.0.7 \--pkg-url https://sealyun.oss-cn-beijing.aliyuncs.com/054f15a19f3c943bf387b3da25ef3de1-1.18.6/kube1.18.6.tar.gz \--version v1.18.6 检查安装是否正常1234567891011121314151617181920212223242526272829303132[root@iZj6cdqfqw4o4o9tc0q44rZ ~]# kubectl get nodeNAME STATUS ROLES AGE VERSIONizj6cdqfqw4o4o9tc0q44rz Ready master 2m25s v1.14.1izj6cdqfqw4o4o9tc0q44sz Ready master 119s v1.14.1izj6cdqfqw4o4o9tc0q44tz Ready master 63s v1.14.1izj6cdqfqw4o4o9tc0q44uz Ready &lt;none&gt; 38s v1.14.1[root@iZj6cdqfqw4o4o9tc0q44rZ ~]# kubectl get pod --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system calico-kube-controllers-5cbcccc885-9n2p8 1/1 Running 0 3m1skube-system calico-node-656zn 1/1 Running 0 93skube-system calico-node-bv5hn 1/1 Running 0 2m54skube-system calico-node-f2vmd 1/1 Running 0 3m1skube-system calico-node-tbd5l 1/1 Running 0 118skube-system coredns-fb8b8dccf-8bnkv 1/1 Running 0 3m1skube-system coredns-fb8b8dccf-spq7r 1/1 Running 0 3m1skube-system etcd-izj6cdqfqw4o4o9tc0q44rz 1/1 Running 0 2m25skube-system etcd-izj6cdqfqw4o4o9tc0q44sz 1/1 Running 0 2m53skube-system etcd-izj6cdqfqw4o4o9tc0q44tz 1/1 Running 0 118skube-system kube-apiserver-izj6cdqfqw4o4o9tc0q44rz 1/1 Running 0 2m15skube-system kube-apiserver-izj6cdqfqw4o4o9tc0q44sz 1/1 Running 0 2m54skube-system kube-apiserver-izj6cdqfqw4o4o9tc0q44tz 1/1 Running 1 47skube-system kube-controller-manager-izj6cdqfqw4o4o9tc0q44rz 1/1 Running 1 2m43skube-system kube-controller-manager-izj6cdqfqw4o4o9tc0q44sz 1/1 Running 0 2m54skube-system kube-controller-manager-izj6cdqfqw4o4o9tc0q44tz 1/1 Running 0 63skube-system kube-proxy-b9b9z 1/1 Running 0 2m54skube-system kube-proxy-nf66n 1/1 Running 0 3m1skube-system kube-proxy-q2bqp 1/1 Running 0 118skube-system kube-proxy-s5g2k 1/1 Running 0 93skube-system kube-scheduler-izj6cdqfqw4o4o9tc0q44rz 1/1 Running 1 2m43skube-system kube-scheduler-izj6cdqfqw4o4o9tc0q44sz 1/1 Running 0 2m54skube-system kube-scheduler-izj6cdqfqw4o4o9tc0q44tz 1/1 Running 0 61skube-system kube-sealyun-lvscare-izj6cdqfqw4o4o9tc0q44uz 1/1 Running 0 86s 其他配置授权信息给非 root 用户 登录需要授权的用户 123mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 安装 Web UI (Kuboard) 在 master 服务器输入以下命令 12kubectl apply -f https://kuboard.cn/install-script/kuboard.yamlkubectl apply -f https://addons.kuboard.cn/metrics-server/0.3.7/metrics-server.yaml 查看 Kuboard 运行状态 1kubectl get pods -l k8s.kuboard.cn/name=kuboard -n kube-system 12NAME READY STATUS RESTARTS AGEkuboard-54c9c4f6cb-6lf88 1/1 Running 0 45s 获取 token 1echo $(kubectl -n kube-system get secret $(kubectl -n kube-system get secret | grep kuboard-user | awk '&#123;print $1&#125;') -o go-template='&#123;&#123;.data.token&#125;&#125;' | base64 -d) 1eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWc4aHhiIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI5NDhiYjVlNi04Y2RjLTExZTktYjY3ZS1mYTE2M2U1ZjdhMGYiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.DZ6dMTr8GExo5IH_vCWdB_MDfQaNognjfZKl0E5VW8vUFMVvALwo0BS-6Qsqpfxrlz87oE9yGVCpBYV0D00811bLhHIg-IR_MiBneadcqdQ_TGm_a0Pz0RbIzqJlRPiyMSxk1eXhmayfPn01upPdVCQj6D3vAY77dpcGplu3p5wE6vsNWAvrQ2d_V1KhR03IB1jJZkYwrI8FHCq_5YuzkPfHsgZ9MBQgH-jqqNXs6r8aoUZIbLsYcMHkin2vzRsMy_tjMCI9yXGiOqI-E5efTb-_KbDVwV5cbdqEIegdtYZ2J3mlrFQlmPGYTwFI8Ba9LleSYbCi4o0k74568KcN_w 访问 Kuboard Kuboard Service 使用了 NodePort 的方式暴露服务，NodePort 为 32567 可以访问 http://任意一个 Worker 节点的 IP 地址:32567/ 输入前一步骤中获得的 token，可进入 Kuboard 集群概览页 不关闭 swap 的情况下加入节点 解压离线包 修改离线包中的内容 ./kube/shell/init.sh 删去 swapoff -a || true ./kube/bin/kubelet-pre-start.sh 删去 swapoff -a ./kube/conf/10-kubeadm.conf 在其他``Environment下再加入一行Environment=”KUBELET_EXTRA_ARGS=–fail-swap-on=false”` 在 ./kube/shell 下执行 sh init.sh 加入 ipvs 相关规则 sealos ipvs --vs 10.103.97.2:6443 --rs 192.168.99.136:6443 --health-path /healthz --health-schem https --run-once echo &quot;10.103.97.2 apiserver.cluster.local&quot; &gt;&gt; /etc/hosts (可选) 安装 socat, apt-get install socat 输入node join 命令加入集群 在 master 上通过 kubeadm token create --print-join-command 得到 node join 命令, 并在后面追加 --ignore-preflight-errors=Swap, 如: kubeadm join apiserver.cluster.local:6443 --token 5w69vr.d6nj668vhefmttrk --discovery-token-ca-cert-hash sha256:13b74ebad8ae8ce1264a0eab7bd02895142009775a23feda158d29da464625ff --ignore-preflight-errors=Swap 官方文档: https://sealyun.com/docs/ https://kuboard.cn/install/install-dashboard.html]]></content>
      <categories>
        <category>programming</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>install</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 集合与同步]]></title>
    <url>%2F2020%2F08%2F05%2FJava-%E9%9B%86%E5%90%88%E4%B8%8E%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Java 垃圾回收]]></title>
    <url>%2F2020%2F07%2F28%2FJava-%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E4%B8%8E%E5%BC%95%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Java 垃圾回收 垃圾回收就是由程序自动的回收已死对象, 可以分为两个部分 如何判断对象已死 如何清理掉已死对象 判断对象是否存活引用计数法 给对象中添加一个引用计数器 每当有一个地方引用它时，计数器就加 1 当引用失效时, 计数器就减 1 当对象的计数器值为 0 时，则代表该对象可以被回收了 优点是实现简单且回收效率高 缺点是无法解决循环引用的问题, 即两个对象相互引用的情况 可达性分析 被商用 JVM 采用 从 GC ROOT 作为起点开始遍历所有节点, GC ROOT 指以下几类对象 虚拟机栈的栈帧的局部变量表所引用的对象 本地方法栈的 JNI 所引用的对象 方法区的静态变量和常量所引用的对象 对于遍历到的每个节点都做一个标识 遍历完成后, 没有标识的节点说明是可回收的 回收算法标记清除 通常使用一张表 (类似) 来记录哪些空间已被使用 首先通过可达性分析找到所有的垃圾，然后将其占用的空间释放掉 该算法的问题是可能会产生大量的内存碎片 标记整理 为了解决内存碎片的问题，标记整理在标记清除算法上做了优化 在找到所有垃圾对象后，不是直接释放掉其占用的空间，而是将所有存活对象往内存一端移动 回收完成后，所有对象都是相邻的 复制算法 复制算法将内存区域划分为两个，同一时间只有一个区域有对象 每次垃圾回收时，通过可达性分析算法，找出所有存活对象，将这些存活对象移动到另一区域 为新对象分配内存时，可以通过智能指针的形式，高效简单 复制算法的缺点是会浪费一部分空间以便存放下次回收后存活的对象且需要一块额外的空间进行担保（当一个区域存放不下存活的对象时） 分代收集 在商用 JVM 中，大多使用的是分代收集算法 根据对象的特性，可以将内存划分为 3 个代：年轻代，老年代，永久代（ JVM 8 后称为元空间） 年轻代存放新分配的对象，使用的是复制算法 老年代使用标记清除或标记整理算法 其中年轻代分为一个 Eden 区和两个 Survivor 区 (From, To)，其比例默认为 8:1:1（-XX:SurvivorRatio） 优先在 Eden 区分配对象 Eden 区空间不足，触发 Minor GC (Young GC)，标记可回收对象，然后 Eden 区存活对象拷贝到往 Survivor-From 区，接下来清空 Eden 区 再次触发 Minor GC，扫描 Eden 区和 From 区，把存活的对象复制到 To 区，清空 Eden 区和 From 区 如果在 Minor GC 复制存活对象到 Survivor 区时，发现 Survivor 区内存不够，则提前把对象放入老年代 大对象直接进入老年代 如果发现需要大量连续内存空间的 Java 对象，如很长的字符串或者数组，则直接把对象放入老年代 可通过 -XX:PretenureSizeThreshold 参数设置大对象的最小大小，该参数只对 Serial 和 ParNew 两款收集器有效 因为新生代采用复制算法收集垃圾，大对象直接进入老年代，避免在 Eden 区和 Survivor 区发生大量内存复制 写程序的时候尽量避免大对象 长期存活对象进入老年代 固定对象年龄判断：默认情况，存活对象在 Survivor 的 From 和 To 区来回交换 15 次后，如果对象最终还是存活，就放入老年代 可以通过 -XX:MaxTenuringThreshold 参数来设置对象的年龄 动态对象年龄判断：如果发现 Survivor 中有相同年龄的对象空间总和大于 Survivor 空间的一半，那么年龄大于或者等于该年龄的对象直接晋升到老年代 空间分配担保 为什么需要分配担保 如果 Survivor 区存活了很多对象，空间不够了，都需要晋升到老年代，那么就需要老年代进行分配担保，也就是将Survivor 无法容纳的对象直接进入老年代 发生 Minor GC 前，JVM 先检查老年代最大可用连续空间是否大于新生代所有对象的总空间 大于：空间足够，直接 Minor GC 小于：进行一次 Full GC JDK 6 Update 24 前会根据 HandlePromotionFailure 参数判断是否允许担保失败 如果允许，则尝试一次 Minor GC 否则，则进行 Full GC 年轻代老年代比例默认为 1:2 (-XX:NewRatio, -Xmn) 年轻代使用复制算法的原因是年轻代对象的创建和回收很频繁，同时大部分对象很快都会死亡，所以复制算法创建和回收对象的效率都比较高 老年代不使用复制算法的原因是老年代对象通常存活时间比较长，如果采用复制算法，则复制存活对象的开销会比较大，且复制算法是需要其他区域担保的。 所以老年代不使用复制算法 垃圾回收器Serial 串行回收器（年轻代） 使用单线程，复制算法实现 在回收的整个过程中需要 STW (Stop The World) 在单核 CPU 的机器上，使用单线程进行垃圾回收效率更高 使用方法：XX:+UseSerialGC ps：在 JDK Client 模式，不指定 VM 参数，默认是串行垃圾回收器 Serial Old 串行回收器（老年代） 与 Serial 相似，但使用标记整理算法实现 ParNew 并行回收器（年轻代） Serial 的多线程形式 -XX:+UseParNewGC（新生代使用并行收集器，老年代使用串行回收收集器） 或者 -XX:+UseConcMarkSweepGC (新生代使用并行收集器，老年代使用 CMS) Parallel Scavenge 基于吞吐量的并行回收器（年轻代） 多线程的回收器，高吞吐量（= 程序运行时间 / (程序运行时间+回收器运行时间)），可以高效率的利用 CPU 时间，尽快完成程序的运算任务，适合后台应用等对响应时间要求不高的场景 有一个自适应条件参数（-XX:+UseAdaptiveSizePolicy），当这个参数打开后，无需手动指定新生代大小（-Xmn），Eden 和Survivor 比例（-XX:SurvivorRatio）等参数，虚拟机会动态调节这些参数来选择最适合的停顿时间（-XX:MaxGCPauseMillis）或吞吐量（ -XX:GCTimeRatio） Parallel Scavenge 是 Server 级别多 CPU 机器上的默认 GC 方式，也可以通过 -XX:+UseParallelGC 来指定，并且可以采用 -XX:ParallelGCThread 来指定线程数 Parallel Scavenge 对应的老年代收集器只有 Serial Old 和 Parallel Old。不能与 CMS 搭配使用的原因是，其使用的框架不同，并不是技术原因 Parallel Old 基于吞吐量的并行回收器（老年代） 使用多线程和标记整理算法 与 Parallen Scavenge 相似，只不过是运用于老年代 CMS 关注暂停时间的回收器 （老年代） 基于标记清除算法实现，关注 GC 的暂停时间，在注重响应时间的应用上使用 三色标记法 在说 CMS 具体步骤前，先看下 CMS 使用的垃圾标记算法：三色标记法 将堆中对象分为 3 个集合：白色、灰色和黑色 白色集合：需要被回收的对象 黑色集合：没有引用白色集合中的对象，且从 GC ROOT 可达。该集合的对象是不会被回收的 灰色集合：从根可达但是还没有扫描完其引用的所有对象，该集合的对象不会被回收，且当其引用的白色对象全部被扫描后，会将其加入到黑色集合中 一般来说，会将被 GC ROOT 直接引用到的对象初始化到灰色集合，其余所有对象初始化到白色集合，然后开始执行算法： 将一个灰色对象加入到黑色集合 将其引用到的所有白色对象加入到灰色集合 重复上述两步，直到灰色集合为空 该算法保证从 GC ROOT 出发，所有没有被引用到的对象都在白色集合中，所以最后白色集合中的所有对象就是要回收的对象 CMS 回收过程 分为 4 个过程，初始标记，并发标记，重新标记，并发清理 初始标记 从 GC ROOT 出发，找到所有被 GC ROOT 直接引用的节点 此过程需要 STW (Stop The World) 并发标记 以上一步骤的节点为根节点，并发的遍历所有节点 同时会开启 Write Barrier 如果在此过程中存在黑色对象新增对白色对象的引用，则会通过 Write Barrier 记录下来 如下图，在 GC 过程中，用三色标记法遍历到 A 这个对象（图 1），将A引用到的BCD标记为灰色 之后，在应用程序线程中创建了一个对象 E，A 引用了它（ 图 2 这个阶段 GC 是并发标记的） 然后将 A 标记为黑色（图 3） 在 GC 扫描结束后，E 这个对象因为是白色的，所以将被回收掉 这显然是不能接受的，并发垃圾回收器的底线是允许一部分垃圾暂时不回收（见下面的浮动垃圾），但绝不允许从根可达的存活对象被当作垃圾处理掉 重新标记 因为并发标记的过程中可能有引用关系的变化，所以该阶段需要 STW 以 GC ROOT，Write Barrier 中记录的对象为根节点，重新遍历 这里为什么还需要再遍历 GC ROOT ？ 因为 Write Barrier 是作用在堆上的，无法感知到 GC ROOT 上引用关系的变更 并发清理： 并发的清理所有垃圾对象 CMS 通过将步骤拆分，实现了降低 STW 时间的目的。但 CMS 也会有以下问题： 浮动垃圾，在并发标记的过程中（及之后阶段），可能存在原来被引用的对象变成无人引用了 在这次 GC 不会对其清理 CPU 敏感，因为用户程序是和 GC 线程同时运行的，所以会导致 GC 的过程中程序运行变慢，GC 运行时间增长，吞吐量降低 默认回收线程是（CPU 数量 + 3）/ 4，也就是 CPU 不足 4 个时，会有一半的 CPU 资源给 GC 线程 空间碎片，标记清除算法共有的问题。当碎片过多时，为大对象分配内存空间就会很麻烦 有时候就是老年代空间有大量空间剩余，但没有连续的大空间来分配当前对象，不得不提前触发 Full GC CMS 提供一个参数（-XX:+UseCMSCompactAtFullCollection），在 Full GC 发生时开启内存合并整理 这个过程是 STW 的 同时还可以通过参数（-XX:CMSFullGCsBeforeCom-paction）设置执行多少次不压缩的 Full GC 后，进行一次压缩的 需要更大的内存空间，因为是同时运行的 GC 和用户程序，所以不能像其他老年代收集器一样，等老年代满了再触发 GC，而是要预留一定的空间 CMS 可以配置当老年代使用率到达某个阈值时（ -XX:CMSInitiatingOccupancyFraction=80 ），开始 CMS GC 在 Old GC 运行的过程中，可能有大量对象从年轻代晋升，而出现老年代存放不下的问题（因为这个时候垃圾还没被回收掉），该问题叫 Concurrent Model Failure, 这时候会启用 Serial Old 收集器，重新回收整个老年代 Concurrent Model Failure 一般伴随着 ParNew promotion failed（晋升担保失败）, 解决这个问题的办法就是可以让 CMS 在进行一定次数的 Full GC（标记清除）的时候进行一次标记整理算法，或者降低触发 CMS GC 的阈值 Java 引用类型原理 Java 中主要有 4 种引用类型：强引用、软引用、弱引用、虚引用 序号 引用类型 取得目标对象方式 垃圾回收条件 是否可能内存泄漏 1 强引用 直接调用 不回收 可能 2 软引用 通过 get() 方法 视内存情况回收 不可能 3 弱引用 通过 get() 方法 永远回收 不可能 4 虚引用 无法取得 不回收 可能 强引用就是我们经常使用的 Object a = new Object(); 这样的形式，在 Java 中并没有对应的 Reference 类 其他三种引用类型都继承于 Reference 类 Reference相关字段1234567891011121314151617181920212223public abstract class Reference&lt;T&gt; &#123; // 引用的对象 private T referent; // 回收队列，由使用者在 Reference 的构造函数中指定, 开发者可以通过从 ReferenceQueue 中 poll 感知到对象被回收的事件 volatile ReferenceQueue&lt;? super T&gt; queue; // 当该引用被加入到 queue 中的时候，该字段被设置为 queue 中的下一个元素，以形成链表结构 volatile Reference next; // 在 GC 时，JVM 底层会维护一个叫 DiscoveredList 的链表，存放的是 Reference 对象，discovered 字段指向的就是链表中的下一个元素，由 JVM 设置 transient private Reference&lt;T&gt; discovered; // 进行线程同步的锁对象 static private class Lock &#123; &#125; private static Lock lock = new Lock(); // 等待加入 queue 的 Reference 对象，在 GC 时由 JVM 设置，会有一个 Java 层的线程 (ReferenceHandler) 源源不断的从pending 中提取元素加入到 queue private static Reference&lt;Object&gt; pending = null; ......&#125; 生命周期 主要分为 Native 层和 Java 层两个部分 Native 层在 GC 时将需要被回收的 Reference 对象加入到 DiscoveredList 中，然后将 DiscoveredList 的元素移动到 PendingList 中, PendingList 的队首就是 Reference 类中的 pending 对象 Java 层代码 123456789101112131415161718192021222324252627282930313233343536373839404142private static class ReferenceHandler extends Thread &#123; ... public void run() &#123; while (true) &#123; tryHandlePending(true); &#125; &#125; &#125; static boolean tryHandlePending(boolean waitForNotify) &#123; Reference&lt;Object&gt; r; Cleaner c; try &#123; synchronized (lock) &#123; if (pending != null) &#123; r = pending; // 如果是 Cleaner 对象，则记录下来，下面做特殊处理 c = r instanceof Cleaner ? (Cleaner) r : null; // 指向 PendingList 的下一个对象 pending = r.discovered; r.discovered = null; &#125; else &#123; // 如果 pending 为 null 就先等待，当有对象加入到 PendingList 中时，JVM 会执行 notify if (waitForNotify) &#123; lock.wait(); &#125; // retry if waited return waitForNotify; &#125; &#125; &#125; ... // 如果是 CLeaner 对象，则调用 clean 方法进行资源回收 if (c != null) &#123; c.clean(); return true; &#125; // 将 Reference 加入到 ReferenceQueue ReferenceQueue&lt;? super Object&gt; q = r.queue; if (q != ReferenceQueue.NULL) q.enqueue(r); return true; &#125; 对于 Cleaner 类型（继承自虚引用）的对象会有额外的处理 在其指向的对象被回收时，会调用 clean 方法，该方法主要是用来做对应的资源回收 在堆外内存 DirectByteBuffer 中就是用 Cleaner 进行堆外内存的回收，这也是虚引用在 Java 中的典型应用 SoftReference123456789101112131415161718192021222324public class SoftReference&lt;T&gt; extends Reference&lt;T&gt; &#123; static private long clock; private long timestamp; public SoftReference(T referent) &#123; super(referent); this.timestamp = clock; &#125; public SoftReference(T referent, ReferenceQueue&lt;? super T&gt; q) &#123; super(referent, q); this.timestamp = clock; &#125; public T get() &#123; T o = super.get(); if (o != null &amp;&amp; this.timestamp != clock) this.timestamp = clock; return o; &#125;&#125; 软引用的实现多了两个字段：clock 和 timestamp clock 是个静态变量，每次 GC 时都会将该字段设置成当前时间 timestamp 字段则会在每次调用 get 方法时将其赋值为 clock（如果不相等且对象没被回收） 通过 JVM 源码查看这两个字段的作用 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455size_tReferenceProcessor::process_discovered_reflist( DiscoveredList refs_lists[], ReferencePolicy* policy, bool clear_referent, BoolObjectClosure* is_alive, OopClosure* keep_alive, VoidClosure* complete_gc, AbstractRefProcTaskExecutor* task_executor)&#123; ... // refs_lists 就是前面提到的 DiscoveredList // 对于 DiscoveredList 的处理分为几个阶段，SoftReference 的处理就在第一阶段 ... for (uint i = 0; i &lt; _max_num_q; i++) &#123; process_phase1(refs_lists[i], policy, is_alive, keep_alive, complete_gc); &#125; ...&#125;// 该阶段的主要目的就是当内存足够时，将对应的 SoftReference 从 refs_list 中移除voidReferenceProcessor::process_phase1(DiscoveredList&amp; refs_list, ReferencePolicy* policy, BoolObjectClosure* is_alive, OopClosure* keep_alive, VoidClosure* complete_gc) &#123; DiscoveredListIterator iter(refs_list, keep_alive, is_alive); // Decide which softly reachable refs should be kept alive. while (iter.has_next()) &#123; iter.load_ptrs(DEBUG_ONLY(!discovery_is_atomic() /* allow_null_referent */)); // 判断引用的对象是否存活 bool referent_is_dead = (iter.referent() != NULL) &amp;&amp; !iter.is_referent_alive(); // 如果引用的对象已经不存活了，则会去调用对应的 ReferencePolicy 判断该对象是不时要被回收 if (referent_is_dead &amp;&amp; !policy-&gt;should_clear_reference(iter.obj(), _soft_ref_timestamp_clock)) &#123; if (TraceReferenceGC) &#123; gclog_or_tty-&gt;print_cr("Dropping reference (" INTPTR_FORMAT ": %s" ") by policy", (void *)iter.obj(), iter.obj()-&gt;klass()-&gt;internal_name()); &#125; // Remove Reference object from list iter.remove(); // Make the Reference object active again iter.make_active(); // keep the referent around iter.make_referent_alive(); iter.move_to_next(); &#125; else &#123; iter.next(); &#125; &#125; ...&#125; refs_lists 中存放了本次 GC 发现的某种引用类型（虚引用、软引用、弱引用等），而 process_discovered_reflist 方法的作用就是将不需要被回收的对象从 refs_lists 移除掉，refs_lists 最后剩下的元素全是需要被回收的元素，最后会将其第一个元素赋值给上文提到过的 Reference.java#pending 字段 ReferencePolicy 一共有4种实现 NeverClearPolicy，永远返回 false, 代表永远不回收 SoftReference，在 JVM 中该类没有被使用 AlwaysClearPolicy，永远返回 true，在 referenceProcessor.hpp#setup 方法中中可以设置 policy 为 AlwaysClearPolicy LRUCurrentHeapPolicy，LRUMaxHeapPolicy should_clear_reference 方法完全相同 123456789101112bool LRUMaxHeapPolicy::should_clear_reference(oop p, jlong timestamp_clock) &#123; jlong interval = timestamp_clock - java_lang_ref_SoftReference::timestamp(p); assert(interval &gt;= 0, "Sanity check"); // The interval will be zero if the ref was accessed since the last scavenge/gc. if(interval &lt;= _max_interval) &#123; return false; &#125; return true;&#125; timestamp_clock 就是 SoftReference 的静态字段 clock java_lang_ref_SoftReference::timestamp(p) 对应是字段 timestamp 如果上次 GC 后有调用 SoftReference#get，interval 值为 0，否则为若干次 GC 之间的时间差 _max_interval 则代表了一个临界值，它的值在 LRUCurrentHeapPolicy 和 LRUMaxHeapPolicy 两种策略中有差异 12345678910111213void LRUCurrentHeapPolicy::setup() &#123; _max_interval = (Universe::get_heap_free_at_last_gc() / M) * SoftRefLRUPolicyMSPerMB; assert(_max_interval &gt;= 0,"Sanity check");&#125;void LRUMaxHeapPolicy::setup() &#123; size_t max_heap = MaxHeapSize; max_heap -= Universe::get_heap_used_at_last_gc(); max_heap /= M; _max_interval = max_heap * SoftRefLRUPolicyMSPerMB; assert(_max_interval &gt;= 0,"Sanity check");&#125; 其中 SoftRefLRUPolicyMSPerMB 默认为 1000 前者的计算方法和上次 GC 后可用堆大小有关 后者计算方法和（堆大小 - 上次 GC 时堆使用大小）有关 所以 SoftReference 什么时候被回收和使用的策略（默认应该是 LRUCurrentHeapPolicy），堆可用大小，该 SoftReference 上一次调用 get 方法的时间都有关系 WeakReference1234567891011public class WeakReference&lt;T&gt; extends Reference&lt;T&gt; &#123; public WeakReference(T referent) &#123; super(referent); &#125; public WeakReference(T referent, ReferenceQueue&lt;? super T&gt; q) &#123; super(referent, q); &#125;&#125; WeakReference 在 Java 层只是继承了 Reference，没有做任何的改动 那 referent 字段是什么时候被置为 null 的呢？我们再看下上文提到过的 process_discovered_reflist 方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778size_tReferenceProcessor::process_discovered_reflist( DiscoveredList refs_lists[], ReferencePolicy* policy, bool clear_referent, BoolObjectClosure* is_alive, OopClosure* keep_alive, VoidClosure* complete_gc, AbstractRefProcTaskExecutor* task_executor)&#123; ... // Phase 1: 将所有不存活但是还不能被回收的软引用从 refs_lists 中移除（只有 refs_lists 为软引用的时候，这里 policy 才不为 null） if (policy != NULL) &#123; if (mt_processing) &#123; RefProcPhase1Task phase1(*this, refs_lists, policy, true /*marks_oops_alive*/); task_executor-&gt;execute(phase1); &#125; else &#123; for (uint i = 0; i &lt; _max_num_q; i++) &#123; process_phase1(refs_lists[i], policy, is_alive, keep_alive, complete_gc); &#125; &#125; &#125; else &#123; // policy == NULL assert(refs_lists != _discoveredSoftRefs, "Policy must be specified for soft references."); &#125; // Phase 2: // 移除所有指向对象还存活的引用 if (mt_processing) &#123; RefProcPhase2Task phase2(*this, refs_lists, !discovery_is_atomic() /*marks_oops_alive*/); task_executor-&gt;execute(phase2); &#125; else &#123; for (uint i = 0; i &lt; _max_num_q; i++) &#123; process_phase2(refs_lists[i], is_alive, keep_alive, complete_gc); &#125; &#125; // Phase 3: // 根据 clear_referent 的值决定是否将不存活对象回收 if (mt_processing) &#123; RefProcPhase3Task phase3(*this, refs_lists, clear_referent, true /*marks_oops_alive*/); task_executor-&gt;execute(phase3); &#125; else &#123; for (uint i = 0; i &lt; _max_num_q; i++) &#123; process_phase3(refs_lists[i], clear_referent, is_alive, keep_alive, complete_gc); &#125; &#125; return total_list_count;&#125;voidReferenceProcessor::process_phase3(DiscoveredList&amp; refs_list, bool clear_referent, BoolObjectClosure* is_alive, OopClosure* keep_alive, VoidClosure* complete_gc) &#123; ResourceMark rm; DiscoveredListIterator iter(refs_list, keep_alive, is_alive); while (iter.has_next()) &#123; iter.update_discovered(); iter.load_ptrs(DEBUG_ONLY(false /* allow_null_referent */)); if (clear_referent) &#123; // NULL out referent pointer // 将 Reference 的 referent 字段置为 null，之后会被 GC 回收 iter.clear_referent(); &#125; else &#123; // keep the referent around // 标记引用的对象为存活，该对象在这次 GC 将不会被回收 iter.make_referent_alive(); &#125; ... &#125; ...&#125; 不管是弱引用还是其他引用类型，将字段 referent 置 null 的操作都发生在 process_phase3 中，而具体行为是由 clear_referent 的值决定的。而 clear_referent 的值则和引用类型相关 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748ReferenceProcessorStats ReferenceProcessor::process_discovered_references( BoolObjectClosure* is_alive, OopClosure* keep_alive, VoidClosure* complete_gc, AbstractRefProcTaskExecutor* task_executor, GCTimer* gc_timer) &#123; NOT_PRODUCT(verify_ok_to_handle_reflists()); ... // process_discovered_reflist 方法的第 3 个字段就是 clear_referent // Soft references size_t soft_count = 0; &#123; GCTraceTime tt("SoftReference", trace_time, false, gc_timer); soft_count = process_discovered_reflist(_discoveredSoftRefs, _current_soft_ref_policy, true, is_alive, keep_alive, complete_gc, task_executor); &#125; update_soft_ref_master_clock(); // Weak references size_t weak_count = 0; &#123; GCTraceTime tt("WeakReference", trace_time, false, gc_timer); weak_count = process_discovered_reflist(_discoveredWeakRefs, NULL, true, is_alive, keep_alive, complete_gc, task_executor); &#125; // Final references size_t final_count = 0; &#123; GCTraceTime tt("FinalReference", trace_time, false, gc_timer); final_count = process_discovered_reflist(_discoveredFinalRefs, NULL, false, is_alive, keep_alive, complete_gc, task_executor); &#125; // Phantom references size_t phantom_count = 0; &#123; GCTraceTime tt("PhantomReference", trace_time, false, gc_timer); phantom_count = process_discovered_reflist(_discoveredPhantomRefs, NULL, false, is_alive, keep_alive, complete_gc, task_executor); &#125; ...&#125; 可以看到，对于 Soft references 和 Weak references clear_referent 字段传入的都是 true 对象不可达后，引用字段就会被置为 null，然后对象就会被回收 对于软引用来说，如果内存足够的话，在 Phase 1 相关的引用就会从 refs_list 中被移除，到 Phase 3 时 refs_list 为空集合 对于 Final references 和 Phantom references，clear_referent 字段传入的是 false 也就意味着被这两种引用类型引用的对象，如果没有其他额外处理，只要 Reference 对象还存活，那引用的对象是不会被回收的 Final references 和对象是否重写了 finalize 方法有关, 不在本文分析范围之内 PhantomReference1234567891011public class PhantomReference&lt;T&gt; extends Reference&lt;T&gt; &#123; public T get() &#123; return null; &#125; public PhantomReference(T referent, ReferenceQueue&lt;? super T&gt; q) &#123; super(referent, q); &#125;&#125; 可以看到虚引用的 get 方法永远返回 null，我们看个 demo 1234567891011121314151617 public static void demo() throws InterruptedException &#123; Object obj = new Object(); ReferenceQueue&lt;Object&gt; refQueue = new ReferenceQueue&lt;&gt;(); PhantomReference&lt;Object&gt; phanRef = new PhantomReference&lt;&gt;(obj, refQueue); Object objg = phanRef.get(); // 这里拿到的是 null System.out.println(objg); // 让 obj 变成垃圾 obj = null; System.gc(); Thread.sleep(3000); // gc 后会将 phanRef 加入到 refQueue 中 Reference&lt;? extends Object&gt; phanRefP = refQueue.remove(); // 这里输出 true System.out.println(phanRefP == phanRef);&#125; 从以上代码中可以看到，虚引用能够在指向对象不可达时得到一个’通知’（其实所有继承 References 的类都有这个功能） 需要注意的是 GC 完成后，phanRef.referent 依然指向之前创建 Object，也就是说 Object 对象一直没被回收 造成这一现象的原因在前面也已经说了：clear_referent 字段传入的是 false 对于虚引用来说，从 refQueue.remove(); 得到引用对象后，可以调用 clear 方法强行解除引用和对象之间的关系，使得对象下次可以 GC 时可以被回收掉 总结 我们经常在网上看到软引用的介绍是：在内存不足的时候才会回收，那内存不足是怎么定义的？为什么才叫内存不足？ 软引用会在内存不足时被回收，内存不足的定义和该引用对象 get 的时间以及当前堆可用内存大小都有关系，计算公式在上文中也已经给出 网上对于虚引用的介绍是：形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。主要用来跟踪对象被垃圾回收器回收的活动。真的是这样吗？ 严格的说，虚引用是会影响对象生命周期的，如果不做任何处理，只要虚引用不被回收，那其引用的对象永远不会被回收 所以一般来说，从 ReferenceQueue 中获得 PhantomReference 对象后，如果 PhantomReference 对象不会被回收的话（比如被其他 GC ROOT 可达的对象引用），需要调用 clear 方法解除 PhantomReference 和其引用对象的引用关系 各个引用的使用场景 软引用 用于缓存，创建的对象放进缓存中，当内存不足时，JVM 就会回收早先创建的对象 弱引用 WeakHashMap 中的 key 使用的是弱引用 Threadlocal 中 ThreadLocalMap 的 Entry 继承自弱引用, 避免 Threadlocal 无法回收 虚引用 DirectByteBuffer 中使用虚引用的子类 Cleaner.java 来实现堆外内存的回收 关于 JVM 堆外内存 Java 中的对象都是在 JVM 堆中分配的，其好处在于开发者不用关心对象的回收 但有利必有弊，堆内内存主要有两个缺点 GC 是有成本的，堆中的对象数量越多，GC 的开销也会越大 使用堆内内存进行文件、网络的 IO 时，JVM 会使用堆外内存做一次额外的中转，也就是会多一次内存拷贝 和堆内内存相对应，堆外内存就是把内存对象分配在 Java 虚拟机堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机），这样做的结果就是能够在一定程度上减少垃圾回收对应用程序造成的影响 堆外内存的实现 (DirectByteBuffer) Java 中分配堆外内存的方式有两种 一是通过 ByteBuffer.java#allocateDirect 得到以一个 DirectByteBuffer 对象 二是直接调用 Unsafe.java#allocateMemory 分配内存，但 Unsafe 只能在 JDK 的代码中调用，一般不会直接使用该方法分配内存 其中 DirectByteBuffer 也是用 Unsafe 去实现内存分配的，对堆内存的分配、读写、回收都做了封装 堆外内存的分配与回收123456789101112131415161718192021222324252627282930313233343536373839// ByteBuffer.java public static ByteBuffer allocateDirect(int capacity) &#123; return new DirectByteBuffer(capacity);&#125;// DirectByteBuffer.java DirectByteBuffer(int cap) &#123; // package-private // 主要是调用 ByteBuffer 的构造方法，为字段赋值 super(-1, 0, cap, cap); // 如果是按页对齐，则还要加一个 Page 的大小；我们分析只 pa 为 false 的情况就好了 boolean pa = VM.isDirectMemoryPageAligned(); int ps = Bits.pageSize(); long size = Math.max(1L, (long)cap + (pa ? ps : 0)); // 预分配内存 Bits.reserveMemory(size, cap); long base = 0; try &#123; // 分配内存 base = unsafe.allocateMemory(size); &#125; catch (OutOfMemoryError x) &#123; Bits.unreserveMemory(size, cap); throw x; &#125; // 将分配的内存的所有值赋值为 0 unsafe.setMemory(base, size, (byte) 0); // 为 address 赋值，address 就是分配内存的起始地址，之后的数据读写都是以它作为基准 if (pa &amp;&amp; (base % ps != 0)) &#123; // Round up to page boundary address = base + ps - (base &amp; (ps - 1)); &#125; else &#123; // pa 为 false 的情况，address == base address = base; &#125; // 创建一个 Cleaner，将 this 和一个 Deallocator 对象传进去 cleaner = Cleaner.create(this, new Deallocator(base, size, cap)); att = null;&#125; DirectByteBuffer 构造方法分为几个步骤 预分配内存 分配内存 将刚分配的内存空间初始化为 0 创建一个 Cleaner 对象，Cleaner 对象的作用是当 DirectByteBuffer 对象被回收时，释放其对应的堆外内存 当 GC 发现 DirectByteBuffer 对象变成垃圾时，会调用 Cleaner#clean 回收对应的堆外内存，一定程度上防止了内存泄露 当然也可以手动的调用该方法，对堆外内存进行提前回收 Cleaner 的实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class Cleaner extends PhantomReference&lt;Object&gt; &#123; ... private Cleaner(Object referent, Runnable thunk) &#123; super(referent, dummyQueue); this.thunk = thunk; &#125; public void clean() &#123; if (remove(this)) &#123; try &#123; // thunk 是一个 Deallocator 对象 this.thunk.run(); &#125; catch (final Throwable var2) &#123; ... &#125; &#125; &#125;&#125;private static class Deallocator implements Runnable &#123; private static Unsafe unsafe = Unsafe.getUnsafe(); private long address; private long size; private int capacity; private Deallocator(long address, long size, int capacity) &#123; assert (address != 0); this.address = address; this.size = size; this.capacity = capacity; &#125; public void run() &#123; if (address == 0) &#123; // Paranoia return; &#125; // 调用 unsafe 方法回收堆外内存 unsafe.freeMemory(address); address = 0; Bits.unreserveMemory(size, capacity); &#125; &#125; 当字段 referent (也就是 DirectByteBuffer 对象)被回收时，会调用到 Cleaner#clean 方法，最终会调用到 Deallocator#run 进行堆外内存的回收 Cleaner 是虚引用在 JDK 中的一个典型应用场景 预分配内存1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374static void reserveMemory(long size, int cap) &#123; // maxMemory 代表最大堆外内存，也就是 -XX:MaxDirectMemorySize 指定的值 if (!memoryLimitSet &amp;&amp; VM.isBooted()) &#123; maxMemory = VM.maxDirectMemory(); memoryLimitSet = true; &#125; // 1.如果堆外内存还有空间，则直接返回 if (tryReserveMemory(size, cap)) &#123; return; &#125; // 走到这里说明堆外内存剩余空间已经不足了 final JavaLangRefAccess jlra = SharedSecrets.getJavaLangRefAccess(); // 2.堆外内存进行回收，最终会调用到 Cleaner#clean 的方法。如果目前没有堆外内存可以回收则跳过该循环 while (jlra.tryHandlePendingReference()) &#123; // 如果空闲的内存足够了，则 return if (tryReserveMemory(size, cap)) &#123; return; &#125; &#125; // 3.主动触发一次 GC，目的是触发老年代 GC System.gc(); // 4.重复上面的过程 boolean interrupted = false; try &#123; long sleepTime = 1; int sleeps = 0; while (true) &#123; if (tryReserveMemory(size, cap)) &#123; return; &#125; if (sleeps &gt;= MAX_SLEEPS) &#123; break; &#125; if (!jlra.tryHandlePendingReference()) &#123; try &#123; Thread.sleep(sleepTime); sleepTime &lt;&lt;= 1; sleeps++; &#125; catch (InterruptedException e) &#123; interrupted = true; &#125; &#125; &#125; // 5.超出指定的次数后，还是没有足够内存，则抛异常 throw new OutOfMemoryError("Direct buffer memory"); &#125; finally &#123; if (interrupted) &#123; // don't swallow interrupts Thread.currentThread().interrupt(); &#125; &#125; &#125; private static boolean tryReserveMemory(long size, int cap) &#123; // size 和 cap 主要是 page 对齐的区别，这里我们把这两个值看作是相等的 long totalCap; // totalCapacity 代表通过 DirectByteBuffer 分配的堆外内存的大小 // 当已分配大小 &lt;= 还剩下的堆外内存大小 时，更新 totalCapacity 的值返回 true while (cap &lt;= maxMemory - (totalCap = totalCapacity.get())) &#123; if (totalCapacity.compareAndSet(totalCap, totalCap + cap)) &#123; reservedMemory.addAndGet(size); count.incrementAndGet(); return true; &#125; &#125; // 堆外内存不足，返回 false return false; &#125; 在创建一个新的 DirecByteBuffer 时，会先确认有没有足够的内存，如果没有的话，会通过一些手段回收一部分堆外内存，直到可用内存大于需要分配的内存。具体步骤如下： 如果可用堆外内存足够，则直接返回 调用 tryHandlePendingReference 方法回收已经变成垃圾的 DirectByteBuffer 对象对应的堆外内存，直到可用内存足够，或目前没有垃圾 DirectByteBuffer 对象 tryHandlePendingReference 最终调用到的是 Reference#tryHandlePending 方法 此方法在前面有介绍过, 对于 Cleaner 对象调用对应的 Cleaner#clean 方法进行回收 触发一次 Full GC, 其主要目的是为了防止冰山现象 一个 DirectByteBuffer 对象本身占用的内存很小，但是它可能引用了一块很大的堆外内存 如果 DirectByteBuffer 对象进入了老年代之后变成了垃圾，因为老年代 GC 一直没有触发，导致这块堆外内存也一直没有被回收 需要注意的是如果使用参数 -XX:+DisableExplicitGC，那 System.gc(); 是无效的 重复 1，2 步骤的流程，直到可用内存大于需要分配的内存 如果超出指定次数还没有回收到足够内存，则 OOM 堆外内存的读写123456789101112131415161718192021222324public ByteBuffer put(byte x) &#123; unsafe.putByte(ix(nextPutIndex()), ((x))); return this;&#125;final int nextPutIndex() &#123; if (position &gt;= limit) throw new BufferOverflowException(); return position++;&#125;private long ix(int i) &#123; return address + ((long)i &lt;&lt; 0);&#125;public byte get() &#123; return ((unsafe.getByte(ix(nextGetIndex()))));&#125;final int nextGetIndex() &#123; // package-private if (position &gt;= limit) throw new BufferUnderflowException(); return position++;&#125; 读写的逻辑比较简单，address 就是构造方法中分配的 native 内存的起始地址 Unsafe 的 putByte/getByte 都是 native 方法，就是写入值到某个地址/获取某个地址的值 堆外内存的使用场景 适合长期存在或能复用的场景, 堆外内存分配回收也是有开销的，所以适合长期存在的对象 适合注重稳定的场景, 堆外内存能有效避免因 GC 导致的暂停问题 堆外内存能有效避免因GC导致的暂停问题。 适合简单对象的存储, 因为堆外内存只能存储字节数组，所以对于复杂的 DTO 对象，每次存储/读取都需要序列化/反序列化 适合注重 IO 效率的场景, 用堆外内存读写文件性能更好 文件IO 堆外内存 IO 为什么有更好的性能 BIO BIO 的文件写 FileOutputStream#write 最终会调用到 native 层的 io_util.c#writeBytes 方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455voidwriteBytes(JNIEnv *env, jobject this, jbyteArray bytes, jint off, jint len, jboolean append, jfieldID fid)&#123; jint n; char stackBuf[BUF_SIZE]; char *buf = NULL; FD fd; ... // 如果写入长度为 0，直接返回 0 if (len == 0) &#123; return; &#125; else if (len &gt; BUF_SIZE) &#123; // 如果写入长度大于 BUF_SIZE（8192），无法使用栈空间 buffer // 需要调用 malloc 在堆空间申请 buffer buf = malloc(len); if (buf == NULL) &#123; JNU_ThrowOutOfMemoryError(env, NULL); return; &#125; &#125; else &#123; buf = stackBuf; &#125; // 复制 Java 传入的 byte 数组数据到 C 空间的 buffer 中 (*env)-&gt;GetByteArrayRegion(env, bytes, off, len, (jbyte *)buf); if (!(*env)-&gt;ExceptionOccurred(env)) &#123; off = 0; while (len &gt; 0) &#123; fd = GET_FD(this, fid); if (fd == -1) &#123; JNU_ThrowIOException(env, "Stream Closed"); break; &#125; // 写入到文件，这里传递的数组是我们新创建的 buf if (append == JNI_TRUE) &#123; n = (jint)IO_Append(fd, buf+off, len); &#125; else &#123; n = (jint)IO_Write(fd, buf+off, len); &#125; if (n == JVM_IO_ERR) &#123; JNU_ThrowIOExceptionWithLastError(env, "Write error"); break; &#125; else if (n == JVM_IO_INTR) &#123; JNU_ThrowByName(env, "java/io/InterruptedIOException", NULL); break; &#125; off += n; len -= n; &#125; &#125;&#125; GetByteArrayRegion 其实就是对数组进行了一份拷贝，该函数的实现在 jni.cpp 宏定义中 12345678910111213// jni.cppJNI_ENTRY(void, \jni_Get##Result##ArrayRegion(JNIEnv *env, ElementType##Array array, jsize start, \ jsize len, ElementType *buf)) \ ... int sc = TypeArrayKlass::cast(src-&gt;klass())-&gt;log2_element_size(); \ // 内存拷贝 memcpy((u_char*) buf, \ (u_char*) src-&gt;Tag##_at_addr(start), \ len &lt;&lt; sc); \... &#125; \JNI_END 传统的 BIO，在 native 层真正写文件前，会在堆外内存（c 分配的内存）中对字节数组拷贝一份，之后真正 IO 时，使用的是堆外的数组, 这样做的原因是: 底层通过 write、read、pwrite，pread 函数进行系统调用时，需要传入 buffer 的起始地址和 buffer count 作为参数 如果使用 Java Heap 的话，我们知道 JVM 中 buffer 往往以 byte[] 的形式存在，这是一个特殊的对象，由于 Java Heap GC 的存在，这里对象在堆中的位置往往会发生移动，移动后我们传入系统函数的地址参数就不是真正的 buffer 地址了，这样的话无论读写都会发生出错。而 C Heap 仅仅受 Full GC 的影响，相对来说地址稳定 JVM 规范中没有要求 Java 的 byte[] 必须是连续的内存空间，它往往受宿主语言的类型约束 而 C Heap 中我们分配的虚拟地址空间是可以连续的，而上述的系统调用要求我们使用连续的地址空间作为 buffer NIO NIO 的文件写最终会调用到 IOUtil#write 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051static int write(FileDescriptor fd, ByteBuffer src, long position, NativeDispatcher nd, Object lock) throws IOException &#123; // 如果是堆外内存，则直接写 if (src instanceof DirectBuffer) return writeFromNativeBuffer(fd, src, position, nd, lock); // Substitute a native buffer int pos = src.position(); int lim = src.limit(); assert (pos &lt;= lim); int rem = (pos &lt;= lim ? lim - pos : 0); // 创建一块堆外内存，并将数据赋值到堆外内存中去 ByteBuffer bb = Util.getTemporaryDirectBuffer(rem); try &#123; bb.put(src); bb.flip(); // Do not update src until we see how many bytes were written src.position(pos); int n = writeFromNativeBuffer(fd, bb, position, nd, lock); if (n &gt; 0) &#123; // now update src src.position(pos + n); &#125; return n; &#125; finally &#123; Util.offerFirstTemporaryDirectBuffer(bb); &#125; &#125; /** * 分配一片堆外内存 */ static ByteBuffer getTemporaryDirectBuffer(int size) &#123; BufferCache cache = bufferCache.get(); ByteBuffer buf = cache.get(size); if (buf != null) &#123; return buf; &#125; else &#123; // No suitable buffer in the cache so we need to allocate a new // one. To avoid the cache growing then we remove the first // buffer from the cache and free it. if (!cache.isEmpty()) &#123; buf = cache.removeFirst(); free(buf); &#125; return ByteBuffer.allocateDirect(size); &#125; &#125; NIO 的文件写，对于堆内内存来说也是会有一次额外的内存拷贝的 参考 https://github.com/farmerjohngit/myblog/issues/3 https://zhuanlan.zhihu.com/p/102758471 https://my.oschina.net/dust8080/blog/3094511 https://github.com/farmerjohngit/myblog/issues/10]]></content>
  </entry>
  <entry>
    <title><![CDATA[Java 线程同步与锁]]></title>
    <url>%2F2020%2F06%2F16%2FJava-%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E4%B8%8E%E9%94%81%2F</url>
    <content type="text"><![CDATA[Java 线程同步与锁锁的相关概念按照其性质分类公平锁, 非公平锁公平锁 多个线程按照申请锁的顺序来获取锁 优点: 等待锁的线程不会长时间获取不到锁 缺点: 吞吐效率不高, 因为还要对顺序进行判断 非公平锁 多个线程随机获取锁 优点: 吞吐效率高 缺点: 等待锁的线程可能长时间获取不到锁 悲观锁, 乐观锁悲观锁 认为在使用数据的时候一定有别的线程来修改数据, 因此在获取数据的时候会先加锁, 确保数据不会被别的线程修改 适合写操作多的场景, 先加锁可以保证写操作时数据正确 synchronized 关键字和 Lock 的实现类都是悲观锁 乐观锁 认为在使用数据时不会有别的线程修改数据, 所以不会添加锁, 只在更新数据的时候去判断之前有没有别的线程更新了这个数据 如果数据已经被其他线程更新, 则根据不同的实现方式执行不同的操作（例如报错或者自动重试） 适合读操作多的场景, 避免读的时候被阻塞 在 Java 中是通过使用无锁编程来实现, 最常采用的是 CAS 算法, Java 原子类中的递增操作就通过 CAS 自旋实现的 CAS 全称 Compare And Swap (比较与交换) CAS 涉及到三个操作数 需要读写的内存值 V 进行比较的值 A 要写入的新值 B 当且仅当 V 的值等于 A 时, CAS 通过原子方式用新值 B 来更新 V 的值（“比较+更新”整体是一个原子操作） 存在的问题 ABA 问题 在 CAS 操作时, 其他线程将变量值 A 改为了 B, 但是又被改回了 A, 等到本线程使用期望值 A 与当前变量进行比较时, 发现变量 A 没有变, 于是 CAS 就将 A 值进行了交换操作, 但是实际上该值已经被其他线程改变过 解决思路是在变量前面添加版本号, 每次变量更新的时候都把版本号加一, 这样变化过程就从 “A－B－A” 变成了 “1A－2B－3A” JDK 从 1.5 开始提供了 AtomicStampedReference 类来解决 ABA 问题, 具体操作封装在 compareAndSet() 中 compareAndSet() 首先检查当前引用和当前标志与预期引用和预期标志是否相等, 如果都相等, 则进行 CAS 循环时间长开销大 CAS 操作如果长时间不成功, 会导致其一直自旋, 给 CPU 带来非常大的开销 只能保证一个共享变量的原子操作 JDK 从 1.5 开始提供了 AtomicReference 类来保证引用对象之间的原子性, 可以把多个变量放在一个对象里来进行 CAS 操作 可重入锁 在同一个线程在获取锁之后, 再次获取相同的锁时, 不会因为之前已经获取过还没释放而阻塞 Java 中 ReentrantLock 和 synchronized 都是可重入锁 优点: 可一定程度避免死锁 独享锁 (排他锁), 共享锁, 互斥锁, 读写锁 独享锁 (排他锁), 共享锁是一种广义的说法, 互斥锁, 读写锁是具体的实现 独享锁 该锁一次只能被一个线程所持有 获得独享锁的线程即能读数据又能修改数据 JDK 中的 synchronized 和 JUC 中 Lock 的实现类就是互斥锁 共享锁 该锁可被多个线程所持有 获得共享锁的线程只能读数据, 不能修改数据 JUC 中 ReadWriteLock 的实现类其读锁是共享锁, 其写锁是独享锁 优点: 可保证并发读, 提高读的效率 按照设计方案来分类自旋锁, 适应性自旋锁自旋锁 指尝试获取锁的线程不会立即阻塞, 而是采用循环的方式去尝试获取锁 优点: 减少线程上下文切换的消耗 缺点: 循环会一直占用 CPU 适应性自旋锁 自适应意味着自旋的时间（次数）不再固定, 而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定 锁粗化, 锁消除 锁粗化和锁消除是虚拟机即时编译器 (JIT) 对锁的优化 锁粗化 如果一系列的连续操作都对同一个对象反复加锁和解锁, 甚至加锁操作是出现在循环体中的 即使没有线程竞争, 频繁地进行互斥同步操作也会导致不必要的性能损耗 如果 JIT 探测到有这样的操作, 将会把加锁同步的范围扩展（粗化）到整个操作序列的外部 锁消除 JIT 在运行时检测到某些代码上要求同步, 但是不可能存在锁竞争, 会对其进行消除 主要判定依据来源于逃逸分析的数据支持, 如果判断在一段代码中, 堆上的所有数据都不会逃逸出去从而被其他线程访问到, 那就可以把它们当做栈上数据对待, 认为它们是线程私有的, 同步加锁自然就无须进行 分段锁 分段锁是一种锁的设计, 并不是具体的一种锁 ConcurrentHashMap 就是用分段锁实现的 内部持有一个 Entry 数组, 数组中的每个元素又是一个链表, 同时又是一个锁 (ReentrantLock) 当需要 put 元素的时候, 并不是对整个 hashmap 进行加锁, 而是先通过 hashcode 来知道他要放在那一个分段中, 然后对这个分段进行加锁, 所以当多线程 put 的时候, 只要不是放在一个分段中, 可以并行插入 在统计 size 的时候, 因为是获取全局信息, 所以需要获取所有的分段锁才能统计 无锁, 偏向锁, 轻量级锁, 重量级锁 这四种锁是指锁的状态, 在 JDK 1.6 中引入针对 synchronized 的锁优化 级别从低到高依次是：无锁、偏向锁、轻量级锁和重量级锁 锁状态只能升级不能降级 ( JVM中的锁也是能降级的, 不过条件很苛刻 ) 无锁 无锁没有对资源进行锁定, 所有的线程都能访问并修改同一个资源, 但同时只有一个线程能修改成功 CAS 原理及应用即是无锁的实现 偏向锁 偏向锁是指一段同步代码一直被一个线程所访问, 那么该线程会自动获取锁, 降低获取锁的代价 直到另一个线程尝试获取此锁的时候, 偏向锁模式才会结束 偏向锁可以提高带有同步但无竞争的程序性能, 但如果在多数锁总会被不同的线程访问时, 偏向锁模式就比较多余 偏向锁在 JDK 1.6 以上默认开启 可以通过 JVM 参数关闭偏向锁：-XX:-UseBiasedLocking=false, 关闭之后程序默认会进入轻量级锁状态 当 JVM 认为存在多线程竞争时, 会将偏向锁升级为轻量级锁 轻量级锁 轻量级锁作用于不同的线程交替的执行同步块中的代码, 不存在锁竞争的情况 只要存在锁竞争, 轻量级锁就会升级为重量级锁 重量级锁 重量级锁会让其他申请的线程进入阻塞, 性能降低 思考: 如何实现锁 如果要实现操作系统的锁, 该如何实现？先暂时不考虑性能、可用性等问题, 从最简单粗暴的方式开始思考, 以下都为伪代码 自旋123456789101112131415volatile int status = 0;void lock() &#123; while(!compareAndSet(0, 1)) &#123; &#125; // get lock&#125;void unlock() &#123; status = 0;&#125;boolean compareAndSet(int except, int newValue) &#123; // CAS 操作, 修改 status 成功则返回 true&#125; 上面的代码通过自旋和 CAS 来实现一个最简单的锁 这样实现的锁显然有个致命的缺点：耗费 CPU 资源, 没有竞争到锁的线程会一直占用 CPU 资源进行 CAS 操作 yield + 自旋 要解决自旋锁的性能问题必须让竞争锁失败的线程不忙等, 而是在获取不到锁的时候能把 CPU 资源给让出来, 说到让 CPU 资源, 可能想到了 yield() 方法 123456void lock() &#123; while(!compareAndSet(0, 1)) &#123; yield(); &#125; // get lock&#125; yield() 方法并没有完全解决问题 yield() 不一定能成功让出 CPU, 还跟线程的优先级有关 如果有 100 个线程竞争锁, 当线程 1 获得锁后, 还有 99 个线程在反复的自旋 + yield, 假如运行在单核 CPU 下, 在竞争锁时最差只有 1% 的 CPU 利用率, 导致获得锁的线程 1 一直被中断, 执行实际业务代码时间变得更长, 从而导致锁释放的时间变的更长 sleep + 自旋 当竞争锁失败后, 可以将用 Thread.sleep 将线程休眠, 从而不占用 CPU 资源 123456void lock() &#123; while(!compareAndSet(0, 1)) &#123; sleep(10); &#125; // get lock&#125; 通常用于实现上层锁, 不适合用于操作系统级别的锁, 因为作为一个底层锁, 其 sleep 时间很难设置 sleep 的时间取决于同步代码块的执行时间 sleep 时间如果太短了, 会导致线程切换频繁 (极端情况和 yield 方式一样) sleep 时间如果设置的过长, 会导致线程不能及时获得锁 park + 自旋 那可不可以在获取不到锁的时候让线程释放 CPU 资源进行等待, 当持有锁的线程释放锁的时候将等待的线程唤起呢？ 12345678910111213141516171819202122232425262728volatile int status = 0;Queue parkQueue;void lock() &#123; while(!compareAndSet(0, 1)) &#123; lock_wait(); &#125; // get lock&#125;void synchronized unlock() &#123; lock_notify();&#125;void lock_wait() &#123; // 将当期线程加入到等待队列 parkQueue.add(nowThread); // 将当期线程释放CPU releaseCPU();&#125;void lock_notify() &#123; // 得到要唤醒的线程 Thread t = parkQueue.poll(); // 唤醒等待线程 wakeAThread(t);&#125; 这种方案相比于 sleep 而言, 只有在锁被释放的时候, 竞争锁的线程才会被唤醒, 不会存在过早或过晚唤醒的问题 对于锁冲突不严重的情况, 用自旋锁会更适合 试想每个线程获得锁后很短的一段时间内就释放锁, 竞争锁的线程只要经历几次自旋运算后就能获得锁, 那就没必要等待该线程了 因为等待线程意味着需要进入到内核态进行上下文切换, 而上下文切换的成本不低, 如果锁很快就释放了, 那上下文切换的开销将超过自旋 目前操作系统中, 一般是用自旋+等待结合的形式实现锁：在进入锁时先自旋一定次数, 如果还没获得锁再进行等待 linux 如何实现锁 linux 底层用 futex 实现锁 futex 由一个内核层的队列和一个用户空间层的 atomic integer 构成 当获得锁时, 尝试 CAS 更改 integer, 如果 integer 原始值是 0, 则修改成功, 该线程获得锁 否则就将当前线程放入到 wait queue中（即操作系统的等待队列） futex 诞生之前 在 futex 诞生之前, linux 下的同步机制可以归为两类 用户态的同步机制 基本上就是利用原子指令实现的自旋锁 (代码层的 CAS) 关于自旋锁其缺点也说过了, 不适用于大的临界区（即锁占用时间比较长的情况） 内核同步机制 如 semaphore (信号量) 等, 使用的是上文说的自旋+等待的形式 它对于大小临界区和都适用 但是因为它是内核层的 (释放 CPU 资源是内核级调用), 所以每次 lock 与 unlock 都是一次系统调用, 即使没有锁冲突, 也必须要通过系统调用进入内核之后才能识别 理想的同步机制应该是没有锁冲突时在用户态利用原子指令就解决问题, 而需要挂起等待时再使用内核提供的系统调用进行睡眠与唤醒。换句话说, 在用户态的自旋失败时, 能不能让进程挂起, 由持有锁的线程释放锁时将其唤醒？ 可能会想出以下代码 12345678910111213141516171819202122void lock(int lockval) &#123; // trylock 是用户级的自旋锁 while(!trylock(lockval)) &#123; wait(); // 释放 CPU, 并将当期线程加入等待队列, 是系统调用 &#125;&#125;boolean trylock(int lockval) &#123; int i = 0; // localval = 1 代表上锁成功 while(!compareAndSet(lockval, 0, 1)) &#123; if(++i &gt; 10) &#123; return false; &#125; &#125; return true;&#125;void unlock(int lockval) &#123; compareAndSet(lockval, 1, 0); notify();&#125; 上述代码的问题是 trylock 和 wait 两个调用之间存在一个窗口, 如果一个线程 trylock 失败后在调用 wait 前, 持有锁的线程释放了锁, 则该线程执行完 wait 后就无人唤醒了 futex 诞生之后 我们来看看 futex 的方法定义 12345// uaddr 指向一个地址, val 代表这个地址期待的值, 当 *uaddr == val 时, 才会进行 waitint futex_wait(int *uaddr, int val);// 唤醒 n 个在 uaddr 指向的锁变量上挂起等待的进程int futex_wake(int *uaddr, int n); futex_wait 真正将进程挂起之前会检查 uaddr 指向的地址的值是否等于 val, 如果不相等则会立即返回, 由用户态继续 trylock, 否则将当前线程插入到一个队列中去并挂起 futex_wait 检查 uaddr 的值前会获取自旋锁, 将当前线程插入等待队列后释放, 最后再挂起线程, 保证条件与等待之间的原子性 futex 内部维护了一个队列, 在线程挂起前会线程插入到其中, 同时对于队列中的每个节点都有一个标识, 代表该线程关联锁的uaddr。这样当用户态调用 futex_wake 时, 只需要遍历这个等待队列, 把带有相同 uaddr 的节点所对应的进程唤醒就行了 作为优化, futex 维护的其实是个类似 Java 中的 ConcurrentHashMap 的结构, 也就是数组加链表的形式 其持有一个总链表, 总链表中每个元素都是一个带有自旋锁的子链表 调用 futex_wait 挂起的进程, 通过其 uaddr hash 放到某一个具体的子链表上去 这样一方面能分散对等待队列的竞争、另一方面减小单个队列的长度, 便于 futex_wake 时的查找 每个链表各自持有一把spinlock, 将 *uaddr 和 val 的比较操作 与 把进程加入队列的操作 保护在一个临界区中 futex 是支持多进程的, 当使用 futex 在多进程间进行同步时, 需要考虑同一个物理内存地址在不同进程中的虚拟地址是不同的 Java 如何实现锁synchronized 在 JDK 1.6 中引入针对 synchronized 进行了锁优化, 分为无锁、偏向锁、轻量级锁和重量级锁四种状态 在前面锁的概念中有介绍各个锁的使用场景 对象头 在 Java 中任意对象都可以用作锁, 因此必定要有一个映射关系, 存储该对象以及其对应的锁信息（比如当前哪个线程持有锁, 哪些线程在等待） 在 JVM 中, 对象在内存中除了本身的数据外还会有个对象头 对于普通对象而言, 其对象头中有两类信息：mark word 和类型指针 mark word 用于存储对象的 HashCode、GC分代年龄、锁状态等信息 在 32 位系统上 mark word 长度为 32bit, 64 位系统上长度为 64bit 为了能在有限的空间里存储下更多的数据, 其存储格式是不固定的, 在 32 位系统上各状态的格式如下： 类型指针是指向该对象所属类对象的指针 对于数组而言还会有一份记录数组长度的数据 重量级锁 重量级锁是我们常说的传统意义上的锁, 其利用操作系统底层的同步机制去实现 Java 中的线程同步 重量级锁的状态下, 对象的 mark word 为指向一个堆中 monitor 对象的指针 关于什么是 monitor, 在 &lt;&lt; Java 线程与线程池&gt;&gt; 中有相关深入分析 当调用一个锁对象的 wait 或 notify 方法时, 如当前锁的状态是偏向锁或轻量级锁则会先膨胀成重量级锁 轻量级锁 网上很多文章说轻量级锁有自旋, 这在源码中是不存在的, 只有重量级锁获取失败才会自旋 线程在执行同步块之前, JVM 会先在当前的线程的栈帧中创建一个 Lock Record, 其包括一个用于存储对象头中的 mark word（官方称之为 Displaced Mark Word）以及一个指向对象的指针。下图右边的部分就是一个Lock Record Lock Record 是从高往低创建的 加锁过程 在线程栈中创建一个 Lock Record, 将其 obj（即上图的 Object reference）字段指向锁对象 直接通过 CAS 将 Lock Record 的地址存储在对象头的 mark word 中 如果对象处于无锁状态则修改成功, 代表该线程获得了轻量级锁 如果失败, 需要判断是否为当前线程的锁重入 是, 则设置 Lock Record 第一部分（Displaced Mark Word）为 null, 起到了一个重入计数器的作用 否, 则说明发生了竞争, 需要膨胀为重量级锁 解锁过程 从低往高遍历线程栈, 找到所有 obj 字段等于当前锁对象的 Lock Record 如果 Lock Record 的 Displaced Mark Word 为 null, 代表是重入的解锁, 将 obj 设置为 null 如果 Lock Record 的 Displaced Mark Word 不为 null, 则利用 CAS 将对象头的 mark word 恢复成为 Displaced Mark Word 如果失败, 则膨胀为重量级锁 偏向锁 在 JDK 1.6 以上默认开启 在程序启动后, 通常有几秒的延迟, 可以通过 -XX:BiasedLockingStartupDelay=0 来关闭延迟 当调用锁对象的 Object#hash 或 System.identityHashCode() 方法会导致该对象的偏向锁升级 因为对象的 hashcode 是在调用这两个方法时才生成的 如果是无锁状态则存放在 mark word 中 如果是重量级锁则存放在对应的 monitor 中 而偏向锁没有地方能存放该信息, 所以必须升级 对象创建 当新创建一个对象的时候, 如果该对象所属的 class 没有关闭偏向锁模式（默认开启）, 那新创建对象的 mark word 将是可偏向状态, 此时 mark word 中的 thread id（参见上文偏向状态下的 mark word 格式）为 0, 表示未偏向任何线程, 也叫做匿名偏向 (anonymously biased) 加锁过程 当该对象第一次被线程获得锁的时候, 发现是匿名偏向状态, 则会用 CAS 将 mark word 中的 thread id 由 0 改成当前线程 Id 如果失败, 说明存在另个线程使用锁, 将偏向锁撤销, 升级为轻量级锁 当被偏向的线程再次进入同步块时, 发现锁对象偏向的就是当前线程, 在通过一些额外的检查后, 会往当前线程的栈中添加一条 Displaced Mark Word 为 null 的 Lock Record , 然后继续执行同步块的代码, 因为操纵的是线程私有的栈, 因此不需要用到 CAS 指令 当其他线程进入同步块时, 发现已经有偏向的线程了, 则会进入撤销偏向锁的逻辑 一般来说, 会在 safepoint 中去查看偏向的线程是否还存活 如果存活且还在同步块中则将锁升级为轻量级锁, 原偏向的线程继续拥有锁, 当前线程则走入到锁升级的逻辑里 如果偏向的线程已经不存活或者不在同步块中, 则将对象头的 mark word 改为无锁状态（unlocked）, 之后再升级为轻量级锁 解锁过程 当有其他线程尝试获得锁时, 是根据遍历偏向线程的 Lock Record 来确定该线程是否还在执行同步块中的代码。因此将栈中的最近一条 Lock Record 的 obj 字段设置为 null 即可 需要注意的是, 偏向锁的解锁步骤中并不会修改对象头中的 thread id 批量重偏向与撤销 重偏向指将已偏向的锁对象头 thread id 指向新的线程 id 撤销偏向指将锁的偏向的状态撤销为无锁状态 从上文提到当有其他线程尝试获得锁时, 需要等到 safepoint 时将偏向锁撤销为无锁状态或升级为轻量级/重量级锁 safe point 在 GC 中经常提到, 其代表了一个状态, 在该状态下所有线程都是暂停的, 详细可以看这篇文章 如果运行时存在多线程竞争, 那偏向锁的存在不仅不能提高性能, 而且会导致性能下降 因此 JVM 中增加了一种批量重偏向/撤销的机制, 存在如下两种情况：（见官方论文第4小节） 一个线程创建了大量对象并执行了初始的同步操作, 之后在另一个线程中将这些对象作为锁进行之后的操作。这种情况下会导致大量的偏向锁撤销操作 存在明显多线程竞争的场景下使用偏向锁是不合适的, 例如生产者/消费者队列 批量重偏向（bulk rebias）机制是为了解决第一种场景, 批量撤销（bulk revoke）则是为了解决第二种场景 其做法是：以 class 为单位, 为每个 class 维护一个偏向锁撤销计数器 每一次该 class 的对象发生偏向撤销操作时, 该计数器 +1 当计数器达到重偏向阈值（默认20）时, JVM 就认为该 class 的偏向锁有问题, 因此会进行批量重偏向 每个 class 还有一个 epoch 字段, 每个处于偏向锁状态对象的 mark word 中也有该字段 其初始值为创建该对象时 class 中的 epoch 的值 每次发生批量重偏向时, 就将该值 +1, 同时遍历 JVM 中所有线程的栈, 找到该 class 所有正处于加锁状态的偏向锁, 将其 epoch 字段改为新值 下次获得锁时, 发现当前对象的 epoch 值和 class 的 epoch 不相等, 那就算当前已经偏向了其他线程, 也不会执行撤销操作, 而是直接通过 CAS 操作将其 mark word 的 thread id 改成当前线程 id 当达到重偏向阈值后, 假设该 class 计数器继续增长, 当其达到批量撤销的阈值后（默认40）, JVM就认为该 class 的使用场景存在多线程竞争, 会标记该 class 为不可偏向, 之后对于该 class 的锁, 直接走轻量级锁的逻辑 锁状态转换流程 AQS AQS 是类 AbstractQueuedSynchronizer 的简称, Java 中的大部分同步类 (Lock、Semaphore、ReentrantLock等) 都是基于 AQS 实现的 AQS 是一种提供了原子式管理同步状态、阻塞和唤醒线程功能以及队列模型的简单框架 大致原理如下 AQS 维护一个 state 变量和一个双向链表, state 用来表示同步状态, 双向链表存储的是等待锁的线程 加锁时首先调用 tryAcquire 尝试获得锁, 如果获得锁失败, 则将线程插入到双向链表中, 并调用 LockSupport.park() 方法阻塞当前线程 释放锁时调用 LockSupport.unpark() 唤起链表中的第一个节点的线程, 被唤起的线程会重新走一遍竞争锁的流程 架构图 上图中有颜色的为方法, 无颜色的为属性 总的来说, AQS 框架共分为五层, 自上而下由浅入深, 从 AQS 对外暴露的 API 到底层基础数据 当有自定义同步器接入时, 只需重写第一层所需要的部分方法即可, 不需要关注底层具体的实现流程 当自定义同步器进行加锁或者解锁操作时, 先经过第一层的 API 进入 AQS 内部方法, 然后经过第二层进行锁的获取, 接着对于获取锁失败的流程, 进入第三层和第四层的等待队列处理, 而这些处理方式均依赖于第五层的基础数据提供层 NodeNode 是 AQS 双向链表中的节点类型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657static final class Node &#123; // 表示线程以共享的模式等待锁 static final Node SHARED = new Node(); // 表示线程正在以独占的方式等待锁 static final Node EXCLUSIVE = null; // waitStatus 的状态, 表示线程获取锁的请求已经取消了 static final int CANCELLED = 1; // waitStatus 的状态, 表示节点的后继节点处于挂起等待 unpark 的状态 static final int SIGNAL = -1; // waitStatus 的状态, 表示节点正在等待 condition signal static final int CONDITION = -2; // waitStatus 的状态, 当前线程处在 SHARED 情况下, 该字段才会使用 static final int PROPAGATE = -3; // 当前节点在队列中的状态, 当一个 Node 被初始化的时候的值为 0 volatile int waitStatus; // 前驱指针 volatile Node prev; // 后继指针 volatile Node next; // 该节点的线程 volatile Thread thread; // 指向下一个处于 CONDITION 状态的节点 Node nextWaiter; // 该节点是否是共享模式 final boolean isShared() &#123; return nextWaiter == SHARED; &#125; // 返回前驱节点, 没有的话抛出异常 final Node predecessor() throws NullPointerException &#123; Node p = prev; if (p == null) throw new NullPointerException(); else return p; &#125; Node() &#123; // Used to establish initial head or SHARED marker &#125; Node(Thread thread, Node mode) &#123; // Used by addWaiter this.nextWaiter = mode; this.thread = thread; &#125; Node(Thread thread, int waitStatus) &#123; // Used by Condition this.waitStatus = waitStatus; this.thread = thread; &#125;&#125; state1234567891011121314151617181920212223242526public abstract class AbstractQueuedSynchronizer extends AbstractOwnableSynchronizer implements java.io.Serializable &#123; ...... // 0 代表锁未被占用 private volatile int state; // 获取 State 的值 protected final int getState() &#123; return state; &#125; // 设置State的值 protected final void setState(int newState) &#123; state = newState; &#125; // 使用 CAS 方式更新 State protected final boolean compareAndSetState(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, stateOffset, expect, update); &#125; ......&#125; 这几个方法都是 final 修饰的, 说明子类中无法重写它们 我们可以通过修改 state 字段表示的同步状态来实现多线程的独占模式和共享模式（加锁过程） 独占锁通过 state 变量的 0 和 1 两个状态来控制是否有线程占有锁 共享锁通过 state 变量 0 或者非 0 来控制多个线程访问 在 JUC 中的应用 同步工具 同步工具与 AQS 的关联 ReentrantLock 使用 AQS 保存锁重复持有的次数。当一个线程获取锁时, ReentrantLock 记录当前获得锁的线程标识, 用于检测是否重复获取, 以及错误线程试图解锁操作时异常情况的处理 Semaphore 使用 AQS 同步状态来保存信号量的当前计数。tryRelease 会增加计数, acquireShared 会减少计数 CountDownLatch 使用 AQS 同步状态来表示计数。计数为 0 时, 所有的 Acquire 操作（CountDownLatch 的 await 方法）才可以通过 ReentrantReadWriteLock 使用 AQS 同步状态中的 16 位保存写锁持有的次数, 剩下的 16 位用于保存读锁的持有次数 ThreadPoolExecutor Worker 利用 AQS 同步状态实现对独占线程变量的设置（tryAcquire 和 tryRelease） LockSupport AQS 中线程的阻塞和唤醒是通过 LockSupport 来实现的 成员变量 UNSAFE: 用于操作内存和一些底层的指令 parkBlockerOffset: 记录 parkBlocker 在内存中的偏移量 在 Thread 中有如下变量 1234567/** * The argument supplied to the current call to * Java.util.concurrent.locks.LockSupport.park. * Set by (private) Java.util.concurrent.locks.LockSupport.setBlocker * Accessed using Java.util.concurrent.locks.LockSupport.getBlocker */ volatile Object parkBlocker; 这个对象是用来记录线程被阻塞时被谁阻塞的, 用于线程监控和分析工具来定位原因 可以通过 LockSupport 的 getBlocker 获取到阻塞的对象 在 LockSupport 初始化的静态代码块中 12345678910static &#123; try &#123; UNSAFE = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; tk = Thread.class; parkBlockerOffset = UNSAFE.objectFieldOffset (tk.getDeclaredField("parkBlocker")); ...... &#125; catch (Exception ex) &#123; throw new Error(ex); &#125;&#125; 先是通过反射机制获取 Thread 的 parkBlocker 字段对象 然后通过 Unsafe 对象的 objectFieldOffset 方法获取到 parkBlocker 在内存里的偏移量 为什么要用偏移量来获取对象? parkBlocker 是在线程处于阻塞的情况下才会被赋值 如果不通过这种内存的方法, 而是直接调用线程内的方法, 线程是不会回应调用的 park 方法 123456public static void park(Object blocker) &#123; Thread t = Thread.currentThread(); setBlocker(t, blocker); UNSAFE.park(false, 0L); setBlocker(t, null);&#125; park 方法调用了 native 方法 UNSAFE.park 第一个参数表明第二个参数是时间间隔还是时间戳 第二个参数代表最长阻塞时间, 为 0 代表不判断超时 被 park 的线程有三种情况会被唤醒, 但是无法知道是哪种原因 其他线程调用 unpark 唤醒该线程 其他线程调用该线程的中断, 所以唤醒后需要检查中断状态以响应中断 与 Object.wait 一样会有虚假唤醒的情况, 需要配合判断条件在循环中调用 ReentrantLock构造函数123456public ReentrantLock() &#123; sync = new NonfairSync(); // 非公平锁&#125;public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; ReentrantLock 里面有一个内部类 Sync, Sync 继承 AQS, 添加锁和释放锁的大部分操作实际上都是在 Sync 中实现的 ReentrantLock 默认使用非公平锁 加锁过程ReentrantLock.lock : 开始加锁操作 lock() 内部调用了抽象方法 sync.lock() , 需要通过 Sync 子类实现 FairSync 中的 lock() 1234final void lock() &#123; // 进行排队 acquire(1);&#125; NonfairSync 中的 lock() 12345678final void lock() &#123; // 尝试将 state 值由 0 置换为 1 if (compareAndSetState(0, 1)) // 将当前线程设置为此锁的持有者 setExclusiveOwnerThread(Thread.currentThread()); else acquire(1);&#125; 非公平锁比公平锁多了一行 compareAndSetState 方法, 如果设置成功说明当前没有其他线程持有该锁, 否则需要通过 acquire 方法进入等待队列 AQS.acquire acquire 方法位于 AQS 类中 12345public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; // 尝试获取锁 acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) // 获取锁失败, 加入到阻塞队列, 然后不断尝试获取锁 selfInterrupt(); // 如果等待过程出现中断, 恢复中断&#125; tryAcquire : 尝试获取锁并更新 state 先调用 tryAcquire 方法尝试获取锁, 其由子类 NonfairSync 和 FairSync 实现 1234567891011121314151617181920protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; // 如果锁没被占用 // 公平锁比非公平锁多了 !hasQueuedPredecessors() 判断 if (!hasQueuedPredecessors() &amp;&amp; // 查询是否还有等待时间更久的线程 compareAndSetState(0, acquires)) &#123; // 尝试获取锁 setExclusiveOwnerThread(current); // 获取成功, 标记被该线程抢占 return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; // 如果为重入的情况 int nextc = c + acquires; if (nextc &lt; 0) // int 自增溢出为负数的情况 throw new Error("Maximum lock count exceeded"); setState(nextc); // 记录重入次数 return true; &#125; return false;&#125; addWaiter : 获取锁失败后把线程放入等待队列 如果获取锁失败, 调用 addWaiter 方法把线程包装成 Node 对象, 放入到队列尾部, 并返回该节点 123456789101112131415private Node addWaiter(Node mode) &#123; // 根据当前线程和锁模式新建节点 Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) &#123; // 如果尾节点不为空 node.prev = pred; // 新节点的前驱指针指向尾节点 if (compareAndSetTail(pred, node)) &#123; // 将新节点设置为尾节点 pred.next = node; // 旧的尾节点后驱指针指向新的尾节点 return node; &#125; &#125; enq(node); // 将节点插入队列中, 如果必要的话进行初始化 return node;&#125; 如果尾节点为空 (说明队列中没有节点 ), 或者 CAS 失败 (说明已经被别的线程修改), 就需要通过 enq 方法插入节点 123456789101112131415private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; // Must initialize if (compareAndSetHead(new Node())) // 初始化头节点, 头节点不储存任何信息 tail = head; // 尾节点也指向头节点 &#125; else &#123; node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125; 现在再来回看下公平锁 tryAcquire 中调用的 hasQueuedPredecessors 方法 12345678910public final boolean hasQueuedPredecessors() &#123; // The correctness of this depends on head being initialized // before tail and on head.next being accurate if the current // thread is first in queue. Node t = tail; // Read fields in reverse initialization order Node h = head; Node s; return h != t &amp;&amp; ((s = h.next) == null || s.thread != Thread.currentThread());&#125; 双向链表中, head 节点为虚节点, 并不存储任何信息, 只是占位, 真正有数据的节点是在第二个节点开始的 addWaiter 中节点入队时不是原子操作, 所以会出现短暂的 head != tail 情况, 当 h != t 时 如果 (s = h.next) == null, 说明有线程正在初始化队列, 但只是进行到了 tail 指向 head, 没有将 head 指向 tail, 此时队列中有元素, 需要返回 true 如果 (s = h.next) != null, 说明此时队列中至少有一个有效节点, 并且 s.thread == Thread.currentThread(), 则说明等待队列的第一个有效节点中的线程与当前线程相同, 那么当前线程是可以获取资源的 acquireQueued : 队列中的节点不断尝试获取锁 调用 addWaiter 加入到队列后, 再通过 acquireQueued 方法不断去获取锁, 直到获取成功或者不再需要获取（中断） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; // 标记是否成功拿到锁 try &#123; boolean interrupted = false; // 标记线程等待过程中是否中断过 // 开始自旋, 要么获取锁, 要么中断 for (;;) &#123; // 获取前驱节点 final Node p = node.predecessor(); // 前驱节点为头节点时, 说明当前节点在真实数据队列的首部, 有权尝试获取锁 if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); // 获取成功, 将当前节点设置为 head 节点 p.next = null; // help GC // 防止前后互相引用无法回收 failed = false; return interrupted; &#125; // 当前节点不在队列首部, 或者在队列首部但没有获取到锁 (可能是非公平锁被抢占了) if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; // 清理和更新前驱节点状态 parkAndCheckInterrupt()) // 挂起线程 // 线程若被中断, 返回 true interrupted = true; &#125; &#125; finally &#123; if (failed) // 发生异常, 取消获取锁的请求 cancelAcquire(node); &#125;&#125;private void setHead(Node node) &#123; head = node; // 设置该节点不关联任何线程, 也就是虚节点 node.thread = null; node.prev = null;&#125;private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; int ws = pred.waitStatus; if (ws == Node.SIGNAL) // 前驱节点的状态为 SIGNAL, 说明当前线程可以被挂起（阻塞） return true; if (ws &gt; 0) &#123; // 若前驱节点状态为 CANCELLED, 那就一直往前找正常等待状态的节点, 这个过程也在清理被取消的节点 do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); // 找到之后将当前节点排在它后边 pred.next = node; &#125; else &#123; // 判断到这里 waitStatus 只可能是 0 或者 PROPAGATE // 把前驱节点的状态修改为 SIGNAL // 然后 acquireQueued 的循环将会再次 tryAcquire // 确保 tryAcquire 失败后会进入上面的 ws == Node.SIGNAL 判断挂起线程 compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false;&#125;private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); // 挂起 return Thread.interrupted(); // 返回并重置中断状态&#125; 通过 cancelAcquire 方法, 会将 Node 的状态标记为 CANCELLED 1234567891011121314151617181920212223242526272829303132333435363738394041424344private void cancelAcquire(Node node) &#123; // 将无效节点过滤 if (node == null) return; node.thread = null; // 通过前驱节点, 跳过并清理取消状态的节点 Node pred = node.prev; while (pred.waitStatus &gt; 0) node.prev = pred = pred.prev; Node predNext = pred.next; // 把当前节点的状态设置为 CANCELLED node.waitStatus = Node.CANCELLED; // 如果当前节点是尾节点, 则将刚刚找到的正常节点设置为尾节点 if (node == tail &amp;&amp; compareAndSetTail(node, pred)) &#123; // 将 tail 的后继节点指向 null compareAndSetNext(pred, predNext, null); &#125; else &#123; // 当前节点不是尾节点或者是尾节点但 CAS 失败 int ws; // 如果当前节点不是 head 的后继节点 // 1: 判断当前节点前驱节点的状态是否为 SIGNAL // 2: 如果不是, 则尝试把前驱节点状态设置为 SINGAL // 如果 1 和 2 中有一个为 true, 再判断前驱节点的线程是否不为 null // 如果上述条件都满足, 把当前节点的前驱节点的后继指针指向当前节点的后继节点 if (pred != head &amp;&amp; ((ws = pred.waitStatus) == Node.SIGNAL || (ws &lt;= 0 &amp;&amp; compareAndSetWaitStatus(pred, ws, Node.SIGNAL))) &amp;&amp; pred.thread != null) &#123; Node next = node.next; if (next != null &amp;&amp; next.waitStatus &lt;= 0) compareAndSetNext(pred, predNext, next); &#125; else &#123; // 如果当前节点是 head 的后继节点或者上述条件不满足, 那就唤醒当前节点的后继节点 unparkSuccessor(node); &#125; node.next = node; // help GC &#125;&#125; 流程图 解锁过程ReentrantLock.unlock, AQS.release : 开始解锁操作12345678910111213141516public void unlock() &#123; sync.release(1);&#125;public final boolean release(int arg) &#123; // 尝试释放锁 if (tryRelease(arg)) &#123; // 释放成功 Node h = head; // 头结点不为空并且头结点的 waitStatus 不是初始化节点情况, 解除线程挂起状态 if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125; 为什么要 h != null &amp;&amp; h.waitStatus != 0 这样判断 若 h == null, 则头节点还没初始化, 说明第一个节点还没入队 若 h != null &amp;&amp; h.waitStatus == 0 表明还没有进入等待的后继节点, 不需要唤醒 若 h != null &amp;&amp; h.waitStatus &lt; 0 表明后继节点可能被阻塞了, 需要唤醒 不会出现 h != null &amp;&amp; h.waitStatus &gt; 0 的情况, 只有获得了锁的节点才会成为 head Sync.tryRelease : 尝试解锁并更新 state123456789101112131415protected final boolean tryRelease(int releases) &#123; // 减少可重入次数 int c = getState() - releases; // 当前线程不是持有锁的线程, 抛出异常 if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; // 如果持有线程全部释放, 将当前独占锁所有线程设置为 null, 并更新 state if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free;&#125; AQS.unparkSuccessor : 唤醒后继节点12345678910111213141516171819private void unparkSuccessor(Node node) &#123; int ws = node.waitStatus; if (ws &lt; 0) // 将节点状态设置为 0 compareAndSetWaitStatus(node, ws, 0); Node s = node.next; // 如果下个节点是 null 或者下个节点被取消 if (s == null || s.waitStatus &gt; 0) &#123; s = null; // 从尾部节点开始找, 找到队列第一个正常状态的节点。 for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; // s 为慢指针, 当 t 遍历到 node 的时候, s 就指向 node 下一个有效节点 &#125; // 如果找到了正常节点便唤醒 if (s != null) LockSupport.unpark(s.thread);&#125; 为什么要从后往前找第一个正常状态的节点 节点入队不是原子操作, 在 addWaiter 方法中先执行 node.prev = pred; compareAndSetTail(pred, node) 将节点入队, 还没执行 pred.next = node;时, 调用 unparkSuccessor 从前往后是找不到这个节点的 在产生 CANCELLED 状态节点的时候, 先断开的是 next 指针, prev 指针并未断开, 因此也是必须要从后往前遍历才能够遍历完全部的节点 中断恢复后的执行流程 被唤醒的线程处于 parkAndCheckInterrupt 中 1234private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); // 之前在此处挂起 return Thread.interrupted(); // 返回并清除线程的中断状态&#125; 再回到 acquireQueued 代码, 如果这个时候获取锁成功, 就会把线程的中断状态返回到 acquire, 如果 acquireQueued 返回 true, 就会执行 selfInterrupt 方法 123456789public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125;static void selfInterrupt() &#123; Thread.currentThread().interrupt();&#125; 为什么获取了锁以后还要中断线程？ 这部分属于 Java 提供的协作式中断知识内容 当等待线程被唤醒时, 并不知道被唤醒的原因, 可能是当前线程在等待中被中断, 也可能是释放了锁以后被唤醒 因此我们通过 Thread.interrupted() 方法检查中断标记（该方法返回了当前线程的中断状态, 并将当前线程的中断标识设置为 false）, 并记录下来, 如果发现该线程被中断过, 就再中断一次 线程在等待资源的过程中被唤醒, 唤醒后还是会不断地去尝试获取锁, 直到抢到锁为止。也就是说, 在整个流程中, 并不响应中断, 只是记录中断记录。最后抢到锁返回了, 那么如果被中断过的话, 就需要恢复中断 Synchronized 和 ReentrantLock 的区别 Synchronized 是 JVM 层次的锁实现, ReentrantLock 是 JDK 层次的锁实现 Synchronized 的锁状态是无法在代码中直接判断的, 但是 ReentrantLock 可以通过 ReentrantLock#isLocked 判断 Synchronized 是非公平锁, ReentrantLock 是可以是公平也可以是非公平的 Synchronized 是不可以被中断的, 而 ReentrantLock#lockInterruptibly 方法是可以被中断的 在发生异常时 Synchronized 会自动释放锁（由 javac 编译时自动实现）, 而 ReentrantLock 需要开发者在 finally 块中显示释放锁 ReentrantLock 获取锁的形式有多种：如立即返回是否成功的 tryLock(), 以及等待指定时长的获取, 更加灵活 Synchronized 在特定的情况下对于已经在等待的线程是后来的线程先获得锁（上文有说）, 而 ReentrantLock 对于已经在等待的线程一定是先来的线程先获得锁 Condition Condition 的作用等同于 Object.wait() 和 Object.notify(), 条件队列是一个 FIFO 队列, 可以通过 signalAll 解锁全部的线程, 也可以通过 signal 单独解锁线程, 可以通过如下方式创建 12ReentrantLock lock = new ReentrantLock();Condition condition = lock.newCondition(); Condition 内部也是维护了一个 FIFO 队列 condition queue 12345678910111213141516public class ConditionObject implements Condition, java.io.Serializable &#123; /** First node of condition queue. */ private transient Node firstWaiter; /** Last node of condition queue. */ private transient Node lastWaiter; ......&#125;static final class Node &#123; ...... Node nextWaiter; ......&#125; awiat : 等待 Condition123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114public final void await() throws InterruptedException &#123; // 判断中断则抛出 InterruptedException if (Thread.interrupted()) throw new InterruptedException(); // 在 condition queue 中增加节点, 节点状态为 CONDITION, 在增加节点的同时清除掉状态为 CANCELED 的节点 Node node = addConditionWaiter(); // 释放锁并储存释放时的 state, 失败会抛出 IllegalMonitorStateException 异常 int savedState = fullyRelease(node); int interruptMode = 0; // 判断节点是否在同步队列中, 只有不在同步队列, 才阻塞线程 while (!isOnSyncQueue(node)) &#123; // 阻塞 node, 直到被 signal 或者中断 LockSupport.park(this); // 唤醒后检查中断, 若发生中断则更新节点状态并将其放入同步队列, 并记录对应的中断操作 if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; // 当 node 被 signal 或中断后, 已被加入到同步队列中, 调用 acquireQueued 重新获取锁 if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; // 移除掉所有状态为 CANCELED 的节点 if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); if (interruptMode != 0) // 根据 interruptMode 抛出中断异常或恢复中断 reportInterruptAfterWait(interruptMode);&#125;private Node addConditionWaiter() &#123; Node t = lastWaiter; // If lastWaiter is cancelled, clean out. if (t != null &amp;&amp; t.waitStatus != Node.CONDITION) &#123; unlinkCancelledWaiters(); // 清理被取消的节点 t = lastWaiter; &#125; Node node = new Node(Thread.currentThread(), Node.CONDITION); if (t == null) firstWaiter = node; else t.nextWaiter = node; lastWaiter = node; return node;&#125;private void unlinkCancelledWaiters() &#123; Node t = firstWaiter; Node trail = null; // 慢指针, 指向上个遍历到的有效节点 while (t != null) &#123; Node next = t.nextWaiter; // 如果 t 的状态不是 CONDITION, 则把 t 节点从链表中摘除 if (t.waitStatus != Node.CONDITION) &#123; t.nextWaiter = null; if (trail == null) firstWaiter = next; else trail.nextWaiter = next; if (next == null) lastWaiter = trail; &#125; else trail = t; t = next; &#125;&#125;final boolean isOnSyncQueue(Node node) &#123; if (node.waitStatus == Node.CONDITION || node.prev == null) return false; if (node.next != null) // If has successor, it must be on queue return true; // 从后往前查找节点是否已经在等待队列中 return findNodeFromTail(node);&#125;private boolean findNodeFromTail(Node node) &#123; Node t = tail; for (;;) &#123; if (t == node) return true; if (t == null) return false; t = t.prev; &#125;&#125;private int checkInterruptWhileWaiting(Node node) &#123; return Thread.interrupted() ? // 如果发生中断, 更新节点状态并将其放入同步队列 // 如果发生在 signal 之前则返回 THROW_IE, 如果中断发生在 signal 之后返回 REINTERRUPT (transferAfterCancelledWait(node) ? THROW_IE : REINTERRUPT) : 0;&#125;final boolean transferAfterCancelledWait(Node node) &#123; if (compareAndSetWaitStatus(node, Node.CONDITION, 0)) &#123; // CAS 成功说明该节点还没被 signal, 将其放入同步队列, 待获取到锁后抛出异常 enq(node); return true; &#125; // 若 CAS 失败则该节点正好被 signal 完, 只需自旋确认节点已经进入同步队列即可, 待获取到锁后恢复异常 while (!isOnSyncQueue(node)) Thread.yield(); return false;&#125;private void reportInterruptAfterWait(int interruptMode) throws InterruptedException &#123; if (interruptMode == THROW_IE) throw new InterruptedException(); else if (interruptMode == REINTERRUPT) selfInterrupt();&#125; signal, signalAll : 通知 Condition12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public final void signal() &#123; // 非持有锁的线程调用会抛出 IllegalMonitorStateException 异常 if (!isHeldExclusively()) throw new IllegalMonitorStateException(); // 取出 condition 队列第一个节点 Node first = firstWaiter; if (first != null) doSignal(first);&#125;public final void signalAll() &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); Node first = firstWaiter; if (first != null) doSignalAll(first);&#125;private void doSignal(Node first) &#123; do &#123; // 向后移动 firstWaiter 指针 if ( (firstWaiter = first.nextWaiter) == null) lastWaiter = null; first.nextWaiter = null; // 更新当前节点状态并将其放入同步队列 &#125; while (!transferForSignal(first) &amp;&amp; // 失败时若队列中还有数据则会取下一个继续尝试 (first = firstWaiter) != null);&#125;private void doSignalAll(Node first) &#123; lastWaiter = firstWaiter = null; do &#123; Node next = first.nextWaiter; first.nextWaiter = null; transferForSignal(first); first = next; &#125; while (first != null);&#125;final boolean transferForSignal(Node node) &#123; // 节点状态改为 0 if (!compareAndSetWaitStatus(node, Node.CONDITION, 0)) return false; // 插入同步队列并获得前驱节点 Node p = enq(node); int ws = p.waitStatus; // 设置前驱节点 waitStatus 为 SIGNAL if (ws &gt; 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL)) // 前驱节点被取消或设置状态失败, 此时节点已经在同步队列中, 唤醒节点线程往下走同步的逻辑即可 LockSupport.unpark(node.thread); return true;&#125; Condition 的中断处理 Condition.await() 跟 Object.wait() 相比, 既有可能抛出 InterruptedException 异常, 也可能恢复中断 处理中断的逻辑是如果在 signal 之前线程被中断则抛出中断异常, 如果在 signal 之后线程被中断, 则恢复中断, 由调用代码自行处理中断标识 ReentrantReadWriteLock ReentrantLock 是独占锁, ReentrantReadWriteLock 是读写锁 读写锁定义：一个资源能够被多个读线程访问, 或者被一个写线程访问, 但是不能同时存在读写线程 ReentrantReadWriteLock 中读锁为共享锁, 写锁为独占锁 1234567891011121314151617public static class ReadLock implements Lock, java.io.Serializable &#123; public void lock() &#123; sync.acquireShared(1); //共享 &#125; public void unlock() &#123; sync.releaseShared(1); //共享 &#125;&#125;public static class WriteLock implements Lock, java.io.Serializable &#123; public void lock() &#123; sync.acquire(1); //独占 &#125; public void unlock() &#123; sync.release(1); //独占 &#125;&#125; state123456789101112131415abstract static class Sync extends AbstractQueuedSynchronizer &#123; private static final long serialVersionUID = 6317671515068378041L; static final int SHARED_SHIFT = 16; static final int SHARED_UNIT = (1 &lt;&lt; SHARED_SHIFT); static final int MAX_COUNT = (1 &lt;&lt; SHARED_SHIFT) - 1; static final int EXCLUSIVE_MASK = (1 &lt;&lt; SHARED_SHIFT) - 1; /** Returns the number of shared holds represented in count */ static int sharedCount(int c) &#123; return c &gt;&gt;&gt; SHARED_SHIFT; &#125; /** Returns the number of exclusive holds represented in count */ static int exclusiveCount(int c) &#123; return c &amp; EXCLUSIVE_MASK; &#125; ......&#125; state 的高 16 位表示读锁的 state, 低 16 位表示写锁的 state 将两个锁的状态放在同一个 int 变量的中原因是对 state 的操作可以使用 CAS 保证原子性 读锁和写锁最多可以获取 2^16 -1 = 65535 个 (包括重入) 写锁加锁过程1234567891011121314151617181920212223242526272829303132333435363738protected final boolean tryAcquire(int acquires) &#123; Thread current = Thread.currentThread(); int c = getState(); // 写锁数量 int w = exclusiveCount(c); if (c != 0) &#123; // 存在读锁或有其他线程持有写锁, 则进入队列等待 // (Note: if c != 0 and w == 0 then shared count != 0) if (w == 0 || current != getExclusiveOwnerThread()) return false; // 当前线程已持有写锁, 进行重入 if (w + exclusiveCount(acquires) &gt; MAX_COUNT) throw new Error("Maximum lock count exceeded"); // Reentrant acquire setState(c + acquires); return true; &#125; // 判断是否需要排队 (公平锁) if (writerShouldBlock() || !compareAndSetState(c, c + acquires)) return false; setExclusiveOwnerThread(current); return true;&#125;static final class NonfairSync extends Sync &#123; final boolean writerShouldBlock() &#123; return false; &#125;&#125;static final class FairSync extends Sync &#123; final boolean writerShouldBlock() &#123; return hasQueuedPredecessors(); &#125;&#125; 解锁过程1234567891011// 逻辑与 ReentrantLock 相似protected final boolean tryRelease(int releases) &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); int nextc = getState() - releases; boolean free = exclusiveCount(nextc) == 0; if (free) setExclusiveOwnerThread(null); setState(nextc); return free;&#125; 读锁加锁过程AQS.acquireShared 和 AQS.doAcquireShared123456789101112131415161718192021222324252627282930313233343536373839404142public final void acquireShared(int arg) &#123; // 返回值小于 0 代表没有获取到共享锁 (读锁) if (tryAcquireShared(arg) &lt; 0) // 进入等待 doAcquireShared(arg);&#125;private void doAcquireShared(int arg) &#123; // 新建共享模式节点并进入等待队列 final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; // 获取前驱节点 final Node p = node.predecessor(); if (p == head) &#123; // 若为队首节点, 则重新尝试获取共享锁 int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; // 头节点后移并传播 // 传播即唤醒后面连续的读节点 setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted) // 等待过程若发生中断则恢复中断 selfInterrupt(); failed = false; return; &#125; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;// 清理和更新前驱节点状态 parkAndCheckInterrupt()) // 挂起线程 // 线程若被中断, 返回 true interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; Sync.tryAcquireShared123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111protected final int tryAcquireShared(int unused) &#123; Thread current = Thread.currentThread(); int c = getState(); // 存在被其他线程持有的写锁, 则进入队列等待 if (exclusiveCount(c) != 0 &amp;&amp; getExclusiveOwnerThread() != current) return -1; // 读锁数量 int r = sharedCount(c); // 判断是否需要排队 if (!readerShouldBlock() &amp;&amp; r &lt; MAX_COUNT &amp;&amp; compareAndSetState(c, c + SHARED_UNIT)) &#123; // 开始统计每个读锁获取线程的重入次数 if (r == 0) &#123; // 首个获得读锁的线程 firstReader = current; firstReaderHoldCount = 1; &#125; else if (firstReader == current) &#123; // 首个获得读锁的线程重入 firstReaderHoldCount++; &#125; else &#123; // 当前线程不是第一个获取读锁的线程, 放入该线程的本地变量中 HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) cachedHoldCounter = rh = readHolds.get(); else if (rh.count == 0) readHolds.set(rh); rh.count++; &#125; return 1; &#125; // 快速获取失败, 进入 TryAcquireShared 完全版本重试 return fullTryAcquireShared(current);&#125; static final class NonfairSync extends Sync &#123; final boolean readerShouldBlock() &#123; return apparentlyFirstQueuedIsExclusive(); &#125;&#125;// AQS.apparentlyFirstQueuedIsExclusivefinal boolean apparentlyFirstQueuedIsExclusive() &#123; Node h, s; // 在非公平模式, 只有同步队列的首节点是写锁才需要排队 return (h = head) != null &amp;&amp; (s = h.next) != null &amp;&amp; !s.isShared() &amp;&amp; s.thread != null;&#125;static final class FairSync extends Sync &#123; final boolean readerShouldBlock() &#123; return hasQueuedPredecessors(); &#125;&#125;// TryAcquireShared 完全版本// 主要为了处理 CAS 失败和 tryAcquireShared 未处理的重入情况final int fullTryAcquireShared(Thread current) &#123; HoldCounter rh = null; for (;;) &#123; // 部分逻辑与 TryAcquireShared 冗余 int c = getState(); if (exclusiveCount(c) != 0) &#123; if (getExclusiveOwnerThread() != current) return -1; &#125; else if (readerShouldBlock()) &#123; // 若队列中存在获取读锁的情况, 是会走到此位置 // 检查当前线程是否已持有过读锁 if (firstReader == current) &#123; // assert firstReaderHoldCount &gt; 0; &#125; else &#123; if (rh == null) &#123; rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) &#123; rh = readHolds.get(); if (rh.count == 0) readHolds.remove(); &#125; if (rh.count == 0) return -1; &#125; &#125; if (sharedCount(c) == MAX_COUNT) throw new Error("Maximum lock count exceeded"); if (compareAndSetState(c, c + SHARED_UNIT)) &#123; if (sharedCount(c) == 0) &#123; firstReader = current; firstReaderHoldCount = 1; &#125; else if (firstReader == current) &#123; firstReaderHoldCount++; &#125; else &#123; if (rh == null) rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) rh = readHolds.get(); else if (rh.count == 0) readHolds.set(rh); rh.count++; cachedHoldCounter = rh; // cache for release &#125; return 1; &#125; // CAS 失败则重新尝试一遍整个逻辑 &#125;&#125; 解锁过程AQS.releaseShared 和 AQS.doReleaseShared123456789101112131415161718192021222324252627282930public final boolean releaseShared(int arg) &#123; // 尝试释放共享锁 if (tryReleaseShared(arg)) &#123; // 共享锁完全释放, 则唤醒队列中的下个节点 doReleaseShared(); return true; &#125; return false;&#125;private void doReleaseShared() &#123; for (;;) &#123; Node h = head; if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; // head state 为 SIGNAL 时唤醒后继节点 if (ws == Node.SIGNAL) &#123; if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases unparkSuccessor(h); &#125; // head state 为 0 时切换状态 else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; if (h == head) // loop if head changed break; &#125;&#125; Sync.tryReleaseShared1234567891011121314151617181920212223242526272829303132protected final boolean tryReleaseShared(int unused) &#123; Thread current = Thread.currentThread(); // 更新线程缓存中的重入次数 if (firstReader == current) &#123; // assert firstReaderHoldCount &gt; 0; if (firstReaderHoldCount == 1) firstReader = null; else firstReaderHoldCount--; &#125; else &#123; HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) rh = readHolds.get(); int count = rh.count; if (count &lt;= 1) &#123; readHolds.remove(); if (count &lt;= 0) throw unmatchedUnlockException(); &#125; --rh.count; &#125; for (;;) &#123; int c = getState(); int nextc = c - SHARED_UNIT; if (compareAndSetState(c, nextc)) // Releasing the read lock has no effect on readers, // but it may allow waiting writers to proceed if // both read and write locks are now free. // 只有读锁数为 0 时才代表完全释放读锁 return nextc == 0; &#125;&#125; 总结 由获取读锁的逻辑可见, 同一个线程拥有写锁之后再获取读锁是允许的, 这也被称为锁降级 在非公平锁情况下, 允许写锁插队, 也允许读锁插队, 但是读锁插队的前提是队列中的头节点不能是获取写锁的线程, 避免写线程饥饿 Semaphore Semaphore (信号量) 信号量 S, 整型变量, 需要初始化值大于0 P 操作, 原子减少 S, 如果 S &lt; 0, 则阻塞当前线程 V 操作, 原子增加 S, 如果 S &lt;= 0, 则唤醒一个阻塞的线程 具体概念请参考该文章 初始化123456789101112131415161718192021public class Semaphore implements java.io.Serializable &#123; private final Sync sync; abstract static class Sync extends AbstractQueuedSynchronizer &#123; Sync(int permits) &#123; // state 用于储存信号量 setState(permits); &#125; ...... &#125; public Semaphore(int permits) &#123; // 默认非公平锁 sync = new NonfairSync(permits); &#125; public Semaphore(int permits, boolean fair) &#123; sync = fair ? new FairSync(permits) : new NonfairSync(permits); &#125; ......&#125; 加锁过程12345678910111213141516171819202122232425262728293031323334353637static final class NonfairSync extends Sync &#123; ...... protected int tryAcquireShared(int acquires) &#123; return nonfairTryAcquireShared(acquires); &#125;&#125;abstract static class Sync extends AbstractQueuedSynchronizer &#123; ...... final int nonfairTryAcquireShared(int acquires) &#123; for (;;) &#123; int available = getState(); int remaining = available - acquires; // 剩余信号量小于 0 直接返回, 线程将进入队列排队 if (remaining &lt; 0 || // 剩余信号量不小于 0 则成功获取到锁, 更新信号量 compareAndSetState(available, remaining)) return remaining; &#125; &#125;&#125;static final class FairSync extends Sync &#123; ...... protected int tryAcquireShared(int acquires) &#123; for (;;) &#123; // 公平锁判断 if (hasQueuedPredecessors()) return -1; int available = getState(); int remaining = available - acquires; if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; &#125; &#125;&#125; 解锁过程1234567891011121314abstract static class Sync extends AbstractQueuedSynchronizer &#123; ...... protected final boolean tryReleaseShared(int releases) &#123; for (;;) &#123; int current = getState(); // 补回信号量 int next = current + releases; if (next &lt; current) // overflow throw new Error("Maximum permit count exceeded"); if (compareAndSetState(current, next)) return true; &#125; &#125;&#125; CountDownLatch CountDownLatch (计数闭锁) 有初始计数值 计数值大于 0 时, 获取锁的线程会被阻塞 计数值被减到 0 时, 所有被阻塞的线程同时被释放 CountDownLatch 是一种闭锁的实现 闭锁可以延迟线程的进度直到其到达终止状态 初始化12345678910111213141516public class CountDownLatch &#123; private static final class Sync extends AbstractQueuedSynchronizer &#123; Sync(int count) &#123; // state 用于储存计数值 setState(count); &#125; ...... &#125; public CountDownLatch(int count) &#123; if (count &lt; 0) throw new IllegalArgumentException("count &lt; 0"); this.sync = new Sync(count); &#125; ......&#125; 加锁过程123456789101112private static final class Sync extends AbstractQueuedSynchronizer &#123; protected int tryAcquireShared(int acquires) &#123; // 根据计数是否到 0 返回是否成功 return (getState() == 0) ? 1 : -1; &#125; ......&#125; public void await() throws InterruptedException &#123; sync.acquireSharedInterruptibly(1);&#125; 解锁过程1234567891011121314151617181920private static final class Sync extends AbstractQueuedSynchronizer &#123; protected boolean tryReleaseShared(int releases) &#123; // Decrement count; signal when transition to zero for (;;) &#123; int c = getState(); if (c == 0) return false; // 计数减 1 int nextc = c-1; if (compareAndSetState(c, nextc)) return nextc == 0; &#125; &#125; ......&#125; public void countDown() &#123; sync.releaseShared(1);&#125; CyclicBarrier 栅栏 (Barrier) 类似于闭锁, 能阻塞一组线程直到某个事件发生 主要区别在于必须所有线程到达栅栏位置才能继续执行 闭锁用于等待事件, 栅栏用于等待其他线程 初始化123456789101112131415161718192021222324252627282930313233343536public class CyclicBarrier &#123; // 用于表示每代栅栏, 并记录该代栅栏是否有被打破 private static class Generation &#123; boolean broken = false; &#125; // 阻塞所需要 private final ReentrantLock lock = new ReentrantLock(); private final Condition trip = lock.newCondition(); // 栅栏的线程数 private final int parties; // 所有线程到达栅栏后会执行的函数 private final Runnable barrierCommand; // 当前代 private Generation generation = new Generation(); // 用于计数, 每有一个到达栅栏的线程就减 1 private int count; public CyclicBarrier(int parties, Runnable barrierAction) &#123; if (parties &lt;= 0) throw new IllegalArgumentException(); this.parties = parties; this.count = parties; this.barrierCommand = barrierAction; &#125; public CyclicBarrier(int parties) &#123; this(parties, null); &#125; ......&#125; 到达栅栏1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889public int await() throws InterruptedException, BrokenBarrierException &#123; try &#123; return dowait(false, 0L); &#125; catch (TimeoutException toe) &#123; throw new Error(toe); // cannot happen &#125;&#125;public int await(long timeout, TimeUnit unit) throws InterruptedException, BrokenBarrierException, TimeoutException &#123; return dowait(true, unit.toNanos(timeout));&#125;private int dowait(boolean timed, long nanos) throws InterruptedException, BrokenBarrierException, TimeoutException &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; final Generation g = generation; if (g.broken) // 若此代栅栏已经被打破, 则抛出 BrokenBarrierException 异常 throw new BrokenBarrierException(); if (Thread.interrupted()) &#123; // 处理中断, 打破栅栏 breakBarrier(); throw new InterruptedException(); &#125; // 计数减 1 int index = --count; if (index == 0) &#123; // tripped // 计数到 0 时代表所有线程都已经到达栅栏 boolean ranAction = false; try &#123; // 执行栅栏函数 final Runnable command = barrierCommand; if (command != null) command.run(); ranAction = true; // 开始下一代栅栏 nextGeneration(); return 0; &#125; finally &#123; if (!ranAction) breakBarrier(); &#125; &#125; // 计数未到 0, 进行阻塞等待 // loop until tripped, broken, interrupted, or timed out for (;;) &#123; try &#123; if (!timed) trip.await(); else if (nanos &gt; 0L) nanos = trip.awaitNanos(nanos); &#125; catch (InterruptedException ie) &#123; if (g == generation &amp;&amp; ! g.broken) &#123; breakBarrier(); throw ie; &#125; else &#123; // We're about to finish waiting even if we had not // been interrupted, so this interrupt is deemed to // "belong" to subsequent execution. Thread.currentThread().interrupt(); &#125; &#125; if (g.broken) throw new BrokenBarrierException(); if (g != generation) return index; if (timed &amp;&amp; nanos &lt;= 0L) &#123; // 等待超时, 打破栅栏 breakBarrier(); throw new TimeoutException(); &#125; &#125; &#125; finally &#123; lock.unlock(); &#125;&#125; 读写一致性的一些思考 在一个线程写，一个或多个线程读的情况下 试想下这样一个场景：一个线程往 HashMap 中写数据，一个线程往 HashMap 中读数据。 这样会有问题吗？ 内存可见性的问题，HashMap 存储数据的 table 并没有用 voliate 修饰，也就是说读线程可能一直读不到数据的最新值 指令重排序的问题，get 的时候可能得到的是一个中间状态的数据，我们看下 put 方法的部分代码 1234567final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; ... if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = new Node&lt;&gt;(hash, key, value, next); ... &#125; 在 put 操作时，如果 table 数组的指定位置为 null，会创建一个 Node 对象，并放到 table 数组上 JVM 中 tab[i] = new Node&lt;&gt;(hash, key, value, next); 这样的操作不是原子的，并且可能因为指令重排序，导致另一个线程调用 get 取 tab[i] 的时候，拿到的是一个还没有调用完构造方法的对象，导致不可预料的问题发生 上述的两个问题可以说都是因为 HashMap 中的内部属性没有被 voliate 修饰导致的 就算给 table 加上了 volatile 应该只是保持了 table 引用的可见性，对于 table 中的元素不起作用 所以 table 加上volatile 也不能保证其中元素的可见性 在 ConcurrentHashMap (1.8) 中 通过 Unsafe 类的 getObjectVolatile 方法保证 table 里获取到的数据每次都是最新的，而不是缓存 而在设置数组元素时, 采用 compareAndSwapObject 方法，而不是直接通过下标去操作 创建对象的原子性问题 对于 Object obj = new Object();这样的操作, 在多线程的情况下可能会拿到一个未初始化的对象 以上 Java 语句分为 4 个步骤 在栈中分配一片空间给 obj 引用 在 JVM 堆中创建一个 Object 对象，注意这里仅仅是分配空间，没有调用构造方法 初始化第 2 步创建的对象，也就是调用其构造方法 栈中的 obj 指向堆中的对象 问题在于 JVM 是会对指令进行重排序的，重排之后可能是第 4 步先于第 3 步执行，那这时候另外一个线程读到的就是没有还执行构造方法的对象，导致未知问题 JVM 重排只保证重排前和重排后在单线程中的结果一致性 注意 Java 中引用的赋值操作一定是原子的，比如说 a 和 b 均是对象的情况下不管是 32 位还是 64 位 JVM，a=b 操作均是原子的 但如果 a 和 b 是 long 或者 double 原子型数据，那在 32 位 JVM 上 a=b 不一定是原子的（看 JVM 具体实现），有可能是分成了两个 32 位操作。 但是对于 voliate 的 long, double 变量来说，其赋值是原子的 数据库中读写一致性 跳出 HashMap，在数据库中都是要用 MVCC 机制避免加读写锁 也就是说如果不用 MVCC，数据库是要加读写锁的，那为什么数据库要加读写锁呢？ 原因是写操作不是原子的，如果不加读写锁或 MVCC，可能会读到中间状态的数据 以 HBase 为例，Hbase 写流程分为以下几个步骤： 获得行锁 开启 MVCC 写到内存 Buffer 写到 Append Log 释放行锁 Flush Log MVCC 结束（这时才对读可见） 试想，如果没有不走 2，7 也不加读写锁，那在步骤 3 的时候，其他的线程就能读到该数据 如果说 3 之后出现了问题，那该条数据其实是写失败的。也就是说其他线程曾经读到过不存在的数据 同理，在 MySQL 中，如果不用 MVCC 也不用读写锁，一个事务还没 commit，其中的数据就能被读到，如果用读写锁，一个事务会对中更改的数据加写锁，这时其他读操作会阻塞，直到事务提交，对于性能有很大的影响，所以大多数情况下数据库都采用 MVCC 机制实现非锁定读 参考 https://www.jianshu.com/p/8601f374f051 https://www.jianshu.com/p/1708ccfb07ae https://zhuanlan.zhihu.com/p/52563959 https://github.com/farmerjohngit/myblog/issues/12 https://tech.meituan.com/2018/11/15/Java-lock.html https://juejin.im/post/5e0b02436fb9a0483040c522#heading-16 https://tech.meituan.com/2019/12/05/aqs-theory-and-apply.html https://github.com/farmerjohngit/myblog/issues/9]]></content>
      <categories>
        <category>programming</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>multithread</tag>
        <tag>concurrency</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 线程与线程池]]></title>
    <url>%2F2020%2F05%2F20%2FJava-%E7%BA%BF%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[Java 线程与线程池线程的状态 NEW, 新建状态, 线程被创建出来, 但尚未启动时的线程状态 RUNNABLE, 就绪状态, 表示可以运行的线程状态, 它可能正在运行, 或者是在排队等待操作系统给它分配 CPU 资源 BLOCKED, 阻塞等待锁的线程状态, 表示处于阻塞状态的线程正在等待监视器锁, 比如等待执行 synchronized 代码块或者使用 synchronized 标记的方法 WAITING, 等待状态, 一个处于等待状态的线程正在等待另一个线程执行某个特定的动作, 比如, 一个线程调用了 Object.wait() 方法, 那它就在等待另一个线程调用 Object.notify() 或 Object.notifyAll() 方法 TIMED_WAITING, 计时等待状态, 和 WAITING 类似, 它只是多了超时时间, 比如调用了有超时时间设置的方法 Object.wait(long timeout) 和 Thread.join(long timeout) 等这些方法时, 它才会进入此状态 TERMINATED, 终止状态, 表示线程已经执行完成 关于 Object.wait/notify相关代码12345678910111213141516171819202122232425262728293031323334353637383940public class WaitNotifyCase &#123; public static void main(String[] args) &#123; final Object lock = new Object(); new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("thread A is waiting to get lock"); synchronized (lock) &#123; try &#123; System.out.println("thread A get lock"); TimeUnit.SECONDS.sleep(1); System.out.println("thread A do wait method"); lock.wait(); System.out.println("wait end"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("thread B is waiting to get lock"); synchronized (lock) &#123; System.out.println("thread B get lock"); try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; lock.notify(); System.out.println("thread B do notify method"); &#125; &#125; &#125;).start(); &#125;&#125; 执行结果1234567thread A is waiting to get lockthread A get lockthread B is waiting to get lockthread A do wait methodthread B get lockthread B do notify methodwait end 疑问 进入 wait/notify 方法之前, 为什么要获取 synchronized 锁？ 线程 A 获取了 synchronized 锁, 执行 wait 方法并挂起, 线程 B 又如何再次获取锁？ 分析 synchronized 代码块通过 javap 生成的字节码中包含 monitorenter 和 monitorexit 指令, 执行 monitorenter 指令可以获取对象的 monitor , 在 wait() 接口注释中有标明 The current thread must own this object&#39;s monitor , 所以通过 synchronized 该线程持有了对象的 monitor 的情况下才能调用对象的 wait() 方法 wait() 接口注释中还提到调用 wait() 后该线程会释放持有的 monitor 进入等待状态直到被唤醒, 被唤醒的线程还要等到能重新持有 monitor 才会继续执行 线程状态变化: 调用 wait(): RUNNABLE -&gt; WAITING 调用 notify: WAITING -&gt; BLOCKED -&gt; RUNNABLE WAITING -&gt; RUNNABLE 具体看 JVM 实现和策略配置 深入: 什么是 monitor 在 HotSpot 虚拟机中 (1.7 版本), monitor 采用 ObjectMonitor 实现 123456789101112131415161718ObjectMonitor() &#123; _header = NULL; _count = 0; // 用来记录该线程获取锁的次数 _waiters = 0, _recursions = 0; // 锁的重入次数 _object = NULL; // 对应的对象 _owner = NULL; // 指向持有 ObjectMonitor 对象的线程 _WaitSet = NULL; // 处于 WAITING 状态的线程, 会被加入到 _WaitSet _WaitSetLock = 0 ; _Responsible = NULL ; _succ = NULL ; _cxq = NULL ; // 竞争锁的线程都会先通过互斥同步或 CAS 操作进入 cxq, 队首的对象会进入到 EntryList 中, 进行 tryLock 操作 FreeNext = NULL ; _EntryList = NULL ; // 处于 BLOCKED 状态的线程, 会被加入到 _EntryList _SpinFreq = 0 ; _SpinClock = 0 ; OwnerIsThread = 0 ;&#125; 每个线程都有两个 ObjectMonitor 对象列表, 分别为 free 和 used 列表, 如果当前 free 列表为空, 线程将向全局 global ListLock 请求分配 ObjectMonitor ObjectMonitor 对象中有两个队列：_WaitSet 和 _EntryList, 用来保存 ObjectWaiter 对象列表；_owner 指向获得 ObjectMonitor 对象的线程 每个等待锁的线程都会被封装成 ObjectWaiter 对象 ObjectWaiter 对象是双向链表结构, 保存了_thread（当前线程）以及当前的状态 TState等数据 ObjectMonitor 获得锁是通过 void ATTR enter(TRAPS); 方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465void ATTR ObjectMonitor::enter(TRAPS) &#123; Thread * const Self = THREAD ; void * cur ; // 通过 CAS 尝试把 monitor 的 _owner 设置为当前线程 cur = Atomic::cmpxchg_ptr (Self, &amp;_owner, NULL) ; // 获取锁失败 if (cur == NULL) &#123; assert (_recursions == 0 , "invariant") ; assert (_owner == Self, "invariant") ; // CONSIDER: set or assert OwnerIsThread == 1 return ; &#125; // 如果旧值和当前线程一样, 说明当前线程已经持有锁, 此次为重入, _recursions 自增即可 if (cur == Self) &#123; // TODO-FIXME: check for integer overflow! BUGID 6557169. _recursions ++ ; return ; &#125; // 如果当前线程是第一次进入该 monitor, 设置 _recursions 为 1, _owner 为当前线程 if (Self-&gt;is_lock_owned ((address)cur)) &#123; assert (_recursions == 0, "internal state error"); _recursions = 1 ; // Commute owner from a thread-specific on-stack BasicLockObject address to // a full-fledged "Thread *". _owner = Self ; OwnerIsThread = 1 ; return ; &#125; // 省略部分代码 ...... // 在调用系统的同步操作之前，先尝试自旋获得锁 if (Knob_SpinEarly &amp;&amp; TrySpin (Self) &gt; 0) &#123; ... //自旋的过程中获得了锁，则直接返回 Self-&gt;_Stalled = 0 ; return ; &#125; ...... // 通过自旋执行 ObjectMonitor::EnterI 方法等待锁的释放 for (;;) &#123; jt-&gt;set_suspend_equivalent(); // cleared by handle_special_suspend_equivalent_condition() // or java_suspend_self() // 将当前线程插入到 cxq 队列, 挂起等待重新尝试获取锁 EnterI (THREAD) ; if (!ExitSuspendEquivalent(jt)) break ; // We have acquired the contended monitor, but while we were // waiting another thread suspended us. We don't want to enter // the monitor while suspended because that would surprise the // thread that suspended us. // _recursions = 0 ; _succ = NULL ; exit (Self) ; jt-&gt;java_suspend_self(); &#125;&#125; ObjectMonitor 释放锁是通过 void ATTR exit(TRAPS); 方法 123456789101112131415161718192021222324252627282930313233void ATTR ObjectMonitor::exit(TRAPS) &#123; Thread * Self = THREAD ; // 如果当前线程不是 Monitor 的所有者 if (THREAD != _owner) &#123; if (THREAD-&gt;is_lock_owned((address) _owner)) &#123; // Transmute _owner from a BasicLock pointer to a Thread address. // We don't need to hold _mutex for this transition. // Non-null to Non-null is safe as long as all readers can // tolerate either flavor. assert (_recursions == 0, "invariant") ; _owner = THREAD ; _recursions = 0 ; OwnerIsThread = 1 ; &#125; else &#123; // NOTE: we need to handle unbalanced monitor enter/exit // in native code by throwing an exception. // TODO: Throw an IllegalMonitorStateException ? TEVENT (Exit - Throw IMSX) ; assert(false, "Non-balanced monitor enter/exit!"); if (false) &#123; THROW(vmSymbols::java_lang_IllegalMonitorStateException()); &#125; return; &#125; &#125; // 如果 _recursions 次数不为 0.自减 if (_recursions != 0) &#123; _recursions--; // this is simple recursive enter TEVENT (Inflated exit - recursive) ; return ; &#125; // 省略部分代码, 根据不同的策略（由 QMode 指定）, 从 _cxq 或 EntryList 中获取头节点, 通过 ObjectMonitor::ExitEpilog 方法唤醒该节点封装的线程, 唤醒操作最终由 unpark 完成。 lock.wait() 方法最终通过 ObjectMonitor 的 void wait(jlong millis, bool interruptable, TRAPS); 实现: 将当前线程封装成 ObjectWaiter 对象 node 通过 ObjectMonitor::AddWaiter 方法将 node 添加到 _WaitSet 列表中 通过 ObjectMonitor::exit 方法释放当前的 ObjectMonitor 对象, 这样其它竞争线程就可以获取该 ObjectMonitor 对象 最终底层的 park 方法会挂起线程 lock.notify() 方法最终通过 ObjectMonitor 的 void notify(TRAPS) 实现: 如果当前 _WaitSet 为空, 即没有正在等待的线程, 则直接返回 通过 ObjectMonitor::DequeueWaiter 方法, 获取 _WaitSet 列表中的第一个 ObjectWaiter节点 根据不同的策略, 将取出来的 ObjectWaiter 节点加入到 _EntryList 或则通过 Atomic::cmpxchg_ptr 指令进行自旋操作 _cxq 关于中断Java 中线程间是协作式，而非抢占式 调用一个线程的 interrupt() 方法中断一个线程，并不是强行关闭这个线程，只是通知线程停止，将线程的中断标志位置为 true，线程是否中断，由线程本身决定, 线程可以进行停止前的释放资源, 完成必要的处理任务 相关方法 在线程内可通过 isInterrupted() 判断终端并进行相应处理 另一个静态方法 Thread.interrupted() 返回当前线程的中断状态，同时重置中断标志位为 false 如何正确停止线程 中断会唤醒阻塞的线程, 并且大部分都会抛出 InterruptedException 如果方法里如果抛出中断异常 InterruptedException，则线程的中断标志位会被复位成 false 实际开发中的两种最佳实践 传递中断 对于底层的方法, 在方法的签名上标注异常 (throws InterruptedException) 抛出异常，而异常的真正处理，应该交给调用它的那个函数 因为标注了异常, 调用者必须对 InterruptedException 异常进行处理 恢复中断 在底层方法中 catch 处理异常 处理完成后手动调用 Thread.currentThread().interrupt() 恢复中断 相关问题1. BLOCKED（阻塞等待）和 WAITING（等待）有什么区别？ 状态形成的调用方法不同 BLOCKED 可以理解为当前线程还处于活跃状态, 只是在阻塞等待其他线程使用完某个锁资源 WAITING 则是因为自身调用了 Object.wait() 或着是 Thread.join() 又或者是 LockSupport.park() 而进入等待状态, 只能等待其他线程执行某个特定的动作才能被继续唤醒, 比如当线程因为调用了 Object.wait() 而进入 WAITING 状态之后, 则需要等待另一个线程执行 Object.notify() 或 Object.notifyAll() 才能被唤醒 2. start() 方法和 run() 方法有什么区别？ 12345678910111213141516171819202122public synchronized void start() &#123; // 状态验证, 不等于 NEW 的状态会抛出异常 if (threadStatus != 0) throw new IllegalThreadStateException(); // 通知线程组, 此线程即将启动 group.add(this); boolean started = false; try &#123; start0(); started = true; &#125; finally &#123; try &#123; if (!started) &#123; // 通知线程组, 此线程启动失败 group.threadStartFailed(this); &#125; &#125; catch (Throwable ignore) &#123; // 不处理任何异常, 如果 start0 抛出异常, 则它将被传递到调用堆栈上 &#125; &#125;&#125; start() 方法属于 Thread 自身的方法, 并且使用了 synchronized 来保证线程安全 run() 方法为 Runnable 的抽象方法, 重写的 run() 方法其实就是此线程要执行的业务方法 调用 start() 方法是另起线程来运行 run() 方法中的内容 3. 线程的优先级有什么用？该如何设置？ 在 Thread 源码中和线程优先级相关的属性有 3 个 12345678// 线程可以拥有的最小优先级public final static int MIN_PRIORITY = 1;// 线程默认优先级public final static int NORM_PRIORITY = 5;// 线程可以拥有的最大优先级public final static int MAX_PRIORITY = 10 线程的优先级可以理解为线程抢占 CPU 时间片的概率, 优先级越高的线程优先执行的概率就越大, 但并不能保证优先级高的线程一定先执行 在程序中我们可以通过 Thread.setPriority() 来设置优先级 12345678910111213141516public final void setPriority(int newPriority) &#123; ThreadGroup g; // 检查当前线程是否有权限修改优先级 checkAccess(); // 先验证优先级的合理性 if (newPriority &gt; MAX_PRIORITY || newPriority &lt; MIN_PRIORITY) &#123; throw new IllegalArgumentException(); &#125; if((g = getThreadGroup()) != null) &#123; // 优先级如果超过线程组的最高优先级, 则把优先级设置为线程组的最高优先级 if (newPriority &gt; g.getMaxPriority()) &#123; newPriority = g.getMaxPriority(); &#125; setPriority0(priority = newPriority); &#125;&#125; 4. 线程的常用方法有哪些？ sleep Thread.sleep() 让线程进入到 TIMED_WAITING 状态, 并停止占用 CPU 资源, 但是不释放持有的 monitor , 直到规定事件后再执行, 休眠期间如果被中断, 会抛出异常并清除中断状态 TimeUnit.SECONDS.sleep() 比 Thread.sleep() 多了非负数判断 join 123456789101112131415161718192021222324public final synchronized void join(long millis) throws InterruptedException &#123; long base = System.currentTimeMillis(); long now = 0; if (millis &lt; 0) &#123; throw new IllegalArgumentException("timeout value is negative"); &#125; if (millis == 0) &#123; while (isAlive()) &#123; wait(0); &#125; &#125; else &#123; while (isAlive()) &#123; long delay = millis - now; if (delay &lt;= 0) &#123; break; &#125; wait(delay); now = System.currentTimeMillis() - base; &#125; &#125;&#125; 本质是用 wait() 实现, 这里 wait() 在循环中调用, 是为了避免可能发生的 虚假唤醒 (spurious wakeup) 情况 JVM 的 Thread 执行完毕会自动执行一次 notifyAll(), 所以不建议在程序中对 Thread 对象调用 wait/notify, 可能会造成干扰 yield A hint to the scheduler that the current thread is willing to yield its current use of a processor. The scheduler is free to ignore this hint. 状态依旧是 RUNNABLE, 不保证释放 CPU 资源 Thread.sleep(0) 可以重新触发 CPU 的竞争, 而 yield 不一定 5. 被弃用的方法有哪些? 为什么被弃用? suspend 使线程暂停, 但不会释放 monitor, 所以容易造成死锁 resume 恢复通过调用 suspend() 方法而停止运行的线程 stop 强制停止当前线程, 会释放该线程所持有对象的 monitor, 因而可能造成这些对象处于不一致的状态 而且这个方法造成的 ThreadDeath 异常不像其他的检查期异常一样被捕获 ThreadLocal ThreadLocal 是线程的内部存储类，可以在指定线程内存储数据。只有指定线程可以得到存储数据 每个线程都有一个 ThreadLocalMap 的实例对象，并且通过 ThreadLocal 管理 ThreadLocalMap 123456789class Thread implements Runnable &#123; ...... /* ThreadLocal values pertaining to this thread. This map is maintained * by the ThreadLocal class. */ ThreadLocal.ThreadLocalMap threadLocals = null; ...... &#125; 应用场景 每个线程需要有自己单独的实例 实例需要在多个方法中共享，但不希望被多线程共享 并非必须使用 ThreadLocal ，其它方式完全可以实现同样的效果，只是 ThreadLocal 使得实现更简洁 相关变量和定义12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class ThreadLocal&lt;T&gt; &#123; // 每个 ThreadLocal 实例都有唯一的 hashCode private final int threadLocalHashCode = nextHashCode(); // threadLocalHashCode 值是从 0 开始每次累加 HASH_INCREMENT private static AtomicInteger nextHashCode = new AtomicInteger(); // 该魔法值是取自 2^32 * 黄金分割数, 为的是能好的散列 private static final int HASH_INCREMENT = 0x61c88647; private static int nextHashCode() &#123; return nextHashCode.getAndAdd(HASH_INCREMENT); &#125; // 通过继承 ThreadLocal 并实现 initialValue() 方法可以实现初始值 protected T initialValue() &#123; return null; &#125; static class ThreadLocalMap &#123; // map 中的 key 为弱引用 static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; // 存放的值 Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125; &#125; // Entry 数组初始容量 private static final int INITIAL_CAPACITY = 16; private Entry[] table; private int size = 0; // Entry 数组扩容阈值 private int threshold; // Default to 0 // 阈值为容量的 2/3 private void setThreshold(int len) &#123; threshold = len * 2 / 3; &#125; ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) &#123; table = new Entry[INITIAL_CAPACITY]; int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); table[i] = new Entry(firstKey, firstValue); size = 1; setThreshold(INITIAL_CAPACITY); &#125; ...... &#125;&#125; 关于为什么 HASH_INCREMENT = 0x61c88647 可阅读 从 ThreadLocal 的实现看散列算法 set()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201public void set(T value) &#123; // 获取当前线程实例 Thread t = Thread.currentThread(); // 从当前线程实例中取出 map ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else // 未初始化 map, 初始化 map createMap(t, value);&#125;void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue);&#125;private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; // 存在哈希冲突的情况时需要后移一位继续判断 e = tab[i = nextIndex(i, len)]) &#123; ThreadLocal&lt;?&gt; k = e.get(); // key 匹配，直接设置值 if (k == key) &#123; e.value = value; return; &#125; if (k == null) &#123; // 原先该位置的 key 已被回收, 进行清除工作并替换 replaceStaleEntry(key, value, i); return; &#125; &#125; // 当 i 位置为空或哈希冲突后移找到空位置时, 插入数组 tab[i] = new Entry(key, value); int sz = ++size; // 从 i 位置进行快速清理，如果清理成功并且 sz 大于阈值则触发扩容 if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash();&#125;private void replaceStaleEntry(ThreadLocal&lt;?&gt; key, Object value, int staleSlot) &#123; Entry[] tab = table; int len = tab.length; Entry e; // 向前寻找最前一个 key 已被回收的 Entry int slotToExpunge = staleSlot; for (int i = prevIndex(staleSlot, len); (e = tab[i]) != null; i = prevIndex(i, len)) if (e.get() == null) slotToExpunge = i; // 考虑哈希冲突的情况, 向后查找要插入的 key 是否存在 for (int i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) &#123; // 存在 key, 覆盖旧 value e.value = value; // 原先冲突的 key 已经被回收, 所以将该 Entry 与其交换 tab[i] = tab[staleSlot]; tab[staleSlot] = e; // slotToExpunge == staleSlot 代表 staleSlot 前没有 key 已被回收的 Entry if (slotToExpunge == staleSlot) // 因为交换过位置, 则 slotToExpunge 也要改为 i slotToExpunge = i; // 从 slotToExpunge 位置开始清理 cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); return; &#125; if (k == null &amp;&amp; slotToExpunge == staleSlot) // staleSlot 前没有 key 已被回收的 Entry, 则 slotToExpunge 从这个位置开始 slotToExpunge = i; &#125; // 找不到存在的 key 则新建一个 Entry 插入 tab[staleSlot].value = null; tab[staleSlot] = new Entry(key, value); // 如果 slotToExpunge != staleSlot 说明还存在其他 key 已被回收的 Entry, 进行清理 if (slotToExpunge != staleSlot) cleanSomeSlots(expungeStaleEntry(slotToExpunge), len);&#125;private int expungeStaleEntry(int staleSlot) &#123; Entry[] tab = table; int len = tab.length; // 将 value 和对应位置置空, 使其能够被回收 tab[staleSlot].value = null; tab[staleSlot] = null; size--; // 继续向后清理, 直到 tab[i] 为 null Entry e; int i; for (i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == null) &#123; e.value = null; tab[i] = null; size--; &#125; else &#123; // 将哈希冲突的 Entry 往前移填补清理后空出来的位置 int h = k.threadLocalHashCode &amp; (len - 1); if (h != i) &#123; tab[i] = null; while (tab[h] != null) h = nextIndex(h, len); tab[h] = e; &#125; &#125; &#125; // 返回 staleSlot 后下一个为 null 的位置 return i;&#125;private boolean cleanSomeSlots(int i, int n) &#123; boolean removed = false; Entry[] tab = table; int len = tab.length; do &#123; i = nextIndex(i, len); Entry e = tab[i]; // 发现 key 为 null 则进行清理 if (e != null &amp;&amp; e.get() == null) &#123; n = len; removed = true; i = expungeStaleEntry(i); &#125; // 每次 n 取半 // 这是权衡后的算法, 虽然不能全部扫描但能使清理工作能保持在 O(log2(n)) // 避免插入时会有 O(n) 的时间消耗 &#125; while ( (n &gt;&gt;&gt;= 1) != 0); return removed;&#125;private void rehash() &#123; // 清理 expungeStaleEntries(); // 清理完之后大小还是不够 3/4 阈值的话进行扩容 if (size &gt;= threshold - threshold / 4) resize();&#125;private void resize() &#123; Entry[] oldTab = table; int oldLen = oldTab.length; int newLen = oldLen * 2; Entry[] newTab = new Entry[newLen]; int count = 0; // 从旧数组循环取出 Entry 放入新数组 for (int j = 0; j &lt; oldLen; ++j) &#123; Entry e = oldTab[j]; if (e != null) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == null) &#123; e.value = null; // Help the GC &#125; else &#123; int h = k.threadLocalHashCode &amp; (newLen - 1); while (newTab[h] != null) h = nextIndex(h, newLen); newTab[h] = e; count++; &#125; &#125; &#125; setThreshold(newLen); size = count; table = newTab;&#125;// 遍历并调用 expungeStaleEntry(j) 清理 Stale Entryprivate void expungeStaleEntries() &#123; Entry[] tab = table; int len = tab.length; for (int j = 0; j &lt; len; j++) &#123; Entry e = tab[j]; if (e != null &amp;&amp; e.get() == null) expungeStaleEntry(j); &#125;&#125; get()1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public T get() &#123; // 获取当前线程实例 Thread t = Thread.currentThread(); // 从当前线程实例中取出 map ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result; &#125; &#125; // 未初始化 map, 初始化 map 并返回初始值 return setInitialValue();&#125;ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125;private Entry getEntry(ThreadLocal&lt;?&gt; key) &#123; int i = key.threadLocalHashCode &amp; (table.length - 1); Entry e = table[i]; if (e != null &amp;&amp; e.get() == key) return e; else // 存在哈希冲突的情况, 无法直接根据坐标取到想要的 Entry return getEntryAfterMiss(key, i, e);&#125;private Entry getEntryAfterMiss(ThreadLocal&lt;?&gt; key, int i, Entry e) &#123; Entry[] tab = table; int len = tab.length; // 向后查找 key, 遇到 Stale Entry 则进行清理 while (e != null) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) return e; if (k == null) expungeStaleEntry(i); else i = nextIndex(i, len); e = tab[i]; &#125; return null;&#125;private T setInitialValue() &#123; T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value;&#125; 关于内存泄露 ThreadLocal 对象实际上存放在 Thread 实例中 threadLocals 成员的 Entry 数组里 如果线程一直处于活跃状态, 则 ThreadLocal 对象就算离开具体业务逻辑的作用域, 也因为 threadLocals 持有强引用而无法被回收 所以 ThreadLocalMap 已经将 Entry 设为弱引用, gc 会在 ThreadLocal 离开作用域后对其回收, 但是这是针对 key 的, Entry 持有的 value 却不会被回收 针对这种情况, ThreadLocal 中 get()、set()、remove()这些方法中都存在清理 threadLocals 中 key 为 null 的逻辑, 起到了惰性删除释放内存的作用 InheritableThreadLocal InheritableThreadLocal 是为了解决在子线程中获取不到父线程 ThreadLocal 值的问题 Thread 中也持有对应的 inheritableThreadLocals, 并在 init 中有相关的初始化逻辑 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364class Thread implements Runnable &#123; /* * InheritableThreadLocal values pertaining to this thread. This map is * maintained by the InheritableThreadLocal class. */ ThreadLocal.ThreadLocalMap inheritableThreadLocals = null; private void init(ThreadGroup g, Runnable target, String name, long stackSize, AccessControlContext acc, boolean inheritThreadLocals) &#123; ...... Thread parent = currentThread(); // 如果存在 inheritableThreadLocals 则传递给新建出来的子线程 if (inheritThreadLocals &amp;&amp; parent.inheritableThreadLocals != null) this.inheritableThreadLocals = ThreadLocal.createInheritedMap(parent.inheritableThreadLocals); ...... &#125; ...... &#125; public class ThreadLocal&lt;T&gt; &#123; static ThreadLocalMap createInheritedMap(ThreadLocalMap parentMap) &#123; return new ThreadLocalMap(parentMap); &#125; static class ThreadLocalMap &#123; private ThreadLocalMap(ThreadLocalMap parentMap) &#123; Entry[] parentTable = parentMap.table; int len = parentTable.length; setThreshold(len); table = new Entry[len]; // 逐一复制 parentMap 的记录 for (int j = 0; j &lt; len; j++) &#123; Entry e = parentTable[j]; if (e != null) &#123; @SuppressWarnings("unchecked") ThreadLocal&lt;Object&gt; key = (ThreadLocal&lt;Object&gt;) e.get(); if (key != null) &#123; Object value = key.childValue(e.value); Entry c = new Entry(key, value); int h = key.threadLocalHashCode &amp; (len - 1); while (table[h] != null) h = nextIndex(h, len); table[h] = c; size++; &#125; &#125; &#125; &#125; ...... &#125; ......&#125; 重写了 ThreadLocal 的三个方法 1234567891011121314public class InheritableThreadLocal&lt;T&gt; extends ThreadLocal&lt;T&gt; &#123; protected T childValue(T parentValue) &#123; return parentValue; &#125; ThreadLocalMap getMap(Thread t) &#123; return t.inheritableThreadLocals; &#125; void createMap(Thread t, T firstValue) &#123; t.inheritableThreadLocals = new ThreadLocalMap(this, firstValue); &#125;&#125; 线程池 ( ThreadPoolExecutor) 线程池是为了避免线程频繁的创建和销毁带来的性能消耗, 而建立的一种池化技术, 它是把已创建的线程放入“池”中, 当有任务来临时就可以重用已有的线程, 无需等待创建的过程, 这样就可以有效提高程序的响应速度 构造函数 12345678910public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; ......&#125; corePoolSize 表示线程池的常驻核心线程数 如果设置为 0, 则表示在没有任何任务时, 销毁线程池 如果大于 0, 即使没有任务时也会保证线程池的线程数量等于此值 此值如果设置的比较小, 则会频繁的创建和销毁线程 如果设置的比较大, 则会浪费系统资源, 所以开发者需要根据自己的实际业务来调整此值 maximumPoolSize 表示线程池在任务最多时, 最大可以创建的线程数 此值必须大于 0, 也必须大于等于 corePoolSize, 此值只有在任务比较多, 且不能存放在任务队列时才会用到 keepAliveTime 表示线程的存活时间 当线程池空闲时并且超过了此时间, 多余的线程就会销毁, 直到线程池中的线程数量销毁的等于 corePoolSize 为止 如果 maximumPoolSize 等于 corePoolSize, 那么线程池在空闲的时候也不会销毁任何线程 unit 表示存活时间的单位, 它是配合 keepAliveTime 参数共同使用的 workQueue 表示线程池执行的任务队列 当线程池的所有线程都在处理任务时, 如果来了新任务就会缓存到此任务队列中排队等待执行 threadFactory 表示线程的创建工厂, 此参数一般用的比较少, 我们通常在创建线程池时不指定此参数, 它会使用默认的线程创建工厂的方法来创建线程: 123456789101112131415161718192021222324252627// 默认的线程创建工厂, 需要实现 ThreadFactory 接口static class DefaultThreadFactory implements ThreadFactory &#123; private static final AtomicInteger poolNumber = new AtomicInteger(1); private final ThreadGroup group; private final AtomicInteger threadNumber = new AtomicInteger(1); private final String namePrefix; DefaultThreadFactory() &#123; SecurityManager s = System.getSecurityManager(); group = (s != null) ? s.getThreadGroup() : Thread.currentThread().getThreadGroup(); namePrefix = "pool-" + poolNumber.getAndIncrement() + "-thread-"; &#125; // 创建线程 public Thread newThread(Runnable r) &#123; Thread t = new Thread(group, r, namePrefix + threadNumber.getAndIncrement(), 0); if (t.isDaemon()) t.setDaemon(false); // 创建一个非守护线程 if (t.getPriority() != Thread.NORM_PRIORITY) t.setPriority(Thread.NORM_PRIORITY); // 线程优先级设置为默认值 return t; &#125;&#125; 我们也可以自定义一个线程工厂, 通过实现 ThreadFactory 接口来完成, 这样就可以自定义线程的名称或线程执行的优先级了 RejectedExecutionHandler 表示指定线程池的拒绝策略 当线程池的任务已经在缓存队列 workQueue 中存储满了之后, 并且不能创建新的线程来执行此任务时, 就会用到此拒绝策略, 它属于一种限流保护的机制 ctl 123456789101112131415private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));private static final int COUNT_BITS = Integer.SIZE - 3;private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1;// runState is stored in the high-order bitsprivate static final int RUNNING = -1 &lt;&lt; COUNT_BITS;private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;private static final int STOP = 1 &lt;&lt; COUNT_BITS;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS;// Packing and unpacking ctlprivate static int runStateOf(int c) &#123; return c &amp; ~CAPACITY; &#125;private static int workerCountOf(int c) &#123; return c &amp; CAPACITY; &#125;private static int ctlOf(int rs, int wc) &#123; return rs | wc; &#125; 用一个 AtomicInteger 包装两个字段: 高 3 位保存 runState, 低 29 位保存 workerCount 用一个变量去存储两个值, 可避免在做相关决策时, 出现不一致的情况, 不必为了维护两者的一致, 而占用锁资源 workerCount: 有效线程数 runState: 线程池的运行状态 定义 RUNNING: 接受新任务并处理排队的任务 SHUTDOWN: 拒绝接受新任务, 但是会处理还在排队的任务 STOP: 拒绝接受新任务, 也不处理排队中任务, 并且会中断正在执行的任务 TIDYING: 所有任务都已经停止, workerCount 为 0, 转换为状态 TIDYING 的线程将运行 terminated() 方法 TERMINATED: terminated() 执行完毕 这些值之间的数字顺序很重要, 可以进行有序的比较 runState 随着时间逐步增加, 但不一定达到每个状态, 过渡的顺序为: RUNNING -&gt; SHUTDOWN, 在调用 shutdown() 时, 可能隐藏在 finalize() 中调用 (RUNNING or SHUTDOWN) -&gt; STOP, 在调用 shutdownNow() 时 SHUTDOWN -&gt; TIDYING, 当队列和池子内的任务都为空时 STOP -&gt; TIDYING, 当池子内的任务为空时 TIDYING -&gt; TERMINATED, 当 terminated() 执行完毕时 线程池工作流程 通过 execute() 执行任务12345678910111213141516171819202122232425262728public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); int c = ctl.get(); // 当前工作的线程数小于核心线程数 if (workerCountOf(c) &lt; corePoolSize) &#123; // 创建新的线程执行此任务, 传入 true 以核心线程数作为判断阈值 if (addWorker(command, true)) return; // 创建失败, 说明 ctl 有变化, 重新获取 c = ctl.get(); &#125; // 检查线程池是否处于运行状态, 如果是则把任务添加到队列 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); // 再次检查线程池是否处于运行状态, 防止在第一次校验通过后线程池关闭 // 如果是非运行状态, 则将刚加入队列的任务移除 if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); // 如果线程池的线程数为 0 时（当 corePoolSize 设置为 0 时会发生） else if (workerCountOf(recheck) == 0) addWorker(null, false); // 新建线程执行任务, 传入 false 以最大线程数作为判断阈值 &#125; // 核心线程和队列都满了, 新建非核心线程执行 else if (!addWorker(command, false)) // 新建线程失败, 执行拒绝策略 reject(command);&#125; addWorker(Runnable firstTask, boolean core) 方法 firstTask, 线程应首先运行的任务, 如果没有则可以设置为 null core, 判断是否可以创建线程的阀值（最大值）, 如果等于 true 则表示使用 corePoolSize 作为阀值, false 则表示使用 maximumPoolSize 作为阀值 Worker构造函数 123456789101112private final class Worker extends AbstractQueuedSynchronizer implements Runnable &#123; final Thread thread; // Worker 持有的线程 Runnable firstTask; // 初始化的任务, 可以为 null Worker(Runnable firstTask) &#123; setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this); &#125; ......&#125; 执行任务流程 继承 AQS 原因分析Worker 是通过继承 AQS, 使用 AQS 来实现独占锁这个功能。不用可重入锁 ReentrantLock 而用 AQS, 为的就是实现不可重入的特性去反应线程现在的执行状态 Worker.lock 方法一旦获取了独占锁, 表示当前线程正在执行任务中, 正在执行任务的线程不应该被中断 如果正在执行任务，则不应该中断线程 线程池在执行 shutdown 方法或 tryTerminate 方法时会调用 interruptIdleWorkers 方法来中断空闲的线程, interruptIdleWorkers 方法会使用 tryLock 方法来判断线程是否在执行任务, 如果是空闲状态则可以安全回收 之所以要不可重入, 是为了避免在 Worker 中会调用到线程池 interruptIdleWorkers , 像 setCorePoolSize 方法。如果使用 ReentrantLock, 它是可重入的, 这样会导致该 Worker 自己被中断 此外, 在构造方法中执行了setState(-1);, 把 state 变量设置为 -1, 是因为 AQS 默认的 state 是 0, 如果刚创建了一个 Worker 对象, 还没有执行任务时, 这时就不应该被中断： 12345678910111213protected boolean tryAcquire(int unused) &#123; if (compareAndSetState(0, 1)) &#123; setExclusiveOwnerThread(Thread.currentThread()); return true; &#125; return false;&#125;protected boolean tryRelease(int unused) &#123; setExclusiveOwnerThread(null); setState(0); return true;&#125; tryAcquire 方法是根据 state 是否是 0 来判断的, 所以, setState(-1); 将 state 设置为 -1 是为了防止在执行任务前就中断了线程 在 runWorker 方法中会先调用 Worker 对象的 unlock 方法将 state 设置为 0, 允许中断和 Worker.lock 相关参数1234567891011121314// 用于操作 workers private final ReentrantLock mainLock = new ReentrantLock();// 持有线程的引用, 管理线程的生命周期private final HashSet&lt;Worker&gt; workers = new HashSet&lt;Worker&gt;();// 用于通知线程池终止完毕private final Condition termination = mainLock.newCondition();// 线程池曾经创建过的最大线程数量private int largestPoolSize;// 线程池已经执行的和未执行的任务总数private long completedTaskCount; 为什么workers 不采用线程安全的集合 ? 有许多复合的操作, 比如说将 worker 添加到 workers 后还需要判断是否需要更新 largestPoolSize 等, workers 只在获取到 mainLock 的情况下才会进行读写 mainLock 也用于在中断线程的时候串行执行, 否则可能会并发进行线程中断, 引起不必要的中断高峰 addWorker : 增加工作线程1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); if (rs &gt;= SHUTDOWN &amp;&amp; // 线程池是否已停止 ! (rs == SHUTDOWN &amp;&amp; // 线程池是否正在停止 firstTask == null &amp;&amp; ! workQueue.isEmpty()) // 线程是否用于执行剩余任务 ) return false; for (;;) &#123; int wc = workerCountOf(c); if (wc &gt;= CAPACITY || // 线程数是否超过容量 wc &gt;= (core ? corePoolSize : maximumPoolSize)) // 是否超过判断的阀值 return false; if (compareAndIncrementWorkerCount(c)) // CAS 尝试登记线程数 break retry; // 登记成功 c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) // 判断线程池状态运行过程中是否有改变 continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); workers.add(w); // 持有引用 int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; // 更新创建过的最大线程数 workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; t.start(); // 启动线程, 而线程的 run 方法就是执行 runWorker() workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) addWorkerFailed(w); &#125; return workerStarted;&#125; runWorker : 不断获取任务并执行 Worker 被创建出来后, 就会不断地进行轮询, 然后获取任务去执行, 核心线程可以无限等待获取任务, 非核心线程要限时获取任务 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455final void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); // 获取第一个任务 Runnable task = w.firstTask; w.firstTask = null; // 允许中断 w.unlock(); // allow interrupts // 是否因为异常退出循环 boolean completedAbruptly = true; try &#123; // 如果task为空, 则通过getTask来获取任务 while (task != null || (task = getTask()) != null) &#123; w.lock(); // If pool is stopping, ensure thread is interrupted; // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt /** * 确保只有在线程 stoping 时，才会被设置中断标示，否则清除中断标示 * 1、如果线程池状态 &gt;= stop，且当前线程没有设置中断状态，wt.interrupt() * 2、如果一开始判断线程池状态 &lt; stop，但 Thread.interrupted() 为 true (调用的同时清除了中断标示)，即线程已经被中断，再次判断线程池状态是否 &gt;= stop * 是，再次设置中断标示，wt.interrupt() * 否，不做操作，清除中断标示后进行后续步骤 */ if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try &#123; beforeExecute(wt, task); Throwable thrown = null; try &#123; task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; thrown = x; throw new Error(x); &#125; finally &#123; afterExecute(task, thrown); &#125; &#125; finally &#123; task = null; w.completedTasks++; w.unlock(); &#125; &#125; // 标明是正常退出 completedAbruptly = false; &#125; finally &#123; processWorkerExit(w, completedAbruptly); &#125;&#125; getTask : 从任务队列获取任务1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162private Runnable getTask() &#123; // timeOut 表示上次从阻塞队列中取任务时是否超时 boolean timedOut = false; // Did the last poll() time out? for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. /* * 1. 线程池已经 stop * 2. 线程池处于 shutdown 并且队列为空 * 如果以上任何条件满足, 则将 workerCount 减 1 并返回 null */ if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; decrementWorkerCount(); return null; &#125; int wc = workerCountOf(c); // Are workers subject to culling? // timed 用于判断是否需要进行超时控制 // allowCoreThreadTimeOut 默认是 false, 也就是核心线程不允许进行超时 // wc &gt; corePoolSize, 表示当前线程池中的线程数量大于核心线程数量 // 对于超过核心线程数量的这些线程, 需要进行超时控制 boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; /* * 1. 判断 wc &gt; maximumPoolSize 是因为可能通过 setMaximumPoolSize 修改过 maximumPoolSize * 2. timed &amp;&amp; timedOut 如果为 true, 表示当前操作需要进行超时控制, 并且上次从阻塞队列中获取任务发生了超时 * 满足 1 或 2 并且如果有效线程数量大于 1 或者阻塞队列是空的, 那么尝试将 workerCount 减 1 * 判断 wc &gt; 1 是防止在 allowCoreThreadTimeOut 为 true 或 corePoolSize 为 0 时无线程执行还在等待中的任务 * 如果减 1 失败, 则返回重试 */ if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; if (compareAndDecrementWorkerCount(c)) return null; continue; &#125; try &#123; /* * 根据 timed 来判断, 如果为 true, 则通过阻塞队列的 poll 方法进行超时控制 * 如果在 keepAliveTime 时间内没有获取到任务, 则返回 null * 否则通过 take 方法, 如果这时队列为空, 则 take 方法会阻塞直到队列不为空。 * */ Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; // 如果 r == null, 说明已经超时, timedOut 设置为 true timedOut = true; &#125; catch (InterruptedException retry) &#123; // 如果获取任务时当前线程发生了中断, 则设置 timedOut 为 false 并返回循环重试 timedOut = false; &#125; &#125;&#125; processWorkerExit : 线程回收 线程池中线程的销毁依赖 JVM 的垃圾回收, 当线程池决定哪些线程需要回收时, 只需要将其引用消除即可 当 Worker 无法获取到任务, 也就是获取的任务为空时, 循环会结束, Worker 会主动消除自身在线程池内的引用 线程回收的工作在 processWorkerExit 方法内完成 12345678910111213141516171819202122232425262728293031323334353637private void processWorkerExit(Worker w, boolean completedAbruptly) &#123; // 如果 completedAbruptly 值为 true, 则说明线程执行时出现了异常, 需要将 workerCount 减 1 // 如果线程执行时没有出现异常, 说明在 getTask() 方法中已经已经对 workerCount 进行了减 1 操作 if (completedAbruptly) // If abrupt, then workerCount wasn't adjusted decrementWorkerCount(); final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // 统计完成的任务数 completedTaskCount += w.completedTasks; // 从 workers 中移除, 也就表示着从线程池中移除了一个工作线程 workers.remove(w); &#125; finally &#123; mainLock.unlock(); &#125; // 根据线程池状态进行判断是否结束线程池 tryTerminate(); int c = ctl.get(); /* * 当线程池是 RUNNING 或 SHUTDOWN 状态时, 如果 worker 是异常结束, 那么会直接 addWorker * 如果 allowCoreThreadTimeOut 为 true, 并且等待队列有任务, 至少保留一个 worker * 如果 allowCoreThreadTimeOut 为 false, workerCount 不少于 corePoolSize */ if (runStateLessThan(c, STOP)) &#123; if (!completedAbruptly) &#123; int min = allowCoreThreadTimeOut ? 0 : corePoolSize; if (min == 0 &amp;&amp; ! workQueue.isEmpty()) min = 1; if (workerCountOf(c) &gt;= min) return; // replacement not needed &#125; addWorker(null, false); &#125;&#125; 事实上在这个方法中, 将线程引用移出线程池就已经结束了线程销毁的部分。但由于引起线程销毁的可能性有很多, 线程池还要判断是什么引发了这次销毁, 是否要改变线程池的现阶段状态, 是否要根据新状态, 重新分配线程 tryTerminate : 根据状态判断是否结束12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849final void tryTerminate() &#123; for (;;) &#123; int c = ctl.get(); /* * 当前线程池的状态为以下几种情况时, 直接返回： * 1. RUNNING, 因为还在运行中, 不能停止 * 2. TIDYING 或 TERMINATED, 说明正在或者已经在终止 * 3. SHUTDOWN 并且等待队列非空, 这时要执行完 workQueue 中的 task； */ if (isRunning(c) || runStateAtLeast(c, TIDYING) || (runStateOf(c) == SHUTDOWN &amp;&amp; ! workQueue.isEmpty())) return; // 到这个位置为以下情况之一 // 1. 线程池的状态为 SHUTDOWN 并且等待队列为空 // 2. 线程池的状态为 STOP // 如果线程数量不为 0, 则中断一个空闲的工作线程, 并返回 if (workerCountOf(c) != 0) &#123; // Eligible to terminate interruptIdleWorkers(ONLY_ONE); return; &#125; // 到这个位置则说明线程数量为 0 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // 尝试设置状态为 TIDYING, 如果成功则调用 terminated 方法 if (ctl.compareAndSet(c, ctlOf(TIDYING, 0))) &#123; try &#123; // terminated 方法默认什么都不做, 留给子类实现 terminated(); &#125; finally &#123; // terminated() 执行完毕, 设置状态为 TERMINATED ctl.set(ctlOf(TERMINATED, 0)); // 通知完成终止 termination.signalAll(); &#125; return; &#125; &#125; finally &#123; mainLock.unlock(); &#125; // else retry on failed CAS // 没设置成功则继续 CAS 尝试 &#125;&#125; shutdown , shutdownNow : 关闭线程池123456789101112131415161718192021222324252627282930313233343536public void shutdown() &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // 安全策略判断 checkShutdownAccess(); // 切换状态为 SHUTDOWN advanceRunState(SHUTDOWN); // 中断空闲线程 interruptIdleWorkers(); onShutdown(); // hook for ScheduledThreadPoolExecutor &#125; finally &#123; mainLock.unlock(); &#125; // 尝试结束线程池 tryTerminate();&#125;public List&lt;Runnable&gt; shutdownNow() &#123; List&lt;Runnable&gt; tasks; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; checkShutdownAccess(); // 设置状态为 STOP advanceRunState(STOP); // 中断所有工作线程 interruptWorkers(); // 取出队列中没有被执行的任务 tasks = drainQueue(); &#125; finally &#123; mainLock.unlock(); &#125; tryTerminate(); return tasks;&#125; interruptIdleWorkers, interruptWorkers : 中断工作线程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354private void interruptIdleWorkers() &#123; interruptIdleWorkers(false);&#125;private void interruptIdleWorkers(boolean onlyOne) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; for (Worker w : workers) &#123; Thread t = w.thread; // 通过 w.tryLock 判断是否为空闲 if (!t.isInterrupted() &amp;&amp; w.tryLock()) &#123; try &#123; t.interrupt(); &#125; catch (SecurityException ignore) &#123; &#125; finally &#123; w.unlock(); &#125; &#125; if (onlyOne) break; &#125; &#125; finally &#123; mainLock.unlock(); &#125;&#125;private void interruptWorkers() &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; for (Worker w : workers) w.interruptIfStarted(); &#125; finally &#123; mainLock.unlock(); &#125;&#125;private final class Worker extends AbstractQueuedSynchronizer implements Runnable &#123; ...... void interruptIfStarted() &#123; Thread t; // 如果线程已经启动, 中断线程 if (getState() &gt;= 0 &amp;&amp; (t = thread) != null &amp;&amp; !t.isInterrupted()) &#123; try &#123; t.interrupt(); &#125; catch (SecurityException ignore) &#123; &#125; &#125; &#125;&#125; awaitTermination : 等待线程池完成终止123456789101112131415161718public boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException &#123; long nanos = unit.toNanos(timeout); final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; for (;;) &#123; if (runStateAtLeast(ctl.get(), TERMINATED)) return true; if (nanos &lt;= 0) return false; // 通过 termination 进行等待 nanos = termination.awaitNanos(nanos); &#125; &#125; finally &#123; mainLock.unlock(); &#125;&#125; 相关问题1. ThreadPoolExecutor 的执行方法有几种？它们有什么区别？ execute() VS submit() 都是用来执行线程池任务, 它们最主要的区别是 submit() 方法可以接收线程池执行的返回值, 而 execute() 不能接收返回值 123456789101112131415161718ThreadPoolExecutor executor = new ThreadPoolExecutor(2, 10, 10L, TimeUnit.SECONDS, new LinkedBlockingQueue(20));// execute 使用executor.execute(new Runnable() &#123; @Override public void run() &#123; System.out.println("Hello, execute."); &#125;&#125;);// submit 使用Future&lt;String&gt; future = executor.submit(new Callable&lt;String&gt;() &#123; @Override public String call() throws Exception &#123; System.out.println("Hello, submit."); return "Success"; &#125;&#125;);System.out.println(future.get()); execute() 方法属于 Executor 接口的方法, 而 submit() 方法则是属于 ExecutorService 接口的方法 在 submit() 中处理的任务如果抛出异常, 只有在调用返回的 Future 对象 get 方法时才会抛出 2. 拒绝策略的分类有哪些? 如何自定义拒绝策略？ 自带的拒绝策略有 4 种: AbortPolicy, 终止策略, 线程池会抛出异常并终止执行, 它是默认的拒绝策略 CallerRunsPolicy, 把任务交给当前线程来执行 DiscardPolicy, 忽略此任务 DiscardOldestPolicy, 忽略最早的任务（最先加入队列的任务） 自定义拒绝策略 自定义拒绝策略只需要新建一个 RejectedExecutionHandler 对象, 然后重写它的 rejectedExecution() 方法即可 1234567891011121314ThreadPoolExecutor executor = new ThreadPoolExecutor(1, 3, 10, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(2), new RejectedExecutionHandler() &#123; // 添加自定义拒绝策略 @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) &#123; // 业务处理方法 System.out.println("执行自定义拒绝策略"); &#125; &#125;);for (int i = 0; i &lt; 6; i++) &#123; executor.execute(() -&gt; &#123; System.out.println(Thread.currentThread().getName()); &#125;);&#125; 3. 线程池的工作队列有哪些? ArrayBlockingQueue, 是一个用数组实现的有界阻塞队列, 按 FIFO 排序任务, 支持公平锁和非公平锁 LinkedBlockingQueue, 基于链表结构的阻塞队列, 按 FIFO 排序任务, 容量可以选择进行设置, 不设置的话, 将是一个无边界的阻塞队列, 最大长度为 Integer.MAX_VALUE, 吞吐量通常要高于 ArrayBlockingQuene DelayQueue, 是一个任务定时周期的延迟执行的队列。根据指定的执行时间从小到大排序, 否则根据插入到队列的先后排序 PriorityBlockingQueue, 是具有优先级的无界阻塞队列, 不能保证同优先级元素的顺序 SynchronousQueue, 一个不存储元素的阻塞队列, 每个插入操作必须等到另一个线程调用移除操作, 否则插入操作一直处于阻塞状态, 吞吐量通常要高于 LinkedBlockingQueue LinkedBlockingDeque, 一个由链表结构组成的双向阻塞队列, 队列头尾都可以插入和移除元素, 多线程并发时, 可以将锁的竞争最多 降到一半 4. ThreadPoolExecutor 如何实现扩展？ 通过重写 beforeExecute() 和 afterExecute() 方法, 我们可以在扩展方法中添加日志或者实现数据统计, 比如统计线程的执行时间 关于 Executors 内的线程池对象 Executors 源码中 Executors.newFixedThreadPool()、Executors.newSingleThreadExecutor() 和 Executors.newCachedThreadPool() 等方法的底层都是通过 ThreadPoolExecutor 实现的 FixedThreadPool (固定数目线程的线程池) 适用于处理 CPU 密集型的任务, 确保 CPU 在长期被工作线程使用的情况下, 尽可能的少的分配线程 特点 核心线程数和最大线程数大小一样 keepAliveTime 为 0 阻塞队列为 LinkedBlockingQueue CachedThreadPool (可缓存线程的线程池) 适用于并发执行大量短期的小任务 特点 核心线程数为 0 最大线程数为 Integer.MAX_VALUE 阻塞队列为 SynchronousQueue 非核心线程空闲存活时间为 60 秒 SingleThreadExecutor (单线程的线程池) 适用于串行执行任务的场景, 一个任务一个任务地执行 特点 核心线程数为 1 最大线程数也为 1 阻塞队列是 LinkedBlockingQueue keepAliveTime 为 0 ScheduledThreadPool (定时及周期执行的线程池) 周期性执行任务的场景, 需要限制线程数量的场景 特点 最大线程数为 Integer.MAX_VALUE 阻塞队列是 DelayedWorkQueue keepAliveTime 为 0 scheduleAtFixedRate() 按某种速率周期执行 scheduleWithFixedDelay() 在某个延迟后执行 在阿里巴巴的《 Java 开发手册 》中是这样规定的： 线程池不允许使用 Executors 去创建, 而是通过 ThreadPoolExecutor 的方式, 这样的处理方式让写的读者更加明确线程池的运行规则, 规避资源耗尽的风险。 Executors 返回的线程池对象的弊端如下： FixedThreadPool 和 SingleThreadPool：允许的请求队列长度为 Integer.MAX_VALUE, 可能会堆积大量的请求, 从而导致 OOM CachedThreadPool 和 ScheduledThreadPool：允许的创建线程数量为 Integer.MAX_VALUE, 可能会创建大量的线程, 从而导致 OOM 参考 https://www.jianshu.com/p/f4454164c017 http://www.jasongj.com/java/threadlocal https://www.hollischuang.com/archives/2030 https://tech.meituan.com/2020/04/02/java-pooling-pratice-in-meituan.html https://github.com/openjdk-mirror/jdk7u-hotspot/blob/50bdefc3afe944ca74c3093e7448d6b889cd20d1/src/share/vm/runtime/objectMonitor.cpp]]></content>
      <categories>
        <category>programming</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>multithread</tag>
        <tag>concurrency</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java HashMap]]></title>
    <url>%2F2020%2F03%2F14%2FJava-HashMap%2F</url>
    <content type="text"><![CDATA[HashMap 底层 在 JDK 1.7 中 HashMap 是以数组加链表的形式组成的，JDK 1.8 之后新增了红黑树的组成结构，当链表大于 8 时，链表结构会转换成红黑树结构 数组中的元素我们称之为哈希桶，它的定义如下 123456789101112131415161718192021222324252627282930313233343536373839static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + "=" + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125;&#125; 重要属性1234567891011121314151617// HashMap 初始化长度static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16// HashMap 最大长度static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; // 1073741824// 默认的加载因子 (扩容因子)static final float DEFAULT_LOAD_FACTOR = 0.75f;// 转换红黑树的临界值，当链表长度大于此值时，会把链表结构转换为红黑树结构static final int TREEIFY_THRESHOLD = 8;// 转换链表的临界值，当元素小于此值时，会将红黑树结构转换成链表结构static final int UNTREEIFY_THRESHOLD = 6;// 最小树容量static final int MIN_TREEIFY_CAPACITY = 64; 什么是加载因子？加载因子为什么是 0.75？ 加载因子也叫扩容因子或负载因子，用来判断什么时候该进行扩容 假如加载因子是 0.5，HashMap 的初始化容量是 16，那么当 HashMap 中有 16 * 0.5 = 8 个元素时，HashMap 就会进行扩容 0.75 是出于容量和性能之间平衡的结果 当加载因子设置比较大的时候，扩容的门槛就被提高了，扩容发生的频率比较低，占用的空间会比较小，但此时发生 Hash 冲突的几率就会提升，因此需要更复杂的数据结构来存储元素，这样对元素的操作时间就会增加，运行效率也会因此降低 而当加载因子值比较小的时候，扩容的门槛会比较低，因此会占用更多的空间，此时元素的存储就比较稀疏，发生哈希冲突的可能性就比较小，因此操作性能会比较高 还为了提升扩容效率，HashMap的容量（capacity）有一个固定的要求，那就是一定是2的幂。所以，如果负载因子是3/4的话，那么和capacity的乘积结果就可以是一个整数 重要方法查询 12345678910111213141516171819202122232425262728293031public V get(Object key) &#123; Node&lt;K,V&gt; e; // 对 key 进行哈希操作 return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; // 非空判断 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 判断第一个元素是否是要查询的元素 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; // 下一个节点非空判断 if ((e = first.next) != null) &#123; // 如果节点是树结构, 则使用 getTreeNode 直接获取相应的数据 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123; // 非树结构，循环节点判断 // hash 相等并且 key 相同，则返回此节点 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 当哈希冲突时需要通过判断 key 值是否相等来确认此元素是否是要查询的元素 新增 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 哈希表为空则创建表 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 根据 key 的哈希值计算出要插入的数组索引 i if ((p = tab[i = (n - 1) &amp; hash]) == null) // 如果 table[i] 等于 null, 则直接插入 tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 如果与第一个元素的 key 相等，直接覆盖第一个元素的 value if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 需要往后插入元素，先判断是否为红黑树 else if (p instanceof TreeNode) // 红黑树直接插入键值对 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; // 为链表结构，循环准备插入 for (int binCount = 0; ; ++binCount) &#123; // 下一个元素为空时直接插入 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); // 链表长度大于 8 转换为红黑树进行处理 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // 如果与下一个元素的 key 相等，直接覆盖下一个元素的 value if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 超过最大容量，扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 扩容 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495final Node&lt;K,V&gt;[] resize() &#123; // 扩容前的数组 Node&lt;K,V&gt;[] oldTab = table; // 扩容前的数组大小 int oldCap = (oldTab == null) ? 0 : oldTab.length; // 扩容前的扩容阈值 int oldThr = threshold; // 预定义新数组的大小和阈值 int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 超过最大值就不再扩容了 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 扩大容量为当前容量的两倍，但不能超过 MAXIMUM_CAPACITY else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; // 当前数组没有数据，使用初始化的值 else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults // 如果初始化的值为 0，则使用默认的初始化容量 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 如果新阈值为 0 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; // 开始扩容 table = newTab; // 原数据不为空，将原数据复制到新 table 中 if (oldTab != null) &#123; // 根据容量循环数组，复制非空元素到新 table for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) // 链表只有一个元素时直接赋值 newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) // 下个元素为红黑树时, 进行相关操作 ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order // 链表复制，JDK 1.8 扩容优化部分 // 用于连接扩容时位置不变的元素 Node&lt;K,V&gt; loHead = null, loTail = null; // 用于连接扩容时位置改变的元素 Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 原索引 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 原索引 + oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 将原索引放到哈希桶中 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 将原索引 + oldCap 放到哈希桶中 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; JDK 1.8 在扩容时并没有像 JDK 1.7 那样，重新计算每个元素的哈希值，而是通过 e.hash &amp; oldCap 来确定元素是否需要移动 如 key1 的信息如下： key1.hash = 10 -&gt; 0000 1010 oldCap = 16 -&gt; 0001 0000 使用 e.hash &amp; oldCap 得到的结果为 0，则在扩容时位置不会发生任何变化 而 key 2 信息如下： key2.hash = 26 -&gt; 0001 1010 oldCap = 16 -&gt; 0001 0000 这时候得到的结果不为 0，新的下标位置等于原下标位置 + 原数组长度 为什么要这么做 ? 首先这里的元素扩容前都是在同一个数组下标中的, 也就是 (oldCap - 1) &amp; hash 值相同 扩容的时候新容量是左移了一位的, 如下 oldCap = 16 -&gt; 0001 0000 newCap = 32 -&gt; 0010 0000 则相应的 oldCap -1 = 15 -&gt; 0000 1111 newCap - 1 = 31 -&gt; 0001 1111 所以对于 e.hash &amp; oldCap 不为 0 的元素, 需要放置到原索引 + oldCap 的位置, 因为在 newCap 的情况下插入该元素时, (newCap - 1) &amp; hash 值就是原索引 + oldCap 死循环分析 在 JDK 1.7 中, 假设 HashMap 默认大小为 2，原本 HashMap 中有一个元素 key(5) 我们再使用两个线程 t1 添加元素 key(3) t2 添加元素 key(7) 当元素 key(3) 和 key(7) 都添加到 HashMap 中之后，开始进行扩容操作 当线程 t1 在执行到 Entry&lt;K,V&gt; next = e.next; 时，交出了 CPU 的使用权，源码如下: 123456789101112131415void transfer(Entry[] newTable, boolean rehash) &#123; int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) &#123; while(null != e) &#123; Entry&lt;K,V&gt; next = e.next; // 线程一执行此处 if (rehash) &#123; e.hash = null == e.key ? 0 : hash(e.key); &#125; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; &#125;&#125; 此时线程 t1 中的 e 指向了 key(3)，而 next 指向了 key(7) 之后线程 t2 重新 rehash 之后链表的顺序被反转, 链表的位置变成了 key(5) → key(7) → key(3)，其中 “→” 用来表示下一个元素 当 t1 重新获得执行权之后，先执行 newTalbe[i] = e; 把 key(3) 的 next 设置为 key(7) 而下次循环时查询到 key(7) 的 next 元素为 key(3)，于是就形成了 key(3) 和 key(7) 的循环引用 发生死循环的原因是 JDK 1.7 链表插入方式为首部倒序插入，这个问题在 JDK 1.8 得到了改善，变成了尾部正序插入 HashMap 本身就是非线程安全的, 所以不建议在多线程下使用]]></content>
      <categories>
        <category>programming</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>hashmap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 关键字 final]]></title>
    <url>%2F2020%2F03%2F13%2FJava-%E5%85%B3%E9%94%AE%E5%AD%97-final%2F</url>
    <content type="text"><![CDATA[final方法 被 final 修饰的方法不可被重写。它可以防止任何继承类修改方法的意义和实现 而且使用 final 修饰方法的执行效率一般高于普通方法，这里不得不提一下 Java 的内联（inline）机制 ​ 当我们调用方法时，实际上是将程序的执行转移到该方法所在的内存地址上，将该方法执行完后，再返回到执行该方法前的位置，这种转移操作要求在转移前保存当前的数据以及内存地址，在执行完后再恢复现场，继续按照转移前的地址执行，也就是通常所说的压栈和出栈（这段文字有点绕口，简单来说，比如 A 方法在执行到第 10 行的时候调用了 B 方法，JVM 会先保存 A 方法当前数据和执行地址，然后跳转到 B 方法所在的内存地址执行 B 方法，执行完后，再返回 A 方法第十行继续执行），因此，函数调用有一定时间和空间方面的开销，对于函数体积不大，但是频繁调用的函数来说，这个开销就会放大。​ 因此，对于这种函数体积不大又频繁调用的的方法，我们可以通过内联函数来提升运行效率，当我们对一个方法使用 final 修饰时，这个方法就有可能成为内联函数（JVM 会根据方法的执行效率决定是否内联）。 内联前： 1234567891011121314class A &#123; int value; public final int get()&#123; return value; &#125;&#125;public class B &#123; public void sum() &#123; A a = new A(); //调用a的get方法 int x = a.get(); &#125;&#125; 内联后： 1234567891011121314class A &#123; int value; public final int get()&#123; return value; &#125;&#125;public class B &#123; public void sum() &#123; A a = new A(); //此处将get方法展开为内联函数 int x = a.value; &#125;&#125; 类 当 final 修饰一个类时，表明其为最终类，它不能被继承 并且类中所有的属性和方法都默认是 final 类型，如 String，Integer 等包装类均为 final 类 方法默认被修饰为 final ，这时方法的内联起到作用了, 这种空间置换时间的策略需要一个平衡点（break-even），如果一个方法过于大，copy 的副本数量过于多，那么这样的平衡就会被打破，优化的目的反而失去了意义 变量 修饰基本类型变量时，变量的值不可改变 修饰引用变量时，变量指向的对象地址不可改变 这里还涉及到了一个类似 C 语言的宏替换概念，由于 final 修饰的 String 变量不可更改，所以，当一个 String 变量被 final 修饰时，这个值在编译期就可以确定，所以将该变量直接替换为它对应的值，如下： 123456789101112131415161718192021public class test &#123; public static void main(String[] args) &#123; final String a = "hello"; String b = "hello"; final String c = "world"; String d = "hello" + "world"; String e = a + c; String f = b + c; String g = "helloworld"; // 在编译期，由于 a 和 c 的值已经确定并且不会再更改（效果同 d）， // 所以 e 的值能够在编译期就确定下来，直接指向了常量区的 g，前两个均为 true System.out.println(g == d); // true System.out.println(g == e); // true // 由于 b 值的不确定性，所以在编译期不能确定其值，只能在运行时确认 System.out.println(g == f); // false // 注意: 以上比较的是变量的地址 &#125;&#125; 参数 final 修饰的参数有一个只读的属性，即可以读取该参数，但是无法更改参数的值，同修饰变量一样，当参数为基本类型时，该参数的值不可改变；当参数为引用类型时，参数的引用地址不可改变。 有什么意义? 对于通过入参引用修改对象时, 可以防止在函数中通过赋值修改该引用, 避免修改失败 可以在函数中将入参再传递给匿名函数 (lambda), 因为该入参被 final 修饰后是存在于常量池中, 而不是函数的调用栈, 这样该函数调用完之后匿名函数还是能读到该入参的值 final 的内存语义 volatile 可以禁止指令重排序，final 同样有这样的作用，对于 final 域，编译器和处理器要遵守两个重排序规则 在构造函数内对一个 final 域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 意思是说，在对象引用为任意线程可见之前，对象的 final 域已经被正确初始化了（JVM 禁止把 final 域的写重排序到构造函数之外，要保证该效果，还要确保 final 引用没有从构造函数溢出）。 初次读一个包含 final 域的对象的引用，与随后初次读这个 final 域，这两个操作之间不能重排序。 意思是说，在读一个对象的 final 域之前，一定会先读包含这个 final 域的对象的引用。 对于内存语义这块，还需要结合代码去理解，参考《Java 并发编程的艺术》一书 3.6 节。 参考: https://zhuanlan.zhihu.com/p/60889552 https://www.jianshu.com/p/f68d6ef2dcf0]]></content>
      <categories>
        <category>programming</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>final</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人月神话笔记]]></title>
    <url>%2F2020%2F02%2F26%2F%E4%BA%BA%E6%9C%88%E7%A5%9E%E8%AF%9D%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[人月神话笔记 1975 出版的经典书籍, 记录其中令我印象深刻且到目前也不过时的内容 人月神话 人月是危险和带有欺骗性的神话，因为它暗示人员数量和时间是可以相互替换的 向进度落后的项目增加人手只会导致项目更加落后 任务重新分配本身和所造成的工作中断 培训新人员 额外的相互沟通 关于进度安排, 1/3计划、1/6编码、1/4构件测试以及1/4系统测试 外科手术队伍 把大型团队拆分为若干个类似于外科手术式的小团队 每个小团队有一名主程序员（类似于主刀医生），所有的问题分解和功能定义都通过主程序员来完成，以此来降低沟通成本 每个主程序员配备若干个平庸的人帮他/她打下手 贵族专制、民主政治和系统设计 概念完整性是系统设计中最重要的考虑因素 为了获得概念完整性，设计必须由一个人或者具有共识的小型团队来完成 将设计方法、体系结构方面的工作与具体实现相分离是获得概念完整性的强有力方法 蛇添足, 贯彻执行, 为什么巴比伦塔会失败？ 不要过度设计，尤其是在第二个系统(第一个系统完成后开发的第二个系统)中，不要过度自信，保持警觉，避免初始的概念和目标得到充分的体现，而不让一些次要的功能喧宾夺主 沟通交流的重要性, 尽早交流, 持续沟通 整体部分 在设计系统结构时精心设计，减少各个部分间的耦合，各个模块的独立性越高，系统级的 bug 的可能性就越低 未雨绸缪 目标上（和开发策略上）的一些正常变化无可避免，事先为它们做准备总比假设它们不会出现要好得多 机器在变化，配置在变化，用户的需求在变化，所以现实系统不可能永远可用。崭新的、对于原有系统的重新设计是完全必要的。 祸起萧墙 里程碑的选择只有一个原则，那就是，里程碑必须是具体的、特定的、可度量的事件，能够进行清晰定义 减少角色的冲突。老板必须规范自己，不对项目经理可以解决的问题做出反应 没有银弹 不可能有某种技术突破（银弹）能够彻底解决“根本性困难”，从而导致软件开发效率有数量级的提高 增量开发，把软件当做是生长的有机体, 从简单的核心开始，一边交付一边开发，不断增加新的功能，一边增加新的功能，一边重构已有的部分]]></content>
      <categories>
        <category>programming</category>
      </categories>
      <tags>
        <tag>programing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes 学习笔记]]></title>
    <url>%2F2020%2F02%2F12%2FKubernetes-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Docker 基础 Namespace 进程之间的可见性 Cgoups 限制资源的使用 K8s 基础组件 Kubernetes 主要由以下几个核心组件组成: master 组件, 可以运行于集群中的任何机器上, 但为了简洁性通常在同一台机器上运行所有的 master 组件，且不在此机器上运行用户的容器 kube-apiserver 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制 etcd 保存了整个集群的状态，就是一个数据库 kube-scheduler 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上 kube-controller-manager 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等 cloud-controller-manager 运行了与具体云基础设施供应商互动的控制器 node 组件, 运行在每一个节点上（包括 master 节点和 worker 节点），负责维护运行中的 Pod 并提供 K8s 运行时环境 kubelet 负责维护容器的生命周期，同时也负责 Volume（CSI）和网络（CNI）的管理 kube-proxy 负责为 Service 提供 cluster 内部的服务发现和负载均衡 container runtime 负责镜像管理以及 Pod 和容器的真正运行（CRI） 除了上面的这些核心组件，还有一些推荐的插件： DNS 负责为整个集群提供 DNS 服务 Ingress Controller 为服务提供外网入口 Heapster 提供资源监控 Dashboard 提供 GUI Kubernetes 多组件之间的通信原理 apiserver 负责 etcd 存储的所有操作，且只有 apiserver 才直接操作 etcd 集群 apiserver 对内（集群中的其他组件）和对外（用户）提供统一的 REST API，其他组件均通过 apiserver 进行通信 controller manager、scheduler、kube-proxy 和 kubelet 等均通过 apiserver watch API 监测资源变化情况，并对资源作相应的操作 所有需要更新资源状态的操作均通过 apiserver 的 REST API 进行 apiserver 也会直接调用 kubelet API（如 logs, exec, attach 等），默认不校验 kubelet 证书，但可以通过 --kubelet-certificate-authority 开启（而 GKE 通过 SSH 隧道保护它们之间的通信） 比如最典型的创建 Pod 的流程： 用户通过 REST API 创建一个 Pod apiserver 将其写入 etcd scheduluer 检测到未绑定 Node 的 Pod，开始调度并更新 Pod 的 Node 绑定 kubelet 检测到有新的 Pod 调度过来，通过 container runtime 运行该 Pod kubelet 通过 container runtime 取到 Pod 状态，并更新到 apiserver 中 架构节点节点状态 执行以下命令可查看节点状态以及节点的其他详细信息 1kubectl describe node &lt;your-node-name&gt; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374Name: demo-worker-temp-01Roles: &lt;none&gt;Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/arch=amd64 kubernetes.io/hostname=demo-worker-temp-01 kubernetes.io/os=linuxAnnotations: kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock node.alpha.kubernetes.io/ttl: 0 projectcalico.org/IPv4Address: 172.17.216.105/20 projectcalico.org/IPv4IPIPTunnelAddr: 192.168.199.128 volumes.kubernetes.io/controller-managed-attach-detach: trueCreationTimestamp: Mon, 30 Sep 2019 06:30:16 +0800Taints: &lt;none&gt;Unschedulable: falseConditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- NetworkUnavailable False Wed, 02 Oct 2019 22:37:33 +0800 Wed, 02 Oct 2019 22:37:33 +0800 CalicoIsUp Calico is running on this node MemoryPressure False Sun, 06 Oct 2019 13:44:41 +0800 Mon, 30 Sep 2019 06:30:16 +0800 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Sun, 06 Oct 2019 13:44:41 +0800 Mon, 30 Sep 2019 06:30:16 +0800 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Sun, 06 Oct 2019 13:44:41 +0800 Mon, 30 Sep 2019 06:30:16 +0800 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Sun, 06 Oct 2019 13:44:41 +0800 Wed, 02 Oct 2019 22:37:41 +0800 KubeletReady kubelet is posting ready statusAddresses: InternalIP: 172.17.216.105 Hostname: demo-worker-temp-01Capacity: cpu: 2 ephemeral-storage: 41147472Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 7733524Ki pods: 110Allocatable: cpu: 2 ephemeral-storage: 37921510133 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 7631124Ki pods: 110System Info: Machine ID: 20190711105006363114529432776998 System UUID: 841EC123-F92C-4A3A-BEC0-DAADDD625067 Boot ID: 70c08b02-45ed-456f-8deb-b5c0ebeab414 Kernel Version: 3.10.0-957.21.3.el7.x86_64 OS Image: CentOS Linux 7 (Core) Operating System: linux Architecture: amd64 Container Runtime Version: docker://18.9.7 Kubelet Version: v1.16.0 Kube-Proxy Version: v1.16.0Non-terminated Pods: (21 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- default nginx-deployment-5754944d6c-8lnlx 0 (0%) 0 (0%) 0 (0%) 0 (0%) 3d14h example gateway-example-6f6f45cd6-mhggv 0 (0%) 0 (0%) 0 (0%) 0 (0%) 3d14h example monitor-grafana-ff99b5b6f-sxppz 0 (0%) 0 (0%) 0 (0%) 0 (0%) 3d14h kube-system calico-node-qjfqd 250m (12%) 0 (0%) 0 (0%) 0 (0%) 6d7h kube-system eip-nfs-cluster-storage-6c9c7d46f4-lmxql 0 (0%) 0 (0%) 0 (0%) 0 (0%) 3d14h kube-system kube-proxy-4xz9h 0 (0%) 0 (0%) 0 (0%) 0 (0%) 3d15h kube-system monitor-prometheus-node-exporter-t7d24 0 (0%) 0 (0%) 0 (0%) 0 (0%) 2d20h kuboard-blog cloud-busybox-867645c5dd-7l97b 0 (0%) 0 (0%) 0 (0%) 0 (0%) 3d14h kuboard-blog db-wordpress-79d88d66b7-j7kj8 0 (0%) 0 (0%) 0 (0%) 0 (0%) 3d14h kuboard-press svc-busybox-6cc877b848-2kl28 0 (0%) 0 (0%) 0 (0%) 0 (0%) 3d14h kuboard-press web-kuboard-press-6d6f8bdbb8-c4q44 0 (0%) 0 (0%) 0 (0%) 0 (0%) 2d3h nginx-ingress nginx-ingress-hsv26 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d7hAllocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 250m (12%) 0 (0%) memory 0 (0%) 0 (0%) ephemeral-storage 0 (0%) 0 (0%)Events: &lt;none&gt; Addresses HostName 在节点命令行界面上执行 hostname 命令所获得的值 启动 kubelet 时，可以通过参数 --hostname-override 覆盖 ExternalIP：通常是节点的外部IP（可以从集群外访问的内网IP地址；上面的例子中，此字段为空） InternalIP：通常是从节点内部可以访问的 IP 地址 Conditions Node Condition 描述 OutOfDisk 如果节点上的空白磁盘空间不够，不能够再添加新的节点时，该字段为 True，其他情况为 False Ready 如果节点是健康的且已经就绪可以接受新的 Pod。则节点Ready字段为 True。False表明了该节点不健康，不能够接受新的 Pod。 MemoryPressure 如果节点内存紧张，则该字段为 True，否则为False PIDPressure 如果节点上进程过多，则该字段为 True，否则为 False DiskPressure 如果节点磁盘空间紧张，则该字段为 True，否则为 False NetworkUnvailable 如果节点的网络配置有问题，则该字段为 True，否则为 False 如果 Ready 类型 Condition 的 status 持续为 Unkown 或者 False 超过 pod-eviction-timeout（kube-controller-manager的参数）所指定的时间 (默认 5 分钟)，节点控制器（node controller）将对该节点上的所有 Pod 执行删除的调度动作 某些情况下（例如，节点网络故障），apiserver 不能够与节点上的 kubelet 通信，直到通信重新建立，删除 Pod 的指令才会下达到节点 Capacity and Allocatable CPU 内存 该节点可调度的最大 pod 数量 Capacity 中的字段表示节点上的资源总数，Allocatable 中的字段表示该节点上可分配给普通 Pod 的资源总数 Info Linux 内核版本 Kubernetes 版本（kubelet 和 kube-proxy 的版本） Docker 版本 操作系统名称 这些信息由节点上的 kubelet 收集 节点管理 节点不是由 Kubernetes 创建的, 向 Kubernetes 中创建节点时，仅仅是创建了一个描述该节点的 API 对象 Kubernetes 在 APIServer 上创建一个节点 API 对象（节点的描述），并且基于 metadata.name 字段对节点进行健康检查 如果节点有效（节点组件正在运行），则可以向该节点调度 Pod；否则，该节点 API 对象将被忽略，直到节点变为有效状态 节点控制器节点控制器是一个负责管理节点的 Kubernetes master 组件 在注册节点时为节点分配 CIDR 地址块 通过云供应商（cloud-controller-manager）接口检查每一个节点对象对应的虚拟机是否可用 在云环境中，只要节点状态异常，节点控制器检查其虚拟机在云供应商的状态，如果虚拟机不可用，自动将节点对象从 APIServer 中删除 当节点变得不可触达时（例如，由于节点已停机，节点控制器不再收到来自节点的心跳信号），节点控制器将节点API对象的 NodeStatus Condition 取值从 NodeReady 更新为 Unknown 然后在等待 pod-eviction-timeout 时间后，将节点上的所有 Pod 从节点驱逐 默认40秒未收到心跳，修改 NodeStatus Condition 为 Unknown 默认 pod-eviction-timeout 为 5分钟 节点控制器每隔 --node-monitor-period 秒检查一次节点的状态 节点自注册如果 kubelet 的启动参数 --register-node为 true（默认为 true），kubelet 会尝试将自己注册到 API Server。kubelet自行注册时，将使用如下选项： --kubeconfig：向 apiserver 进行认证时所用身份信息的路径 --cloud-provider：向云供应商读取节点自身元数据 --register-node：自动向 API Server 注册节点 --register-with-taints：注册节点时，为节点添加污点（逗号分隔，格式为 =: --node-ip：节点的 IP 地址 --node-labels：注册节点时，为节点添加标签 --node-status-update-frequency：向 master 节点发送心跳信息的时间间隔 手动管理节点 如果管理员想要手工创建节点 API 对象，可以将 kubelet 的启动参数 --register-node 设置为 false 管理员可以修改节点 API 对象（不管是否设置了 --register-node 参数） 增加 / 减少标签 节点的标签与 Pod 上的节点选择器（node selector）配合，可以控制调度方式，例如限定 Pod 只能在某一组节点上运行 标记节点为不可调度（unschedulable） 1kubectl cordon $NODENAME 此时将阻止新的 Pod 被调度到该节点上，但是不影响任何已经在该节点上运行的 Pod DaemonSet Controller 创建的 Pod 将绕过 Kubernetes 调度器，并且忽略节点的 unschedulable 属性 节点容量 节点 API 对象中描述了节点的容量（Capacity），例如，CPU数量、内存大小等信息 通常，节点在向 APIServer 注册的同时，在节点 API 对象里汇报了其容量（Capacity） 如果是手工创建节点 API 对象, 需要在添加节点时自己设置节点的容量 集群内的通信Cluster to Master 所有从集群访问 Master 节点的通信，都是针对 apiserver 的（没有任何其他 master 组件发布远程调用接口） 通常安装 Kubernetes 时，apiserver 监听 HTTPS 端口（443），并且配置了一种或多种 客户端认证方式 authentication 至少需要配置一种形式的 授权方式 authorization，尤其是 匿名访问 anonymous requests 或 Service Account Tokens 被启用的情况下 节点上必须配置集群（apiserver）的公钥根证书（public root certificate） 在提供有效的客户端身份认证的情况下，节点可以安全地访问 APIServer 对于需要调用 APIServer 接口的 Pod，应该为其关联 Service Account Kubernetes 将在创建 Pod 时自动为其注入公钥根证书（public root certificate）以及一个有效的 bearer token（放在 HTTP 请求头 Authorization 字段） 所有名称空间中，都默认配置了名为 kubernetes Kubernetes Service，该 Service 对应一个虚拟 IP（默认为 10.96.0.1），发送到该地址的请求将由 kube-proxy 转发到 apiserver 的 HTTPS 端口上 得益于这些措施，默认情况下从集群（节点以及节点上运行的 Pod）访问 master 的连接是安全的，因此可以通过不受信的网络或公网连接 Kubernetes 集群 Master to Cluster从 master（apiserver）到 Cluster 存在着两条主要的通信路径： apiserver 访问集群中每个节点上的 kubelet 进程 使用 apiserver 的 proxy 功能，从 apiserver 访问集群中的任意节点、Pod、Service apiserver to kubelet apiserver 在如下情况下访问 kubelet： 抓取 Pod 的日志 通过 kubectl exec -it 指令获得容器的命令行终端 提供 kubectl port-forward 功能 这些连接的访问端点是 kubelet 的 HTTPS 端口 默认情况下，apiserver 不校验 kubelet 的 HTTPS 证书，连接可能会收到 man-in-the-middle 攻击 因此该连接如果在不受信网络或者公网上运行时，是 不安全 的 如果要校验 kubelet 的 HTTPS 证书，可以通过 --kubelet-certificate-authority 参数为 apiserver 提供校验 kubelet 证书的根证书 如果不能完成这个配置，又需要通过不受信网络或公网将节点加入集群，则需要使用 SSH隧道 连接 apiserver 和 kubelet apiserver 将向集群中的每一个节点建立一个 SSH 隧道（连接到端口 22 的 ssh 服务）并通过隧道传递所有发向 kubelet、node、pod、service 的请求 SSH隧道当前已被不推荐使用（deprecated），Kubernetes 正在设计新的替代通信方式 同时 Kubelet authentication/authorization 需要激活，以保护 kubelet API apiserver to nodes, pods, services 从 apiserver 到 节点/Pod/Service 的连接使用的是 HTTP 连接，没有进行身份认证，也没有进行加密传输 也可以通过增加 https 作为 节点/Pod/Service 请求 URL 的前缀 但是 HTTPS 证书并不会被校验，也无需客户端身份认证，因此该连接是无法保证一致性的 目前此类连接如果运行在非受信网络或公网上时，是 不安全 的 控制器 控制器不断监控着集群的状态，并对集群做出对应的变更调整 每一个控制器都不断地尝试着将 当前状态 调整到 目标状态 控制器模式 在 Kubernetes 中，每个控制器至少追踪一种类型的资源 这些资源对象中有一个 spec 字段代表了目标状态 资源对象对应的控制器负责不断地将当前状态调整到目标状态 理论上控制器可以自己直接执行调整动作，然而在 Kubernetes 中更普遍的做法是发送消息到 API Server，而不是直接自己执行 通过 APIServer 进行控制 Kubernetes 自带的控制器都是通过与集群中 API Server 交互来达到调整状态的目的 以 Kubernetes 中自带的一个控制器 Job Controller 为例 Job 是一种 Kubernetes API 对象，一个 Job 将运行一个（或多个）Pod，执行一项任务，然后停止 当新的 Job 对象被创建时，Job Controller 将确保集群中有合适数量的节点上的 kubelet 启动了指定个数的 Pod，以完成 Job 的执行任务 Job Controller 自己并不执行任何 Pod 或容器，而是发消息给 API Server，由其他的控制组件配合 API Server，以执行创建或删除 Pod 的实际动作 当新的 Job 对象被创建时，目标状态是指定的任务被执行完成 Job Controller 调整集群的当前状态以达到目标状态：创建 Pod 以执行 Job 中指定的任务 控制器同样也会更新其关注的 API 对象 例如一旦 Job 的任务执行结束，Job Controller 将更新 Job 的 API 对象，将其标注为 Finished 直接控制 某些特殊的控制器需要对集群外部的东西做调整 例如想用一个控制器确保集群中有足够的节点，此时控制器需要调用云供应商的接口以创建新的节点或移除旧的节点 这类控制器将从 API Server 中读取关于目标状态的信息，并直接调用外部接口以实现调整目标 目标状态 vs 当前状态 Kubernetes 使用了 云原生（cloud-native）的视角来看待系统，并且可以持续应对变化 集群在运行的过程中，任何时候都有可能发生突发事件，而控制器则自动地修正这些问题 这是一种更高级的系统形态，尤其是在运行一个大规模的复杂集群的情况下 设计 作为一个底层设计原则，Kubernetes 使用了大量的控制器，每个控制器都用来管理集群状态的某一个方面 普遍来说，任何一个特定的控制器都使用一种 API 对象作为其目标状态，并使用和管理多种类型的资源，以达到目标状态 使用许多个简单的控制器比使用一个全能的控制器要更加有优势 控制器可能会出故障，而这也是在设计 Kubernetes 时要考虑到的事情 可能存在多种控制器可以创建或更新相同类型的 API 对象 为了避免混淆，Kubernetes 控制器在创建新的 API 对象时，会将该对象与对应的控制 API 对象关联，并且只关注与控制对象关联的那些对象 例如 Deployment 和 Job，这两类控制器都创建 Pod Job Controller 不会删除 Deployment Controller 创建的 Pod，因为控制器可以通过标签信息区分哪些 Pod 是它创建的 运行控制器的方式 Kubernetes 在 kube-controller-manager 中运行了大量的内建控制器（例如，Deployment Controller、Job Controller、StatefulSet Controller、DaemonSet Controller 等） 这些内建控制器提供了 Kubernetes 非常重要的核心功能 Kubernetes 可以运行一个 master 集群，以实现内建控制器的高可用 也可以安装一些运行在 kube-controller-manager 之外的控制器，这些控制器通常是对 Kubernetes 已有功能的一些扩展 或者在必要的情况下，也可以自己编写自己需要的控制器，将其部署为一组 Pod，或者在 Kubernetes 集群之外部署 如何选择取决于想要用这个控制器做什么 操作 KubernetesKubernetes 对象 Kubernetes 将应用程序数据以 Kubernetes 对象的形式通过 api server 存储在 etcd 中 Kubernetes 对象主要描述了: 集群中运行了哪些容器化应用程序（以及在哪个节点上运行） 集群中对应用程序可用的资源 应用程序相关的策略定义，例如，重启策略、升级策略、容错策略 其他 Kubernetes 管理应用程序时所需要的信息 对象的 spec 和 status 每一个 Kubernetes 对象都包含两个重要字段： spec 必须由开发者来提供，描述了对该对象所期望的 目标状态 status 只能由 Kubernetes 系统来修改，描述了该对象在 Kubernetes 系统中的 实际状态 Kubernetes 通过对应的控制器，不断地使实际状态趋向于期望的目标状态 描述 Kubernetes 对象12345678910111213141516171819apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deploymentspec: selector: matchLabels: app: nginx replicas: 2 # 运行 2 个容器化应用程序副本 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 在 Kubernetes 中创建一个对象时，必须提供 该对象的 spec 字段，通过该字段描述期望的 目标状态 该对象的一些基本信息，例如名字 在上述的 .yaml 文件中，如下字段是必须填写的 apiVersion 用来创建对象时所使用的 Kubernetes API 版本 kind 被创建对象的类型 metadata 用于唯一确定该对象的元数据：包括 name 和 namespace，如果 namespace 为空，则默认值为 default spec 描述对该对象的期望状态 管理 Kubernetes 对象 管理方式 操作对象 推荐的环境 指令性的命令行 Kubernetes 对象 开发环境 指令性的对象配置 单个 yaml 文件 生产环境 声明式的对象配置 包含多个 yaml 文件的多个目录 生产环境 指令性的命令行1kubectl run nginx --image nginx 与编写 .yaml 文件进行配置的方式相比的优势 命令简单，易学易记 只需要一个步骤，就可以对集群执行变更 缺点 使用命令，无法进行变更 review 的管理 不提供日志审计 没有创建新对象的模板 指令性的对象配置1kubectl create -f nginx.yaml 与指令性命令行相比的优点 对象配置文件可以存储在源代码管理系统中，例如 git 对象配置文件可以整合进团队的变更管理流程，并进行审计和复核 对象配置文件可以作为一个模板，直接用来创建新的对象 与指令性命令行相比的缺点 需要理解对象配置文件的基本格式 需要额外编写 yaml 文件 与声明式的对象配置相比的优点 指令性的对象配置更简单更易于理解 指令性的对象配置更成熟 与声明式的对象配置相比的缺点 指令性的对象配置基于文件进行工作，而不是目录 如果直接更新 Kubernetes 中对象，最好也同时修改配置文件，否则在下一次替换时，这些更新将丢失 声明式的对象配置12kubectl diff -R -f configs/kubectl apply -R -f configs/ 与指令性的对象配置相比，优点有： 直接针对 Kubernetes 已有对象的修改将被保留，即使这些信息没有合并到配置文件中 也许这是一个缺点, 因为对象可能与配置文件描述的不一致，或者需要禁止使用其他手段修改 Kubernetes 中已有的对象 声明式的对象配置可以支持多文件目录的处理，可以自动探测应该对具体每一个对象执行什么操作（创建、更新、删除） 缺点 声明式的对象配置复杂度更高，Debug 更困难 部分更新对象时，带来复杂的合并操作 名称 Kubernetes REST API 中，所有的对象都是通过 name 和 UID 唯一性确定 Names 可以通过 namespace + name 唯一性地确定一个 RESTFUL 对象 例如 /api/v1/namespaces/{namespace}/pods/{name} 同一个名称空间下，同一个类型的对象，可以通过 name 唯一性确定 如果删除该对象之后，可以再重新创建一个同名对象 DNS Subdomain Names 绝大部分资源类型的名称必须符合 DNS subdomain 命名规则 RFC 1123 最长不超过 253个字符 必须由小写字母、数字、减号 -、小数点 . 组成 由字母开始 由字母结束 DNS Label Names 部分类型的资源要求其名称符合 DNS Label 的命名规则 RFC 1123 最长不超过 63 个字符 必须由小写字母、数字、减号 -、小数点 . 组成 由字母开始 由字母结束 Path Segment Names 部分类型的资源要求其名称可以被编码到路径中。换句话说，名称中不能包含 .、..、/、% UIDs UID 是由 Kubernetes 系统生成的，唯一标识某个 Kubernetes 对象的字符串 Kubernetes 集群中，每创建一个对象，都有一个唯一的 UID, 用于区分多次创建的同名对象 如前所述，按照名字删除对象后，重新再创建同名对象时，两次创建的对象 name 相同，但是 UID 不同 Kubernetes 中的 UID 是全局唯一的标识符（UUIDs，符合规范 ISO/IEC 9834-8 以及 ITU-T X.667） 名称空间 Kubernetes 通过名称空间（namespace）在同一个物理集群上支持多个虚拟集群 名称空间内部的同类型对象不能重名，但是跨名称空间可以有同名同类型对象 名称空间不可以嵌套，任何一个 Kubernetes 对象只能在一个名称空间中 名称空间可以用来在不同的团队（用户）之间划分集群的资源 Kubernetes 安装成功后，默认有初始化了三个名称空间 default 默认名称空间，如果 Kubernetes 对象中不定义 metadata.namespace 字段，该对象将放在此名称空间下 kube-system Kubernetes 系统创建的对象放在此名称空间下 kube-public 此名称空间在安装集群时自动创建，并且所有用户都是可以读取的（即使是那些未登录的用户） 主要是为集群预留的，例如某些情况下，某些 Kubernetes 对象应该被所有集群用户看到 某些低层级的对象是不在任何名称空间中的，例如 nodes、persistentVolumes、storageClass 等 12345# 查看在名称空间里的对象kubectl api-resources --namespaced=true# 查看不在名称空间里的对象kubectl api-resources --namespaced=false 名称空间与 DNS 创建一个 Service 时，Kubernetes 为其创建一个对应的 DNS 条目 该 DNS 记录的格式为 &lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local 如果在容器中只使用 &lt;service-name&gt;，其 DNS 将解析到同名称空间下的 Service 这个特点在多环境的情况下非常有用，例如将开发环境、测试环境、生产环境部署在不同的名称空间下，应用程序只需要使用 &lt;service-name&gt; 即可进行服务发现，无需为不同的环境修改配置 如果想跨名称空间访问服务，则必须使用完整的域名（fully qualified domain name，FQDN） 使用名称空间共享集群查看名称空间 1kubectl get namespaces 1234NAME STATUS AGEdefault Active 11dkube-system Active 11dkube-public Active 11d 查看概要信息 1kubectl describe namespaces &lt;name&gt; 1234567891011Name: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Status: ActiveNo resource quota.Resource Limits Type Resource Min Max Default ---- -------- --- --- --- Container cpu - - 100m Resource quota 汇总了名称空间中使用的资源总量，并指定了集群管理员定义该名称空间最多可以使用的资源量 Limit range 定义了名称空间中某种具体的资源类型的最大、最小值 名称空间可能有两种状态（phase） Active 名称空间正在使用中 Termining 名称空间正在被删除，不能再向其中创建新的对象 创建名称空间 直接使用命令创建名称空间 1kubectl create namespace &lt;名称空间的名字&gt; 通过 yaml 文件 1234apiVersion: v1kind: Namespacemetadata: name: &lt;名称空间的名字&gt; 名称空间的名字必须与 DNS 兼容 不能带小数点 . 不能带下划线 _ 使用数字、小写字母和减号 - 组成的字符串 名称空间可以定义一个可选项字段 finalizers，在名称空间被删除时，用来清理相关的资源 如果定义了一个不存在的 finalizer，仍然可以成功创建名称空间，但是您删除该名称空间时，将卡在 Terminating 状态 删除名称空间 1kubectl delete namespaces &lt;名称空间的名字&gt; 该操作将删除名称空间中的所有内容 此删除操作是异步的，可能会观察到名称空间停留会在 Terminating 状态停留一段时间 标签和选择器 标签（Label）是附加在 Kubernetes 对象上的一组键值对，其意图是按照对用户有意义的方式来标识Kubernetes对象，同时又不对Kubernetes 的核心逻辑产生影响 句法和字符集 标签的 key 可以有两个部分：可选的前缀和标签名，通过 / 分隔 标签名： 标签名部分是必须的 不能多于 63 个字符 必须由字母、数字开始和结尾 可以包含字母、数字、减号-、下划线_、小数点. 标签前缀： 标签前缀部分是可选的 如果指定，必须是一个 DNS 的子域名，例如：k8s.eip.work 不能多于 253 个字符 使用 / 和标签名分隔 如果省略标签前缀，则标签的 key 将被认为是专属于用户的 Kubernetes 的系统组件（例如，kube-scheduler、kube-controller-manager、kube-apiserver、kubectl 或其他第三方组件）向用户的 Kubernetes 对象添加标签时，必须指定一个前缀 kubernetes.io/ 和 k8s.io/ 这两个前缀是 Kubernetes 核心组件预留的 标签的 value 必须： 不能多于 63 个字符 可以为空字符串 如果不为空，则 必须由字母、数字开始和结尾 可以包含字母、数字、减号-、下划线_、小数点. 标签选择器 通过使用标签选择器（label selector），用户/客户端可以选择一组对象 Kubernetes api server 支持两种形式的标签选择器，equality-based 基于等式的 和 set-based 基于集合的 标签选择器可以包含多个条件，并使用逗号分隔，此时只有满足所有条件的 Kubernetes 对象才会被选中 基于等式的选择方式 Equality-based 或者 Inequality-based 选择器可以使用标签的名和值来执行过滤选择 只有匹配所有条件的对象才被选中（被选中的对象可以包含未指定的标签） 可以使用三种操作符 =、==、!= 前两个操作符含义是一样的，都代表相等，后一个操作符代表不相等 1234# 选择了标签名为 `environment` 且 标签值为 `production` 的 Kubernetes 对象environment = production# 选择了标签名为 `tier` 且标签值不等于 `frontend` 的对象，以及不包含标签 `tier` 的对象tier != frontend 也可以使用逗号分隔的两个等式 environment=production,tier!=frontend，此时将选中所有 environment 为 production 且 tier 不为 frontend 的对象 以 Pod 的节点选择器 为例，下面的 Pod 可以被调度到包含标签 accelerator=nvidia-tesla-p100 的节点上： 12345678910111213apiVersion: v1kind: Podmetadata: name: cuda-testspec: containers: - name: cuda-test image: "k8s.gcr.io/cuda-vector-add:v0.1" resources: limits: nvidia.com/gpu: 1 nodeSelector: accelerator: nvidia-tesla-p100 基于集合的选择方式 Set-based 标签选择器可以根据标签名的一组值进行筛选 支持的操作符有：in、notin、exists 12345678# 选择所有的包含 `environment` 标签且值为 `production` 或 `qa` 的对象environment in (production, qa)# 选择所有的 `tier` 标签不为 `frontend` 和 `backend`的对象，或不含 `tier` 标签的对象tier notin (frontend, backend)# 选择所有包含 `partition` 标签的对象partition# 选择所有不包含 `partition` 标签的对象!partition 可以组合多个选择器，用 , 分隔，, 相当于 AND 操作符 12# 选择包含 `partition` 标签（不检查标签值）且 `environment` 不是 `qa` 的对象partition,environment notin (qa) 基于集合的选择方式是一个更宽泛的基于等式的选择方式, 也可以和基于等式的选择方式混合使用，例如： partition in (customerA, customerB),environment!=qa API查询条件 LIST 和 WATCH 操作时，可指定标签选择器作为查询条件，以筛选指定的对象集合 两种选择方式都可以使用，但是要符合 URL 编码 基于等式的选择方式： ?labelSelector=environment%3Dproduction,tier%3Dfrontend 基于集合的选择方式： ?labelSelector=environment+in+%28production%2Cqa%29%2Ctier+in+%28frontend%29 两种选择方式都可以在 kubectl 的 list 和 watch 命令中使用 基于等式的选择方式： kubectl get pods -l environment=production,tier=frontend 基于集合的选择方式： kubectl get pods -l &#39;environment in (production),tier in (frontend)&#39; Kubernetes 对象引用 某些 Kubernetes 对象中（例如，Service和Deployment），使用标签选择器指定一组其他类型的 Kubernetes 对象（例如，Pod） Service Service 中通过 spec.selector 字段来选择一组 Pod，并将服务请求转发到选中的 Pod 上 在 yaml 或 json 文件中，标签选择器用一个 map 来定义，且支持基于等式的选择方式 12selector: component: redis 有些对象支持基于集合的选择方式 Job、Deployment、ReplicaSet 和 DaemonSet 同时支持基于等式的选择方式和基于集合的选择方式 123456selector: matchLabels: component: redis matchExpressions: - &#123;key: tier, operator: In, values: [cache]&#125; - &#123;key: environment, operator: NotIn, values: [dev]&#125; matchLabels 是一个 {key,value} 组成的 map, map 中的一个条目相当于 matchExpressions 中的一个元素 key 为 map 的 key，operator 为 In， values 数组则只包含 value 一个元素 matchExpression 等价于基于集合的选择方式，支持的 operator 有 In、NotIn、Exists 和 DoesNotExist 当 operator 为 In 或 NotIn 时，values 数组不能为空 所有的选择条件都以 AND 的形式合并计算，即所有的条件都满足才可以算是匹配 注解 注解（annotation）可以用来向 Kubernetes 对象的 metadata.annotations 字段添加任意的信息 Kubernetes 的客户端或者自动化工具可以存取这些信息以实现其自定义的逻辑 Kubernetes 对象的 metadata 字段可以添加自定义的标签（label）或者注解（annotation） 注解不是用来标记对象或者选择对象的 123456metadata: annotations: deployment.kubernetes.io/revision: 7 # 由 Deployment 控制器添加，用于记录当前发布的修改次数 k8s.eip.work/displayName: busybox # Kuboard 添加，Deployment 显示在 Kuboard 界面上的名字 k8s.eip.work/ingress: false # Kuboard 添加，根据此参数显示 Deployment 是否配置了 Ingress k8s.eip.work/service: none # Kuboard 添加，根据此参数显示 Deployment 是否配置了 Service 注解的 key 与标签的 key 的规则相同, value 的无限制 字段选择器 字段选择器（Field Selector）可以用来基于的一个或多个字段的取值来选取一组 Kubernetes 对象 12# 选择所有字段 `status.phase` 的取值为 `Running` 的 Podkubectl get pods --field-selector status.phase=Running 不同的 Kubernetes 对象类型，可以用来查询的字段不一样 所有的对象类型都支持的两个字段是 metadata.name 和 metadata.namespace 在字段选择器中使用不支持的字段将报错 字段选择器中可以使用的操作符有 =、==、!= （= 和 == 含义相同） 可以指定多个字段选择器，用逗号 , 分隔 字段选择器可以跨资源类型使用 12# 查询所有的不在 `default` 名称空间的 StatefulSet 和 Servicekubectl get statefulsets,services --all-namespaces --field-selector metadata.namespace!=default 容器更新镜像 Kubernetes 中，默认的镜像抓取策略是 IfNotPresent 使用此策略，kubelet 在发现本机有镜像的情况下，不会向镜像仓库抓取镜像 imagePullPolicy 字段和 image tag 的可能取值将影响到 kubelet 如何抓取镜像： imagePullPolicy: IfNotPresent 仅在节点上没有该镜像时，从镜像仓库抓取 imagePullPolicy: Always 每次启动 Pod 时，从镜像仓库抓取 imagePullPolicy 未填写，镜像 tag 为 :latest 或者未填写，则同 Always 每次启动 Pod 时，从镜像仓库抓取 imagePullPolicy 未填写，镜像 tag 已填写但不是 :latest，则同 IfNotPresent 仅在节点上没有该镜像时，从镜像仓库抓取 imagePullPolicy: Never，Kubernetes 假设本地存在该镜像，并且不会尝试从镜像仓库抓取镜像 在生产环境部署时应该避免使用 :latest tag，如果这样做将无法追踪当前正在使用的镜像版本，也无法正确地执行回滚动作 如果要 100% 确保所有的容器都使用了同样的镜像版本，可以尝试使用镜像的 digest，例如 sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2 Digest 唯一地标识了容器镜像的版本，并且永远不会改变 容器的环境变量 在容器创建时，集群中所有的 Service 的连接信息将以环境变量的形式注入到容器中 例如，已创建了一个名为 Foo 的 Service，此时再创建任何容器时，该容器将包含如下环境变量 12FOO_SERVICE_HOST=&lt;Service的ClusterIP&gt;FOO_SERVICE_PORT=&lt;Service的端口&gt; Runtime Class 通过 RuntimeClass，使不同的 Pod 使用不同的容器引擎，以在性能和安全之间取得平衡 RuntimeClass 的 feature gate 在 apiserver 和 kubelet 上默认都是激活状态 容器的生命周期 - 容器钩子 Kubernetes中为容器提供了两个 hook（钩子函数） PostStart 此钩子函数在容器创建后将立刻执行 并不能保证该钩子函数在容器的 ENTRYPOINT 之前执行 该钩子函数没有输入参数 PreStop 此钩子函数在容器被 terminate（终止）之前执行，例如： 通过接口调用删除容器所在 Pod 某些管理事件的发生：健康检查失败、资源紧缺等 如果容器已经被关闭或者进入了 completed 状态，preStop 钩子函数的调用将失败 该函数的执行是同步的，即 Kubernetes 将在该函数完成执行之后才删除容器 该钩子函数没有输入参数 hook handler 的实现 Kubernetes 中，容器可以实现两种类型的 hook handler Exec - 在容器的名称空进和 cgroups 中执行一个指定的命令，例如 pre-stop.sh 该命令所消耗的 CPU、内存等资源，将计入容器可以使用的资源限制 HTTP - 向容器的指定端口发送一个 HTTP 请求 hook handler 的执行 对于 Pod 而言，hook handler 的调用是同步的 如果是 PostStart hook，容器的 ENTRYPOINT 和 hook 是同时出发的，然而如果 hook 执行的时间过长或者挂起了，容器将不能进入到 Running 状态 PreStop hook 的行为与此相似 如果 hook 在执行过程中挂起了，Pod phase 将停留在 Terminating 的状态，并且在 terminationGracePeriodSeconds 超时之后，Pod 被删除 如果 PostStart 或者 PreStop hook 执行失败，则 Kubernetes 将 kill（杀掉）该容器 用户应该使其 hook handler 越轻量级越好 hook 触发的保证 hook 将至少被触发一次，即当指定事件 PostStart 或 PreStop 发生时，hook 有可能被多次触发 例如 kueblet 在触发 hook 的过程中重启了，该 hook 将在 kubelet 重启后被再次触发 hook handler 的实现需要保证即使多次触发，执行也不会出错 调试 hook handler hook handler 的日志并没有在 Pod 的 events 中发布 如果 handler 因为某些原因失败了，kubernetes 将广播一个事件 PostStart hook 发送 FailedPreStopHook 事件 定义 postStart 和 preStop 处理程序123456789101112131415apiVersion: v1kind: Podmetadata: name: lifecycle-demospec: containers: - name: lifecycle-demo-container image: nginx lifecycle: postStart: exec: command: ["/bin/sh", "-c", "echo Hello from the postStart handler &gt; /usr/share/message"] preStop: exec: command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"] postStart 命令向 usr/share/message 文件写入了一行文字 preStop 命令优雅地关闭了 nginx 如果容器碰到问题被 Kubernetes 关闭，这个操作是非常有帮助的，可以使得程序在关闭前执行必要的清理任务 工作负载Pod (容器组) Kubernetes 中最小的可部署单元 一个 Pod 包含了一个应用程序容器（某些情况下是多个容器） 所有容器都运行在同一个节点 每一个 Pod 被分配一个独立的 IP 地址 同一个 Pod 中的所有容器 IP 地址都相同 同一个 Pod 中的不同容器不能使用相同的端口，否则会导致端口冲突 同一个 Pod 中的不同容器可以通过 localhost:port 进行通信 同一个 Pod 中的不同容器可以通过使用常规的进程间通信手段，例如 SystemV semaphores 或者 POSIX 共享内存 Pod 中可以定义一组共享的数据卷 Pod 中所有的容器都可以访问这些共享数据卷，以便共享数据 Pod 中数据卷的数据也可以存储持久化的数据，使得容器在重启后仍然可以访问到之前存入到数据卷中的数据 在 Pod 中指定容器端口只是展示作用 可以通过对端口命名更方便的使用 使用 kubectl port-forward 将本地网络端口转发到 Pod 端口 kubectl port-forward nginx 8888:80 Pod 和 Controller 应该始终使用 Controller (控制器) 来创建 Pod，而不是直接创建 Pod Pod 本身并不能自愈（self-healing） Pod 一直保留在被调度的节点上，直到以下情况发生 Pod 中的容器全部结束运行 Pod 被删除 由于节点资源不够，Pod 被驱逐 节点出现故障（例如死机） 控制器可以提供如下特性： 水平扩展（运行 Pod 的多个副本） rollout（版本更新） self-healing（故障恢复） 在 Kubernetes 中，广泛使用的控制器有： Deployment StatefulSet DaemonSet 控制器通过其中配置的 Pod Template 信息来创建 Pod Pod Template Pod Template 是关于 Pod 的定义，但是被包含在其他的 Kubernetes 对象中（例如 Deployment、StatefulSet、DaemonSet 等控制器） 控制器通过 Pod Template 信息来创建 Pod Pod 由控制器依据 Pod Template 创建以后，此时再修改 Pod Template 的内容，已经创建的 Pod 不会被修改 Termination of Pods Pod 代表了运行在集群节点上的进程，而进程的终止有两种方式： gracefully terminate （优雅地终止） 直接 kill，此时进程没有机会执行清理动作 Kubernetes 收到用户删除 Pod 的指令后： 记录强制终止前的等待时长（grace period） 向 Pod 中所有容器的主进程发送 TERM 信号 一旦等待超时，向超时的容器主进程发送 KILL 信号 删除 Pod 在 API Server 中的记录 每个容器并不是同一时间接受到 TERM 信号，如果容器的关闭顺序很重要，可能需要为每个容器都定义一个 preStop hook 默认情况下，删除 Pod 的 grace period（等待时长）是 30 秒 可以通过 kubectl delete 命令的选项 --grace-period=&lt;seconds&gt; 自己指定 grace period（等待时长） 如果要强制删除 Pod，必须为 kubectl delete 命令同时指定两个选项 --grace-period=0 和 --force 生命周期Pod phase Pod phase 代表 Pod 所处生命周期的阶段, 并不是用来代表其容器的状态，也不是一个严格的状态机 Phase 描述 Pending Kubernetes 已经创建并确认该 Pod, 此时可能有两种情况：1. Pod 还未完成调度（例如没有合适的节点）2. 正在从 docker registry 下载镜像 Running 该 Pod 已经被绑定到一个节点，并且该 Pod 所有的容器都已经成功创建其中至少有一个容器正在运行，或者正在启动 / 重启 Succeeded Pod 中的所有容器都已经成功终止，并且不会再被重启 Failed Pod 中的所有容器都已经终止，至少一个容器终止于失败状态：容器的进程退出码不是 0，或者被系统 kill Unknown 因为某些未知原因，不能确定 Pod 的状态，通常的原因是 master 与 Pod 所在节点之间的通信故障 通常如果没有人或者控制器删除 Pod，Pod 不会自己消失 只有一种例外，那就是 Pod 处于 Scucceeded 或 Failed 的 phase，并超过了垃圾回收的时长（在 kubernetes master 中通过 terminated-pod-gc-threshold 参数指定），kubelet 自动将其删除 Pod conditions 每一个 Pod 都有一个数组描述其是否达到某些指定的条件 该数组的每一行可能有六个字段 字段名 描述 type type 是最重要的字段，可能的取值有：PodScheduled： Pod 已被调度到一个节点Ready： Pod 已经可以接受服务请求，应该被添加到所匹配 Service 的负载均衡的资源池Initialized：Pod 中所有初始化容器已成功执行Unschedulable：不能调度该 Pod（缺少资源或者其他限制）ContainersReady：Pod 中所有容器都已就绪 status 可能能的取值有：True, False, Unknown reason Condition 发生变化的原因，使用一个符合驼峰规则的英文单词描述 message Condition 发生变化的原因的详细描述，human-readable lastTransitionTime Condition 发生变化的时间戳 lastProbeTime 上一次针对 Pod 做健康检查/就绪检查的时间戳 健康检查 - Probe (探针) Probe 是指 kubelet 周期性地检查容器的状况, 有三种类型的 Probe: ExecAction 在容器内执行一个指定的命令 如果该命令的退出状态码为 0，则成功 TCPSocketAction 探测容器的指定 TCP 端口 如果该端口处于 open 状态，则成功 HTTPGetAction 探测容器指定端口/路径上的 HTTP Get 请求 如果 HTTP 响应状态码在 200 到 400（不包含400）之间，则成功 Probe 有三种可能的结果： Success： 容器通过检测 Failure： 容器未通过检测 Unknown： 检测执行失败，此时 kubelet 不做任何处理 Kubelet 可以在两种情况下对运行中的容器执行 Probe: 就绪检查 readinessProbe 确定容器是否已经就绪并接收服务请求 如果就绪检查失败，kubernetes 将该 Pod 的 IP 地址从所有匹配的 Service 的资源池中移除掉 健康检查 livenessProbe 确定容器是否正在运行 如果健康检查失败，kubelete 将结束该容器，并根据 restart policy（重启策略）确定是否重启该容器 何时使用 如果容器中的进程在碰到问题时可以自己 crash，则并不需要执行健康检查 kubelet 可以自动的根据 Pod 的 restart policy（重启策略）执行对应的动作 如果希望在容器的进程无响应后，将容器 kill 掉并重启，则指定一个健康检查 liveness probe，并同时指定 restart policy（重启策略）为 Always 或者 OnFailure 如果想在探测 Pod 确实就绪之后才向其分发服务请求，请指定一个就绪检查 readiness probe 此时就绪检查的内容可能和健康检查相同 就绪检查适合如下几类容器： 初始化时需要加载大量的数据、配置文件 启动时需要执行迁移任务 其他 参数 initialDelaySeconds，它表示的是说这个 Pod 启动延迟多久进行一次检查 比如说现在有一个 Java 的应用，它启动的时间可能会比较长，因为涉及到 JVM 的启动，包括 Java 自身 jar 的加载 所以前期可能有一段时间是没有办法被检测的，而这个时间又是可预期的，那这时可能要设置一下 initialDelaySeconds periodSeconds，它表示的是检测的时间间隔 正常默认的这个值是 10 秒 timeoutSeconds，它表示的是检测的超时时间，当超时时间之内没有检测成功，那它会认为是失败的一个状态 successThreshold，它表示的是：当这个 pod 从探测失败到再一次判断探测成功，所需要的阈值次数 默认情况下是 1 次，表示原本是失败的，那接下来探测这一次成功了，就会认为这个 Pod 是处在一个探针状态正常的一个状态 failureThreshold，它表示的是探测失败的重试次数 默认值是 3，表示的是当从一个健康的状态连续探测 3 次失败，那此时会判断当前这个 Pod 的状态处在一个失败的状态 容器的状态 一旦 Pod 被调度到节点上，kubelet 便开始使用容器引擎（通常是 docker）创建容器 容器有三种可能的状态： Waiting： 容器的初始状态 处于 Waiting 状态的容器，仍然有对应的操作在执行，例如：拉取镜像、应用 Secrets等 Running： 容器处于正常运行的状态 容器进入 Running 状态之后，如果指定了 postStart hook，该钩子将被执行 Terminated： 容器处于结束运行的状态 容器进入 Terminated 状态之前，如果指定了 preStop hook，该钩子将被执行 重启策略 定义 Pod 或工作负载时，可以指定 restartPolicy，可选的值有： Always （默认值） OnFailure Never restartPolicy 将作用于 Pod 中的所有容器 kubelete 将在五分钟内，按照递延的时间间隔（10s, 20s, 40s ……）尝试重启已退出的容器，并在十分钟后再次启动这个循环，直到容器成功启动，或者 Pod 被删除 控制器 Deployment/StatefulSet/DaemonSet 中，只支持 Always 这一个选项，不支持 OnFailure 和 Never 选项 初始化容器 Pod 可以包含多个工作容器，也可以包含一个或多个初始化容器，初始化容器在工作容器启动之前执行 初始化容器与工作容器完全相同，除了如下几点： 初始化容器总是运行并自动结束 kubelet 按顺序执行 Pod 中的初始化容器，前一个初始化容器成功结束后，下一个初始化容器才开始运行 所有的初始化容器成功执行后，才开始启动工作容器 如果 Pod 的任意一个初始化容器执行失败，kubernetes 将反复重启该 Pod，直到初始化容器全部成功（除非 Pod 的 restartPolicy 被设定为 Never） 初始化容器的 Resource request / limits 处理不同 初始化容器不支持 readiness probe，因为初始化容器必须在 Pod ready 之前运行并结束 初始化容器可以指定不同于工作容器的镜像，这使得初始化容器相较于直接在工作容器中编写启动相关的代码更有优势： 初始化容器可以包含工作容器中没有的工具代码或者自定义代码 例如，无需仅仅为了少量的 setup 工作（使用 sed, awk, python 或 dig 进行环境设定）而重新从一个基础镜像制作另外一个镜像 初始化容器可以更安全地执行某些使工作容器变得不安全的代码 应用程序的镜像构建者和部署者可以各自独立地工作，而无需一起构建一个镜像 初始化容器相较于工作容器，可以以另外一个视角处理文件系统 例如，他们可以拥有访问 Secrets 的权限，而工作容器却不一定被授予该权限 初始化容器在任何工作容器启动之前结束运行，这个特性使得我们可以阻止或者延迟工作容器的启动，直到某些前提条件得到满足 一旦前提条件满足，所有的工作容器将同时并行启动 初始化容器的行为 Pod 的启动时，首先初始化网络和数据卷，然后按顺序执行每一个初始化容器 任何一个初始化容器都必须成功退出，才能开始下一个初始化容器 如果某一个容器启动失败或者执行失败，kubelet 将根据 Pod 的 restartPolicy 决定是否重新启动 Pod 只有所有的初始化容器全都执行成功，Pod 才能进入 ready 状态 初始化容器的端口是不能够通过 kubernetes Service 访问的 Pod 在初始化过程中处于 Pending 状态，并且同时有一个 type 为 Initializing status 为 True 的 Condition 如果 Pod 重启，所有的初始化容器也将被重新执行 可以重启、重试、重新执行初始化容器，因此初始化容器中的代码必须是 幂等 的 具体来说, 向 emptyDir 写入文件内容的代码应该考虑到该文件已经存在的情况 可以组合使用就绪检查和 activeDeadlineSeconds，以防止初始化容器始终失败 Pod 中不能包含两个同名的容器（初始化容器和工作容器也不能同名） Resources在确定初始化容器的执行顺序以后，以下 resource 使用规则将适用： 所有初始化容器中最高的 resource request/limit 是最终生效的 request/limit 对于 Pod 来说，最终生效的 resource request/limit 是如下几个当中较高的一个： 所有工作容器某一个 resource request/limit 的和 最终生效的初始化容器的 request/limit 的和 Kubelet 依据最终生效的 request/limit 执行调度，这意味着，在执行初始化容器时，就已经为 Pod 申请了其资源需求 Pod 重启的原因 Pod 重启时，所有的初始化容器都会重新执行 Pod 重启的原因可能有： 用户更新了 Pod 的定义，并改变了初始化容器的镜像 改变任何一个初始化容器的镜像，将导致整个 Pod 重启 改变工作容器的镜像，将只重启该工作容器，而不重启 Pod Pod 容器基础设施被重启（例如 docker engine），这种情况不常见，通常只有 node 节点的 root 用户才可以执行此操作 Pod 中所有容器都已经结束，restartPolicy 是 Always，且初始化容器执行的记录已经被垃圾回收，此时将重启整个 Pod 配置初始化容器1234567891011121314151617181920212223242526272829apiVersion: v1kind: Podmetadata: name: init-demospec: containers: - name: nginx image: nginx ports: - containerPort: 80 volumeMounts: - name: workdir mountPath: /usr/share/nginx/html # These containers are run during pod initialization initContainers: - name: install image: busybox command: - wget - "-O" - "/work-dir/index.html" - https://kuboard.cn volumeMounts: - name: workdir mountPath: "/work-dir" dnsPolicy: Default volumes: - name: workdir emptyDir: &#123;&#125; Pod 中初始化容器和应用程序共享了同一个数据卷 初始化容器将该共享数据卷挂载到 /work-dir 路径 应用程序容器将共享数据卷挂载到 /usr/share/nginx/html 路径 初始化容器执行命令后，就退出执行 执行该命令时，初始化容器将结果写入了应用程序容器 nginx 服务器对应的 html 根路径下的 index.html Debug 初始化容器 检查初始化容器的状态 1kubectl get pod &lt;pod-name&gt; 12NAME READY STATUS RESTARTS AGE&lt;pod-name&gt; 0/1 Init:1/2 0 7s 状态是 Init:1/2，则表明了两个初始化容器当中的一个已经成功执行 查看初始化容器的详情 1kubectl describe pod &lt;pod-name&gt; 12345678910111213141516171819202122232425Init Containers: &lt;init-container-1&gt;: Container ID: ... ... State: Terminated Reason: Completed Exit Code: 0 Started: ... Finished: ... Ready: True Restart Count: 0 ... &lt;init-container-2&gt;: Container ID: ... ... State: Waiting Reason: CrashLoopBackOff Last State: Terminated Reason: Error Exit Code: 1 Started: ... Finished: ... Ready: False Restart Count: 3 ... 查看初始化容器的日志 1kubectl logs &lt;pod-name&gt; -c &lt;init-container-1&gt; 理解 Pod 状态 如果 Pod 的状态以 Init: 开头，表示该 Pod 正在执行初始化容器 下表描述了 Debug 初始化容器的过程中，一些可能出现的 Pod 状态 状态 描述 Init:N/M Pod 中包含 M 个初始化容器，其中 N 个初始化容器已经成功执行 Init:Error Pod 中有一个初始化容器执行失败 Init:CrashLoopBackOff Pod 中有一个初始化容器反复执行失败 Pending Pod 还未开始执行初始化容器 PodInitializing or Running Pod 已经完成初始化容器的执行 Disruptions (毁坏) 非自愿的毁坏（involuntary disruption） 节点所在物理机的硬件故障 集群管理员误删了虚拟机 云供应商或管理程序故障导致虚拟机被删 Linux 内核故障 集群所在的网络发生分片，导致节点不可用 节点资源耗尽，导致 Pod 被驱逐 自愿的毁坏（voluntary disruptions）, 主要包括由应用管理员或集群管理员主动执行的操作 删除 Deployment 或其他用于管理 Pod 的控制器 修改 Deployment 中 Pod 模板的定义，导致 Pod 重启 直接删除一个 Pod 处理 Disruption 弥补非自愿的毁坏可以采取的方法有： 确保您的 Pod 申请合适的计算资源 如果需要高可用，为您的程序运行多个副本，参考 Deployment、StatefulSet 如果需要更高的高可用性，将应用程序副本分布到多个机架上（参考 anti-affinity]）或分不到多个地区（使用 multi-zone cluster） Disruption Budget Kubernetes 提供了 Disruption Budget 这一特性，以帮助我们在高频次自愿的毁坏会发生的情况下，仍然运行高可用的应用程序 应用程序管理员可以为每一个应用程序创建 PodDisruptionBudget 对象（PDB） PDB 限制了多副本应用程序在自愿的毁坏情况发生时，最多有多少个副本可以同时停止 例如，一个 web 前端的程序需要确保可用的副本数不低于总副本数的一定比例 应该使用兼容 PodDisruption Budget 的工具（例如 kubectl drain，此类工具调用 Eviction API）而不是直接删除 Pod 或者 Deployment kubectl drain 命令会尝试将节点上所有的 Pod 驱逐掉 驱逐请求可能会临时被拒绝，kubectl drain 将周期性地重试失败的请求，直到节点上所有的 Pod 都以终止，或者直到超过了预先配置的超时时间 PDB 指定了应用程序最少期望的副本数（相对于总副本数） 例如，某个 Deployment 的 .spec.replicas 为 5，期望的副本数是 5个 如果他对应的 PDB 允许最低 4个副本数，则 Eviction API（kubectl drain）在同一时刻最多会允许 1 个自愿的毁坏，而不是 2 个或更多 PDB 通过 Pod 的 .metadata.ownerReferences 查找到其对应的控制器（Deployment、StatefulSet） PDB 通过 控制器（Deployment、StatefulSet）的 .spec.replicas 字段来确定期望的副本数 PDB 通过控制器（Deployment、StatefulSet）的 label selector 来确定哪些 Pod 属于同一个应用程序 PDB 不能阻止非自愿的毁坏发生，但是当这类毁坏发生时，将被计入到当前毁坏数里 通过 kubectl drain 驱逐 Pod 时，Pod 将被优雅地终止（gracefully terminated） 在滚动更新过程中被删除的 Pod 也将计入到 PDB 的当前毁坏数，但是控制器（例如 Deployment、StatefulSet）在执行滚动更新时，并不受 PDB 的约束 滚动更新过程中，同时可以删除的 Pod 数量在控制器对象（Deployment、StatefulSet 等）的定义中规定 区分集群管理员和应用管理员的角色 集群管理员和应用管理员是不同的角色，且相互之间所共有的知识并不多 对这两个角色的职责进行区分，在如下场景中是非常有用的： 多个应用程序团队共享一个 Kubernetes 集群 第三方工具或服务将集群的管理自动化 Pod Disruption Budget 是区分两种角色时的必要的界面，双方要就此概念达成共识 如果你所在的组织中，并不严格区分集群管理员和应用程序管理员，则并不需要使用 Pod Disruption Budget 如何执行毁坏性的操作（Disruptive Action） 如果您是集群管理员，且需要在所有节点上执行毁坏性的操作（disruptive action），例如节点或系统软件的升级，此时可能的选择有： 接受升级过程中的停机时间 故障转移（Failover）到另外一个集群副本 无停机时间，但是将有额外的代价，因为需要由双份的节点以及更多的人力成本来管理集群之间的切换 编写容错的应用程序（disruption tolerant application）并使用 PDB 无停机时间 最少的资源冗余 支持更多的集群管理自动化 编写容错的应用程序（disruption-tolerant application）非常需要技巧，但是要容忍自愿的毁坏所做的工作与支持自动伸缩（autoscaling）与容忍非自愿的毁坏（tolerating involuntary disruption）所做的工作是大量重叠的 配置 Pod Disruption Budget PodDisruptionBudget 包含三个字段： 标签选择器 .spec.selector 用于指定 PDB 适用的 Pod。此字段为必填 .spec.minAvailable：当完成驱逐时，最少仍然要保留多少个 Pod 可用 该字段可以是一个整数，也可以是一个百分比 .spec.maxUnavailable： 当完成驱逐时，最多可以有多少个 Pod 被终止 该字段可以是一个整数，也可以是一个百分比 只能应用到那些有控制器的 Pod 上 在一个 PDB 中，只能指定 maxUnavailable 和 minAvailable 中的一个 如果指定 minAvailable 或 maxUnavailable 为百分数，其计算结果可能不会正好是一个整数 Kubernetes 将向上舍入 假设有 7 个 Pod，minAvailable 设置为 50%，则必须有 4 个 Pod 始终可用 PDB 只能保护应用避免受到自愿毁坏的影响，而不是所有原因的毁坏 确定哪个应用程序需要使用 PDB 保护 通常如下几种 Kubernetes 控制器创建的应用程序可以使用 PDB： Deployment ReplicationController ReplicaSet StatefulSet PDB 中 .spec.selector 字段的内容必须与控制器中 .spec.selector 字段的内容相同 也可以直接为 Pod 设置 PDB，但是存在一些限制条件 只能使用 .spec.minAvailable，不能使用 .spec.maxUnavailable .spec.minAvailable 字段中只能使用整型数字，不能使用百分比 思考应用程序如何处理毁坏（disruption） 当自愿毁坏发生时，在短时间内应用程序最多可以容许多少个实例被终止 无状态的前端： 关注点：不能让服务能力（serving capacity）降低超过 10% 解决方案：在 PDB 中配置 minAvailable 90% 单实例有状态应用： 关注点：未经同意不能关闭此应用程序 解决方案1： 不使用 PDB，并且容忍偶尔的停机 解决方案2： 在 PDB 中设置 maxUnavailable=0 与集群管理员达成一致，请集群管理员在终止应用之前与你沟通 当集群管理员联系你时，准备好停机时间，删除 PDB 以表示已准备好应对毁坏。并做后续处理 多实例有状态应用，例如 consul、zookeeper、etcd： 关注点：不能将实例数降低到某个数值，否则写入会失败 解决方案1： 在 PDB 中设置 maxUnavailable 为 1 （如果副本数会发生变化，可以使用此设置） 解决方案2： 在 PDB 中设置 minAvailable 为最低数量（可以同时容忍更多的毁坏数） 可以重新开始的批处理任务： 关注点：当发生自愿毁坏时，Job 仍然需要完成其执行任务 解决方案： 不创建 PDB。Job 控制器将会创建一个 Pod 用于替换被毁坏的 Pod 问题诊断常见应用异常Pod 停留在 Pending pending 表示调度器没有进行介入 此时可以通过 kubectl describe pod 来查看相应的事件 如果由于资源或者说端口占用，或者是由于 node selector 造成 pod 无法调度的时候，可以在相应的事件里面看到相应的结果，这个结果里面会表示说有多少个不满足的 node，有多少是因为 CPU 不满足，有多少是由于 node 不满足，有多少是由于 tag 打标造成的不满足 Pod 停留在 waiting pod 的 states 处在 waiting 的时候，通常表示说这个 pod 的镜像没有正常拉取 可能是由于这个镜像是私有镜像，但是没有配置 Pod secret 可能由于这个镜像地址是不存在的，造成这个镜像拉取不下来 这个镜像可能是一个公网的镜像，造成镜像的拉取失败 Pod 不断被拉取并且可以看到 crashing pod 不断被拉起，而且可以看到类似像 backoff 这个通常表示说 pod 已经被调度完成了，但是启动失败 这个时候通常要关注的应该是这个应用自身的一个状态，并不是说配置是否正确、权限是否正确 此时需要查看的应该是 pod 的具体日志 Pod 处在 Runing 但是没有正常工作 此时比较常见的一个点就可能是由于一些非常细碎的配置，类似像有一些字段可能拼写错误，造成了 yaml 下发下去了，但是有一段没有正常地生效，从而使得这个 pod 处在 running 的状态没有对外服务，那此时可以通过 apply-validate-f pod.yaml 的方式来进行判断当前 yaml 是否是正常的 如果 yaml 没有问题，那么接下来可能要诊断配置的端口是否是正常的，以及 Liveness 或 Readiness 是否已经配置正确 Service 无法正常的工作 比较常见的 service 出现问题的时候，是自己的使用上面出现了问题 因为 service 和底层的 pod 之间的关联关系是通过 selector 的方式来匹配的，也就是说 pod 上面配置了一些 label，然后 service 通过 match label 的方式和这个 pod 进行相互关联 如果这个 label 配置的有问题，可能会造成这个 service 无法找到后面的 endpoint，从而造成相应的 service 没有办法对外提供服务，那如果 service 出现异常的时候，第一个要看的是这个 service 后面是不是有一个真正的 endpoint，其次来看这个 endpoint 是否可以对外提供正常的服务 应用远程调试Pod 远程调试12345# Pod 中只有一个容器时kubectl exec -it pod-name /bin/bash# Pod 中有多个容器时kubectl exec -it pod-name -c container-name /bin/bash Servic 远程调试 service 的远程调试其实分为两个部分： 想将一个服务暴露到远程的一个集群之内，让远程集群内的一些应用来去调用本地的一个服务，这是一条反向的一个链路 开源组件 Telepresence 可以将本地的应用代理到远程集群中的一个 service 上面 1telepresence --swap-deployment $Deployment_Name 想让这个本地服务能够去调远程的服务，那么这是一条正向的链路 可以通过 port-forward 的方式将远程的应用调用到本地的端口之上 比如现在远程有一个 API server 提供了一些接口，本地在调试 Code 时候，想要直接调用这个 API server 1kubectl port-forward svc/app -n app-namespace 它的使用方式是 kubectl port-forward，然后 service 加上远程的 service name，再加上相应的 namespace，后面还可以加上一些额外的参数，比如说端口的一个映射，通过这种机制就可以把远程的一个应用代理到本地的端口之上，此时通过访问本地端口就可以访问远程的服务 kubectl-debug 通常情况下应用的镜像里并不会带特别多的调试工具，类似像 netstat telnet 等等这些 ，因为这个会造成应用整体非常冗余 那么如果想要调试的时候就可以依赖类似于像 kubectl-debug 这样一个工具 kubectl-debug 是依赖于 Linux namespace 的方式来去做的 它可以 datash 一个 Linux namespace 到一个额外的 container，然后在这个 container 里面执行任何的 debug 动作 其实和直接去 debug 这个 Linux namespace 是一致的 Controller (控制器) Kubernetes 通过引入控制器的概念来管理 Pod 实例 应该始终通过创建 Controller 来创建 Pod，而不是直接创建 Pod 控制器可以提供如下特性： 水平扩展（运行 Pod 的多个副本） rollout（版本更新） self-healing（故障恢复） 例如当一个节点出现故障，控制器可以自动地在另一个节点调度一个配置完全一样的 Pod，以替换故障节点上的 Pod ReplicaSet ReplicaSet 用来维护一个数量稳定的 Pod 副本集合，可以保证某种定义一样的 Pod 始终有指定数量的副本数在运行 ReplicaSet 的定义123456789101112131415161718192021apiVersion: apps/v1kind: ReplicaSetmetadata: name: frontend labels: app: guestbook tier: frontendspec: # modify replicas according to your case replicas: 3 selector: matchLabels: tier: frontend template: metadata: labels: tier: frontend spec: containers: - name: nginx image: nginx 与其他 Kubernetes 对象一样，ReplicaSet 需要的字段有： apiVersion：apps/v1 kind：ReplicaSet metadata spec： ReplicaSet 的详细定义 template： Pod 模板，在 ReplicaSet 使用 Pod 模板的定义创建新的 Pod 必须定义 .spec.template.metadata.labels 字段 该字段不要与其他控制器的 selector 重合，以免这些控制器尝试接管该 Pod .spec.template.spec.restartPolicy 的默认值为 Always selector： 用于识别可以接管哪些 Pod .spec.template.metadata.labels 必须与 .spec.selector 匹配，否则将不能成功创建 ReplicaSet 如果两个 ReplicaSet 指定了相同的 .spec.selector 但是不同的 .spec.template.metadata.labels 和不同的 .spec.tempalte.spec 字段，两个 ReplicaSet 都将忽略另外一个 ReplicaSet 创建的 Pod replicas： 副本数，用于指定该 ReplicaSet 应该维持多少个 Pod 副本 默认值为 1 Deployment 最常用的用于部署无状态服务的方式, 使得能够以声明的方式更新 Pod（容器组）和 ReplicaSet（副本集） 创建123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 1kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml 可以为该命令增加 –record 选项，此时 kubectl 会将 kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml --record 写入 Deployment 的 annotation（注解） kubernetes.io/change-cause 中 这样可以回顾某一个 Deployment 版本变化的原因 更新 执行以下命令，将容器镜像从 nginx:1.7.9 更新到 nginx:1.9.1 1kubectl --record deployment.apps/nginx-deployment set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 或者可以 edit 该 Deployment，并将 .spec.template.spec.containers[0].image 从 nginx:1.7.9 修改为 nginx:1.9.1 1kubectl edit deployment.v1.apps/nginx-deployment Deployment 将确保更新过程中，任意时刻只有一定数量的 Pod 被关闭 默认情况下，确保至少 .spec.replicas 的 75% 的 Pod 保持可用（25% max unavailable） Deployment 将确保更新过程中，任意时刻只有一定数量的 Pod 被创建 默认情况下，确保最多 .spec.replicas 的 25% 的 Pod 被创建（25% max surge） 覆盖更新 Rollover （更新过程中再更新） 每创建一个 Deployment，Deployment Controller 都为其创建一个 ReplicaSet，并设定其副本数为期望的 Pod 数（ .spec.replicas 字段） 如果 Deployment 被更新，旧的 ReplicaSet 将被 Scale down，新建的 ReplicaSet 将被 Scale up 直到最后新旧两个 ReplicaSet，一个副本数为 .spec.replias，另一个副本数为 0 这个过程称为 rollout 当 Deployment 的 rollout 正在进行中的时候，如果再次更新 Deployment 的信息，此时 Deployment 将再创建一个新的 ReplicaSet 并开始将其 scale up，将先前正在 scale up 的 ReplicaSet 也作为一个旧的 ReplicaSet，并开始将其 scale down 假设创建了一个 Deployment 有 5 个 nginx:1.7.9 的副本 立刻更新该 Deployment 使得其 .spec.replicas 为 5，容器镜像为 nginx:1.9.1，而此时只有 3 个 nginx:1.7.9 的副本已创建 此时 Deployment Controller 将立刻开始 kill 已经创建的 3 个 nginx:1.7.9 的 Pod，并开始创建 nginx:1.9.1 的 Pod Deployment Controller 不会等到 5 个 nginx:1.7.9 的 Pod 都创建完之后在开始新的更新 滚动更新 通过 Deployment 中 .spec.strategy 字段，可以指定使用 滚动更新 RollingUpdate 的部署策略还是使用 重新创建 Recreate 的部署策略 .spec.strategy.type Recreate Deployment 将先删除原有副本集中的所有 Pod，然后再创建新的副本集和新的 Pod 如此更新过程中将出现一段应用程序不可用的情况 RollingUpdate 滚动更新 .spec.strategy.minReadySeconds: Kubernetes 在等待设置的时间后才进行升级 如果没有设置该值，Kubernetes 会假设该容器启动起来后就提供服务了 如果没有设置该值，在某些极端情况下可能会造成服务不正常运行 .spec.strategy.maxSurge: 数字或百分比, 默认值为 25% 升级过程中最多可以比原先设置多出的 Pod 数量 例如：maxSurage=1，replicas=5, 则表示 Kubernetes 会先启动 1 个新的 Pod 后才删掉一个旧的 Pod，整个升级过程中最多会有 5+1 个 Pod .spec.strategy.maxUnavaible: 数字或百分比, 默认值为 25% 升级过程中最多有多少个 Pod 处于无法提供服务的状态 当 maxSurge 不为 0 时，该值也不能为 0 例如：maxUnavaible=1，则表示 Kubernetes 整个升级过程中最多会有 1 个 Pod 处于无法服务的状态 回滚 当且仅当 Deployment 的 .spec.template 字段被修改时，K8s 将为其创建一个 Deployment revision（版本） 通过 Deployment 中 .spec.revisionHistoryLimit 字段，可指定为该 Deployment 保留多少个旧的 ReplicaSet 超出该数字的将被在后台进行垃圾回收 该字段的默认值是 10 执行命令 kubectl rollout history deployment.v1.apps/nginx-deployment 检查 Deployment 的历史版本 12345deployments "nginx-deployment"REVISION CHANGE-CAUSE1 kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml --record=true2 kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true3 kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.91 --record=true 可以通过如下方式制定 CHANGE-CAUSE 信息： 为 Deployment 增加注解，kubectl annotate deployment.v1.apps/nginx-deployment kubernetes.io/change-cause=&quot;image updated to 1.9.1&quot; 执行 kubectl apply 命令时，增加 --record 选项 手动编辑 Deployment 的 .metadata.annotation 信息 执行命令 kubectl rollout history deployment.v1.apps/nginx-deployment --revision=2，查看 revision（版本）的详细信息 12345678910111213deployments "nginx-deployment" revision 2 Labels: app=nginx pod-template-hash=1159050644 Annotations: kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true Containers: nginx: Image: nginx:1.9.1 Port: 80/TCP QoS Tier: cpu: BestEffort memory: BestEffort Environment Variables: &lt;none&gt; No volumes. 执行命令 kubectl rollout undo deployment.v1.apps/nginx-deployment 将当前版本回滚到前一个版本 也可以使用 --to-revision 选项回滚到前面的某一个指定版本 1kubectl rollout undo deployment.v1.apps/nginx-deployment --to-revision=2 伸缩 执行命令 kubectl scale deployment.v1.apps/nginx-deployment --replicas=10，可以伸缩 Deployment 暂停和继续 可以先暂停 Deployment，然后再触发一个或多个更新，最后再继续（resume）该 Deployment 这种做法使得可以在暂停和继续中间对 Deployment 做多次更新，而无需触发不必要的滚动更新 执行命令 kubectl rollout pause deployment.v1.apps/nginx-deployment 暂停 Deployment 暂停 Deployment 之前的信息当前仍然在起作用，而暂停 Deployment 之后，修改的 Deployment 信息尚未生效，因为该 Deployment 被暂停了 执行命令 kubectl rollout resume deployment.v1.apps/nginx-deployment，继续（resume）该 Deployment，可使前面所有的变更一次性生效 不能回滚（rollback）一个已暂停的 Deployment，除非继续（resume）该 Deployment 查看状态Progressing 状态 当如下任何一个任务正在执行时，Kubernete 将 Deployment 的状态标记为 progressing： Deployment 创建了一个新的 ReplicaSet Deployment 正在 scale up 其最新的 ReplicaSet Deployment 正在 scale down 其旧的 ReplicaSet 新的 Pod 变为 就绪（ready） 或 可用（available） 可以使用命令 kubectl rollout status 监控 Deployment 滚动更新的过程 Complete 状态 如果 Deployment 符合以下条件，Kubernetes 将其状态标记为 complete： 该 Deployment 中的所有 Pod 副本都已经被更新到指定的最新版本 该 Deployment 中的所有 Pod 副本都处于 可用（available） 状态 该 Deployment 中没有旧的 ReplicaSet 正在运行 Failed 状态 Deployment 在更新其最新的 ReplicaSet 时，可能卡住而不能达到 complete 状态 如下原因都可能导致此现象发生： 集群资源不够 就绪检查（readiness probe）失败 镜像抓取失败 权限不够 资源限制 应用程序的配置错误导致启动失败 指定 .spec.progressDeadlineSeconds 字段，Deployment Controller 在等待指定的时长后，将标记为处理失败 如果暂停了 Deployment，Kubernetes 将不会检查 .spec.progressDeadlineSeconds 金丝雀发布（灰度发布） 创建新旧两个版本的 Deployment, 拥有相同的 label 将 Service 的 LabelSelector 设置为新旧 Deployment 所共有的 label 在新旧版本之间，流量分配的比例为两个版本副本数的比例 按照 Kubernetes 默认支持的这种方式进行金丝雀发布，有一定的局限性： 不能根据用户注册时间、地区等请求中的内容属性进行流量分配 同一个用户如果多次调用该 Service，有可能第一次请求到了旧版本的 Pod，第二次请求到了新版本的 Pod Kubernetes Service 只在 TCP 层面解决负载均衡的问题，并不对请求响应的消息内容做任何解析和识别 如果想要更完善地实现金丝雀发布，可以考虑如下三种选择： 业务代码编码实现 Spring Cloud 灰度发布 Istio 灰度发布 StatefulSet 用于管理 Stateful（有状态）的应用程序 StatefulSet 管理 Pod 时，确保其 Pod 有一个按顺序增长的 ID 与 Deployment 最大的不同在于 StatefulSet 始终将一系列不变的名字分配给其 Pod 这些 Pod 从同一个模板创建，但是并不能相互替换：每个 Pod 都对应一个特有的持久化存储标识 对于有如下要求的应用程序，StatefulSet 非常适用： 稳定、唯一的网络标识（dnsname） 每个 Pod 始终对应各自的存储路径（PersistantVolumeClaimTemplate） 按顺序地增加副本、减少副本，并在减少副本时执行清理 按顺序自动地执行滚动更新 限制 Pod 的存储要么由 storage class 对应的 PersistentVolume Provisioner 提供，要么由集群管理员事先创建 删除或 scale down 一个 StatefulSet 将不会删除其对应的数据卷 这样做的考虑是数据安全 删除 StatefulSet 时，将无法保证 Pod 的终止是正常的 如果要按顺序 gracefully 终止 StatefulSet 中的 Pod，可以在删除 StatefulSet 前将其 scale down 到 0 当使用默认的 Pod Management Policy (OrderedReady) 进行滚动更新时，可能进入一个错误状态，并需要人工介入才能修复 创建123456789101112131415161718192021222324252627282930313233343536373839404142434445464748apiVersion: v1kind: Servicemetadata: name: nginx labels: app: nginxspec: ports: - port: 80 name: web clusterIP: None selector: app: nginx---apiVersion: apps/v1kind: StatefulSetmetadata: name: webspec: selector: matchLabels: app: nginx # has to match .spec.template.metadata.labels serviceName: "nginx" replicas: 3 # by default is 1 template: metadata: labels: app: nginx # has to match .spec.selector.matchLabels spec: terminationGracePeriodSeconds: 10 containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ "ReadWriteOnce" ] storageClassName: "my-storage-class" resources: requests: storage: 1Gi Pod 的标识 StatefulSet 中的 Pod 具备一个唯一标识，该标识由以下几部分组成： 序号 稳定的网络标识 稳定的存储 该标识始终与 Pod 绑定，无论该 Pod 被调度（重新调度）到哪一个节点上 序号 假设一个 StatefulSet 的副本数为 N，其中的每一个 Pod 都会被分配一个序号，序号的取值范围从 0 到 N - 1，并且该序号在 StatefulSet 内部是唯一的 稳定的网络 ID StatefulSet 中 Pod 的 hostname 格式为 $(StatefulSet name)-$(Pod 序号) 上面的例子将要创建三个 Pod，其名称分别为： web-0，web-1，web-2 StatefulSet 可以使用 Headless Service 来控制其 Pod 所在的域 Headless Service 为 .spec.clusterIP: None 的 Service 该域（domain）的格式为 $(service name).$(namespace).svc.cluster.local，其中 “cluster.local” 是集群的域 StatefulSet 中每一个 Pod 将被分配一个 dnsName，格式为： $(podName).$(所在域名) 稳定的存储 Kubernetes 为每一个 VolumeClaimTemplate 创建一份 PersistentVolume（存储卷） 在上面的例子中，每一个 Pod 都将由 StorageClass（存储类）my-storage-class 为其创建一个 1Gib 大小的 PersistentVolume（存储卷） 当 Pod 被调度（或重新调度）到一个节点上，其挂载点将挂载该存储卷声明（关联到该 PersistentVolume） 当 Pod 或 StatefulSet 被删除时，其关联的 PersistentVolumeClaim（存储卷声明）以及其背后的 PersistentVolume（存储卷）仍然存在 如果相同的 Pod 或 StatefulSet 被再次创建，则新建的名为 web-0 的 Pod 仍将挂载到原来名为 web-0 的 Pod 所挂载的存储卷声明及存储卷 这确保了 web-0、web-1、web-2 等，不管被删除重建多少次，都将 “稳定” 的使用各自所对应的存储内容 部署和伸缩 在创建一个副本数为 N 的 StatefulSet 时，其 Pod 将被按 {0 … N-1} 的顺序逐个创建 在删除一个副本数为 N 的 StatefulSet （或其中所有的 Pod）时，其 Pod 将按照相反的顺序（即 {N-1 … 0}）终止和删除 在对 StatefulSet 执行扩容（scale up）操作时，新增 Pod 所有的前序 Pod 必须处于 Running（运行）和 Ready（就绪）的状态 终止和删除 StatefulSet 中的某一个 Pod 时，该 Pod 所有的后序 Pod 必须全部已终止 StatefulSet 中 pod.spec.terminationGracePeriodSeconds 不能为 0 Pod 管理策略 可以为 StatefulSet 设定 .spec.podManagementPolicy 字段，以便可以继续使用 StatefulSet 唯一 ID 的特性，但禁用其有序创建和销毁 Pod 的特性 OrderedReady OrderedReady 是 .spec.podManagementPlicy 的默认值, 就是按前面介绍的顺序执行 Parallel StatefulSet Controller 将同时并行地创建或终止其所有的 Pod 此时 StatefulSet Controller 将不会逐个创建 Pod，等待 Pod 进入 Running 和 Ready 状态之后再创建下一个 Pod，也不会逐个终止 Pod 此选项只影响到伸缩（scale up/scale down）操作, 更新操作不受影响 更新 可以为 StatefulSet 设定 .spec.updateStrategy 字段，以便可以在改变 StatefulSet 中 Pod 的某些字段时（container/labels/resource request/resource limit/annotation 等）禁用滚动更新 OnDelete OnDelete 策略实现了 StatefulSet 的遗留版本（kuberentes 1.6及以前的版本）的行为 如果 StatefulSet 的 .spec.updateStrategy.type 字段被设置为 OnDelete 修改 .spec.template 的内容时，StatefulSet Controller 将不会自动更新其 Pod 必须手工删除 Pod，此时 StatefulSet Controller 在重新创建 Pod 时，使用修改过的 .spec.template 的内容创建新 Pod RollingUpdate .spec.updateStrategy.type 字段的默认值是 RollingUpdate 在用户更新 StatefulSet 的 .spec.tempalte 字段时，StatefulSet Controller 将自动地删除并重建 StatefulSet 中的每一个 Pod 从序号最大的 Pod 开始，逐个删除和更新每一个 Pod，直到序号最小的 Pod 被更新 当正在更新的 Pod 达到了 Running 和 Ready 的状态之后，才继续更新其前序 Pod Partitions 通过指定 .spec.updateStrategy.rollingUpdate.partition 字段，可以分片（partitioned）执行 RollingUpdate 更新策略 当更新 StatefulSet 的 .spec.template 时： 序号大于或等于 .spec.updateStrategy.rollingUpdate.partition 的 Pod 将被删除重建 序号小于 .spec.updateStrategy.rollingUpdate.partition 的 Pod 将不会更新，及时手工删除该 Pod，kubernetes 也会使用前一个版本的 .spec.template 重建该 Pod 如果 .spec.updateStrategy.rollingUpdate.partition 大于 .spec.replicas，更新 .spec.tempalte 将不会影响到任何 Pod 大部分情况下不需要使用 .spec.updateStrategy.rollingUpdate.partition，除非碰到如下场景： 执行预发布 执行金丝雀更新 执行按阶段的更新 Forced Rollback 当使用默认的 Pod 管理策略时（OrderedReady），很有可能会进入到一种卡住的状态，需要人工干预才能修复 如果更新 Pod template 后，该 Pod 始终不能进入 Running 和 Ready 的状态（例如，镜像错误或应用程序配置错误），StatefulSet 将停止滚动更新并一直等待 此时如果仅仅将 Pod template 回退到一个正确的配置仍然是不够的 由于一个已知的问题，StatefulSet 将继续等待出错的 Pod 进入就绪状态（该状态将永远无法出现），才尝试将该 Pod 回退到正确的配置 在修复 Pod template 以后，还必须删除掉所有已经尝试使用有问题的 Pod template 的 Pod StatefulSet 此时才会开始使用修复了的 Pod template 重建 Pod DaemonSet DaemonSet 控制器确保所有（或一部分）的节点都运行了一个指定的 Pod 副本 每当向集群中添加一个节点时，指定的 Pod 副本也将添加到该节点上 当节点从集群中移除时，Pod 也就被垃圾回收了 删除一个 DaemonSet 可以清理所有由其创建的 Pod DaemonSet 的典型使用场景有： 在每个节点上运行集群的存储守护进程，例如 glusterd、ceph 在每个节点上运行日志收集守护进程，例如 fluentd、logstash 在每个节点上运行监控守护进程，例如 Prometheus Node Exporter、Sysdig Agent、collectd、Dynatrace OneAgent、APPDynamics Agent、Datadog agent、New Relic agent、Ganglia gmond、Instana Agent 等 通常情况下，一个 DaemonSet 将覆盖所有的节点 复杂一点儿的用法，可能会为某一类守护进程设置多个 DaemonSets，每一个 DaemonSet 针对不同类硬件类型设定不同的内存、cpu请求 创建123456789101112131415161718192021222324252627282930313233343536373839404142apiVersion: apps/v1kind: DaemonSetmetadata: name: fluentd-elasticsearch namespace: kube-system labels: k8s-app: fluentd-loggingspec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd-elasticsearch image: fluent/fluentd-kubernetes-daemonset:v1.7.1-debian-syslog-1.0 resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers DaemonSet 的 .spec.template.spec.restartPolicy 字段必须为 Always，或者不填（默认值为 Always） 默认通过 Kubernetes 调度器来调度 DaemonSet 的 Pod DaemonSet Controller 会向 DaemonSet 的 Pod 添加 .spec.nodeAffinity 字段，而不是 .spec.nodeName 字段 并进一步由 Kubernetes 调度器将 Pod 绑定到目标节点 如果 DaemonSet 的 Pod 已经存在了 nodeAffinity 字段，该字段的值将被替换 12345678nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchFields: - key: metadata.name operator: In values: - target-host-name 污点和容忍 在调度 DaemonSet 的 Pod 时，污点和容忍（taints and tolerations）会被考量到，同时以下容忍（toleration）将被自动添加到 DaemonSet 的 Pod 中 Toleration Key Effect 描述 node.kubernetes.io/not-ready NoExecute 节点未就绪。对应着 NodeCondition Ready 为 False 的情况 node.kubernetes.io/unreachable NoExecute 节点不可触达。对应着 NodeCondition Ready 为 Unknown 的情况 node.kubernetes.io/disk-pressure NoSchedule 节点磁盘吃紧 node.kubernetes.io/memory-pressure NoSchedule 节点内存吃紧 node.kubernetes.io/unschedulable NoSchedule 节点不可调度 node.kubernetes.io/network-unavailable NoSchedule 节点网络不可用, 只对 host network 生效 通信模式 Push： DaemonSet 容器组用来向另一个服务推送信息，例如数据库的统计信息 这种情况下 DaemonSet 容器组没有客户端 NodeIP + Port： DaemonSet 容器组可以使用 hostPort，此时可通过节点的 IP 地址直接访问该容器组 客户端需要知道节点的 IP 地址，以及 DaemonSet 容器组的 端口号 DNS： 创建一个 Headless Service，且该 Service 与 DaemonSet 有相同的 Pod Selector 此时客户端可通过该 Service 的 DNS 解析到 DaemonSet 的 IP 地址 Service： 创建一个 Service，且该 Service 与 DaemonSet 有相同的 Pod Selector，客户端通过该 Service，可随机访问到某一个节点上的 DaemonSet 容器组 更新 如果节点的标签被修改，DaemonSet 将立刻向新匹配上的节点添加 Pod， 同时删除不匹配的节点上的 Pod 可以修改 DaemonSet 生成的 Pod, 只允许修改部分字段，但是 DaemonSet 控制器在创建新的 Pod 时，仍然会使用原有的 Template 进行 Pod 创建 可以删除 DaemonSet 如果在 kubectl 命令中指定 --cascade=false 选项，DaemonSet 容器组将不会被删除 紧接着如果创建一个新的 DaemonSet，与之前删除的 DaemonSet 有相同的 .spec.selector，新建 DaemonSet 将直接把这些未删除的 Pod 纳入管理 DaemonSet 根据其 updateStrategy 决定是否更新这些 Pod 可以对 DaemonSet 执行滚动更新操作 Job Kubernetes中的 Job 对象将创建一个或多个 Pod，并确保指定数量的 Pod 可以成功执行到进程正常结束： 当 Job 创建的 Pod 执行成功并正常结束时，Job 将记录成功结束的 Pod 数量 当成功结束的 Pod 达到指定的数量时，Job 将完成执行 删除 Job 对象时，将清理掉由 Job 创建的 Pod 定义12345678910111213apiVersion: batch/v1kind: Jobmetadata: name: pispec: template: spec: containers: - name: pi image: perl command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"] restartPolicy: Never backoffLimit: 4 .spec.template 是必填字段 与 Pod 有相同的字段内容，但由于是内嵌元素，pod template 不包括阿 apiVersion 字段和 kind 字段 除了 Pod 所需的必填字段之外，Job 中的 pod template 必须指定 合适的标签 .spec.template.spec.labels 指定重启策略 .spec.template.spec.restartPolicy，此处只允许使用 Never 和 OnFailure 两个取值 处理 Pod 和容器的失败 Pod 中的容器可能会因为多种原因执行失败，例如： 容器中的进程退出了，且退出码（exit code）不为 0 容器因为超出内存限制而被 Kill 其他原因 整个 Pod 也可能因为多种原因执行失败，例如： Pod 从节点上被驱逐（节点升级、重启、被删除等） Pod 的容器执行失败，且 .spec.template.spec.restartPolicy = &quot;Never&quot; 即使指定 .spec.parallelism = 1、 .spec.completions = 1 以及 .spec.template.spec.restartPolicy = &quot;Never&quot;，同一个应用程序仍然可能被启动多次 如果指定 .spec.parallelism 和 .spec.completions 的值都大于 1，则，将可能有多个 Pod 同时执行 此时 Pod 还必须能够处理并发的情况 若期望在 Job 多次重试仍然失败的情况下停止该 Job 可通过 .spec.backoffLimit 来设定 Job 最大的重试次数 该字段的默认值为 6 Job 中的 Pod 执行失败之后，Job 控制器将按照一个指数增大的时间延迟（10s,20s,40s … 最大为 6 分钟）来多次重新创建 Pod 如果没有新的 Pod 执行失败，则重试次数的计数将被重置 建议在 debug 时，设置 restartPolicy = &quot;Never&quot;，或者使用日志系统确保失败的 Job 的日志不会丢失 终止和清理 当 Job 完成后： 将不会创建新的 Pod 已经创建 Job 和其 Pod 不会被清理掉 可以继续查看已结束 Pod 的日志，以检查 errors/warnings 或者其他诊断用的日志输出 可通过 kubectl delete 命令删除 Job .spec.activeDeadlineSeconds 字段限定了 Job 对象在集群中的存活时长 一旦达到 .spec.activeDeadlineSeconds 指定的时长，该 Job 创建的所有的 Pod 都将被终止，Job 的 Status 将变为 type:Failed 、 reason: DeadlineExceeded 优先级高于 .spec.backoffLimit 正在重试失败 Pod 的 Job，在达到 .spec.activeDeadlineSecondes 时，将立刻停止重试，即使 .spec.backoffLimit 还未达到 自动清理 CronJob 可以根据其中定义的基于容量的清理策略（capacity-based cleanup policy）自动清理 Job TTL 机制是另外一种自动清理已结束 Job（Completed 或 Finished）的方式： TTL 机制由 TTL 控制器提供 在 Job 对象中指定 .spec.ttlSecondsAfterFinished 字段可激活该特性 当 TTL 控制器清理 Job 时，TTL 控制器将删除 Job 对象，以及由该 Job 创建的所有 Pod 对象 删除 Job 时，其生命周期函数将被触发，例如 finalizer 12345678910111213apiVersion: batch/v1kind: Jobmetadata: name: pi-with-ttlspec: ttlSecondsAfterFinished: 100 template: spec: containers: - name: pi image: perl command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"] restartPolicy: Never Job pi-with-ttl 的 ttlSecondsAfterFinished 值为 100，则在其结束 100 秒之后，将可以被自动删除 如果 ttlSecondsAfterFinished 被设置为 0，则 TTL 控制器在 Job 执行结束后，立刻就可以清理该 Job 及其 Pod 如果 ttlSecondsAfterFinished 值未设置，则 TTL 控制器不会清理该 Job CronJob CronJob 按照预定的时间计划（schedule）创建 Job 所有 CronJob 的 schedule 中所定义的时间，都是基于 master 所在时区来进行计算的 CronJob 在时间计划中的每次执行时刻, 可能会创建不止一个 Job 对象, 所以需要保持 Job 的幂等性 定义123456789101112131415161718apiVersion: batch/v1beta1kind: CronJobmetadata: name: hellospec: schedule: "*/1 * * * *" jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure 所有对 CronJob 对象作出的修改，尤其是 .spec 的修改，都只对修改之后新建的 Job 有效，已经创建的 Job 不会受到影响 .spec.schedule 必填字段 类型为 Cron 格式的字符串，例如 0 * * * * 或者 @hourly，该字段定义了 CronJob 应该何时创建和执行 Job。 该字段同样支持 step 值（step values），参考 FreeBSD manual 例如，指定 CronJob 每隔两个小时执行一次，可以有如下三种写法： 0 0,2,4,5,6,8,12,14,16,17,20,22 * * * 使用 范围值 + Step 值的写法：0 0-23/2 * * * Step 也可以跟在一个星号后面，如 0 */2 * * * .spec.jobTemplate 必填字段 该字段的结构与 Job 相同，只是不需要 apiVersion 和 kind .spec.startingDeadlineSeconds 选填字段 代表从计划的时间点开始，最迟多少秒之内必须启动 Job, 超过了这个时间点，CronJob 就不会为其创建 Job CronJob 控制器将检查自上一次执行的时间点到现在为止有多少次执行被错过了 如果错过的执行次数超过了 100，则 CronJob 控制器将不再创建 Job 对象 如果设置了 startingDeadlineSeconds, 则按照从 startingDeadlineSeconds 秒之前到现在为止的时间段计算被错过的执行次数 例如被设置为 200，则控制器将计算过去 200 秒内，被错过的执行次数 .spec.concurrencyPolicy 选填字段 指定了如何控制该 CronJob 创建的 Job 的并发性，可选的值有： Allow： 默认值，允许并发运行 Job Forbid： 不允许并发运行 Job 如果新的执行时间点到了，而上一个 Job 还未执行完，则 CronJob 将跳过新的执行时间点，保留仍在运行的 Job，且不会在此刻创建新的 Job Replace： 如果新的执行时间点到了，而上一个 Job 还未执行完，则 CronJob 将创建一个新的 Job 以替代正在执行的 Job .spec.suspend 选填字段 如果该字段设置为 true，所有的后续执行都将挂起，该字段不会影响到已经创建的 Job 默认值为 false 挂起（suspend）的时间段内，如果恰好存在有计划的执行时间点，则这些执行时间计划都被记录下来 如果不指定 .spec.startingDeadlineSeconds，并将 .spec.suspend 字段从 true 修改为 false，则挂起这段时间内的执行计划都将被立刻执行 .spec.successfulJobsHistoryLimit 和 .spec.failedJobsHistoryLimit 字段是可选的 这些字段指定了 CronJob 应该保留多少个 completed 和 failed 的 Job 记录 .spec.successfulJobsHistoryLimit 的默认值为 3 .spec.failedJobsHistoryLimit 的默认值为 1 如果将其设置为 0，则 CronJob 不会保留已经结束的 Job 的记录 垃圾回收 Kubernetes garbage collector（垃圾回收器）的作用是删除那些曾经有 owner，后来又不再有 owner 的对象 所有者和从属对象 每一个从属对象都有一个 metadata.ownerReferences 字段，标识其拥有者是哪一个对象 对于 ReplicationController、ReplicaSet、StatefulSet、DaemonSet、Deployment、Job、CronJob等创建或管理的对象，Kubernetes 都将自动为其设置 ownerReference 的值 跨名称空间的所有者-从属对象的关系是不被允许的 名称空间内的从属对象只能指定同名称空间的对象作为其所有者 集群级别的对象只能指定集群级别的对象作为其所有者 垃圾收集器如何删除从属对象 当删除某个对象时，可以指定该对象的从属对象是否同时被自动删除，这种操作叫做级联删除（cascading deletion） 级联删除有两种模式：后台（background）和前台（foreground） 如果删除对象时不删除自动删除其从属对象，此时从属对象被认为是孤儿（或孤立的 orphaned） Foreground 级联删除 根对象（直接被删除的对象）先进入“正在删除”（deletion in progress）状态，此时： 对象仍然可以通过 REST API 查询到 对象的 deletionTimestamp 字段被设置 对象的 metadata.finalizers 包含值 foregroundDeletion 一旦对象被设置为 “deletion in progress” 状态，垃圾回收器将删除其从属对象 当垃圾回收器已经删除了所有的 “blocking” 从属对象（ownerReference.blockOwnerDeletion=true 的对象）以后，将删除所有者对象 在 “foregroundDeletion” 模式下，只有 ownerReference.blockOwnerDeletion=true 的对象将阻止所有者对象的删除 在 Kubernetes 版本 1.7 开始，增加了 admission controller ，可以基于所有者对象的删除权限配置限制用户是否可以设置 blockOwnerDeletion 字段为 true，因此未经授权的从属对象将不能阻止所有者对象的删除 如果对象的 ownerReferences 字段由控制器自动设置（例如，Deployment、ReplicaSet），blockOwnerDeletion 也将被自动设置，无需手工修改该字段的取值 Background 级联删除 将立刻删除所有者对象，并由垃圾回收器在后台删除其从属对象 设置级联删除策略 在删除对象时，通过参数 deleteOptions 的 propagationPolicy 字段，可以设置级联删除的策略。可选的值有： Orphan、Foreground、Background 默认值： 在 Kubernetes 1.9 之前，许多类型控制器的默认垃圾回收策略都是 orphan，例如，ReplicationController、ReplicaSet、StatefulSet、DaemonSet、Deployment 对于 apiVersion 为 extensions/v1beta1、apps/v1beta1、和 apps/v1beta2 的对象，除非特殊指定，垃圾回收策略默认为 orphan 在 Kubernetes 1.9 中，对于所有 apiVersion 为 apps/v1 的对象，从属对象默认都将被删除 服务与网络Service Service 是 Kubernetes 中的一种服务发现机制： Pod 有自己的 IP 地址 Service 被赋予一个唯一的 dns name Service 通过 label selector 选定一组 Pod Service 实现负载均衡，可将请求均衡分发到选定这一组 Pod 中 使用 Kubernetes 的最佳实践： Service 与 Controller 同名 Service 与 Controller 使用相同的 label selector Service 从自己的 IP 地址和 port 端口接收请求，并将请求映射到符合条件的 Pod 的 targetPort 相关 IP 概念 Node IP 集群中节点的物理网卡 IP 地址 (一般为内网) Pod IP 每个 Pod 的 IP 地址，是 Docker Engine 根据 docker0 网桥的 IP 地址段进行分配的 Cluster IP 虚拟的 IP，由 K8s 自己来进行管理和分配地址 仅仅作用于 Kubernetes Service 这个对象 只能结合 Service Port 来组成一个可以通信的服务 类型 ClusterIP 默认 type 在群集中的内部 IP 上公布服务，只在集群内部可以访问到 NodePort 使用 NAT 在集群中每个的同一端口上公布服务 可以通过访问集群中任意节点 + 端口号的方式访问服务 &lt;NodeIP&gt;:&lt;NodePort&gt; 此时 ClusterIP 的访问方式仍然可用 LoadBalancer 在云环境中（需要云供应商可以支持）创建一个集群外部的负载均衡器，并为使用该负载均衡器的 IP 地址作为服务的访问地址 此时 ClusterIP 和 NodePort 的访问方式仍然可用 ExternalName 通过返回 CNAME 和它的值，可以将服务映射到 externalName 字段的内容 使用 Pod 中可以直接访问 Service 名字 创建 Service12345678910111213141516apiVersion: v1kind: Servicemetadata: name: my-servicespec: selector: app: MyApp ports: - name: http protocol: TCP port: 80 targetPort: 9376 - name: https protocol: TCP port: 443 targetPort: 9377 端口的名字只能包含小写字母、数字、-，并且必须以数字或字母作为开头及结尾 Service 中额外字段的作用： service.spec.sessionAffinity 默认值为 “None” 如果设定为 “ClientIP”，则同一个客户端的连接将始终被转发到同一个 Pod service.spec.sessionAffinityConfig.clientIP.timeoutSeconds 默认值为 10800 （3 小时） 设定会话保持的持续时间 无 label selector Service 通常用于提供对 Kubernetes Pod 的访问，但也可以将其用于任何其他形式的后端。例如： 在生产环境中使用一个 Kubernetes 外部的数据库集群，在测试环境中使用 Kubernetes 内部的数据库 将 Service 指向另一个名称空间中的 Service，或者另一个 Kubernetes 集群中的 Service 正在将程序迁移到 Kubernetes，但是只将一部分后端程序运行在 Kubernetes 中 因为 Service 没有 selector，相应的 Endpoint 对象就无法自动创建 可以手动创建一个 Endpoint 对象，以便将该 Service 映射到后端服务真实的 IP 地址和端口 Endpoint 中的 IP 地址不可以是 loopback（127.0.0.0/8 IPv4 或 ::1/128 IPv6），或 link-local（169.254.0.0/16 IPv4、224.0.0.0/24 IPv4 或 fe80::/64 IPv6） Endpoint 中的 IP 地址不可以是集群中其他 Service 的 ClusterIP 1234567891011121314151617181920212223apiVersion: v1kind: Servicemetadata: name: my-servicespec: selector: app: MyApp ports: - protocol: TCP port: 80 targetPort: 9376 ---apiVersion: v1kind: Endpointsmetadata: name: my-servicesubsets: - addresses: - ip: 192.0.2.42 ports: - port: 9376 虚拟 IP 和服务代理 Kubernetes 集群中的每个节点都运行了一个 kube-proxy，负责为 Service（ExternalName 类型的除外）提供虚拟 IP 访问 使用 proxy 的原因大致有如下几个： DNS 不确保严格检查 TTL（Time to live），并且在缓存的 dns 解析结果应该过期以后，仍然继续使用缓存中的记录 某些应用程序只做一次 DNS 解析，并一直使用缓存下来的解析结果 即使应用程序对 DNS 解析做了合适的处理，为 DNS 记录设置过短（或者 0）的 TTL 值，将给 DNS 服务器带来过大的负载 User space 代理模式 kube-proxy 监听 kubernetes master 以获得添加和移除 Service / Endpoint 的事件 kube-proxy 在其所在的节点（每个节点都有 kube-proxy）上为每一个 Service 打开一个随机端口 kube-proxy 安装 iptables 规则，将发送到该 Service 的 ClusterIP（虚拟 IP）/ Port 的请求重定向到该随机端口 任何发送到该随机端口的请求将被代理转发到该 Service 的后端 Pod 上（kube-proxy 从 Endpoint 信息中获得可用 Pod） kube-proxy 在决定将请求转发到后端哪一个 Pod 时，默认使用 round-robin（轮询）算法，并会考虑到 Service 中的 SessionAffinity 的设定 Iptables 代理模式 (默认模式) kube-proxy 监听 kubernetes master 以获得添加和移除 Service / Endpoint 的事件 kube-proxy 在其所在的节点（每个节点都有 kube-proxy）上为每一个 Service 安装 iptable 规则 iptables 将发送到 Service 的 ClusterIP / Port 的请求重定向到 Service 的后端 Pod 上 对于 Service 中的每一个 Endpoint，kube-proxy 安装一个 iptable 规则 默认情况下，kube-proxy 随机选择一个 Service 的后端 Pod 优点: 更低的系统开销：在 linux netfilter 处理请求，无需在 user space 和 kernel space 之间切换 更稳定 与 user space mode 的差异： 使用 iptables mode 时，如果第一个 Pod 没有响应，则创建连接失败 使用 user space mode 时，如果第一个 Pod 没有响应，kube-proxy 会自动尝试连接另外一个后端 Pod 可以配置 Pod 就绪检查（readiness probe）确保后端 Pod 正常工作，此时在 iptables 模式下 kube-proxy 将只使用健康的后端 Pod，从而避免了 kube-proxy 将请求转发到已经存在问题的 Pod 上 IPVS 代理模式 kube-proxy 监听 kubernetes master 以获得添加和移除 Service / Endpoint 的事件 kube-proxy 根据监听到的事件，调用 netlink 接口，创建 IPVS 规则；并且将 Service/Endpoint 的变化同步到 IPVS 规则中 当访问一个 Service 时，IPVS 将请求重定向到后端 Pod 优点: 基于 netfilter 的 hook 功能，与 iptables 代理模式相似，但是 IPVS 代理模式使用 hash table 作为底层的数据结构，并在 kernel space 运作。这就意味着 IPVS 代理模式可以比 iptables 代理模式有更低的网络延迟，在同步代理规则时，也有更高的效率 与 user space 代理模式 / iptables 代理模式相比，IPVS 模式可以支持更大的网络流量 提供更多的负载均衡选项： rr: round-robin lc: least connection (最小打开的连接数) dh: destination hashing sh: source hashing sed: shortest expected delay nq: never queue 如果要使用 IPVS 模式，您必须在启动 kube-proxy 前为节点的 linux 启用 IPVS kube-proxy 以 IPVS 模式启动时，如果发现节点的 linux 未启用 IPVS，则退回到 iptables 模式 服务发现环境变量 kubelet 查找有效的 Service，并针对每一个 Service，向其所在节点上的 Pod 注入一组环境变量。支持的环境变量有： Docker links 兼容 的环境变量 {SVCNAME}_SERVICE_HOST 和 {SVCNAME}_SERVICE_PORT Service name 被转换为大写 小数点 . 被转换为下划线 _ 如果要在 Pod 中使用基于环境变量的服务发现方式，必须先创建 Service，再创建调用 Service 的 Pod 否则 Pod 中不会有该 Service 对应的环境变量 DNS CoreDNS 监听 Kubernetes API 上创建和删除 Service 的事件，并为每一个 Service 创建一条 DNS 记录 集群中所有的 Pod 都可以使用 DNS Name 解析到 Service 的 IP 地址 例如名称空间 my-ns 中的 Service my-service，将对应一条 DNS 记录 my-service.my-ns 名称空间 my-ns 中的 Pod 可以直接 nslookup my-service （my-service.my-ns 也可以） 其他名称空间的 Pod 必须使用 my-service.my-ns my-service 和 my-service.my-ns 都将被解析到 Service 的 Cluster IP 同样支持 DNS SRV（Service）记录，用于查找一个命名的端口 假设 my-service.my-ns Service 有一个 TCP 端口名为 http，则可以 nslookup _http._tcp.my-service.my-ns 以发现该Service 的 IP 地址及端口 http 对于 ExternalName 类型的 Service，只能通过 DNS 的方式进行服务发现 Headless Services “Headless” Service 不提供负载均衡的特性，也没有自己的 IP 地址 创建 “headless” Service 时，只需要指定 .spec.clusterIP 为 “None”。 “Headless” Service 可以用于对接其他形式的服务发现机制，而无需与 Kubernetes 的实现绑定 对于 “Headless” Service 而言： 没有 Cluster IP kube-proxy 不处理这类 Service Kubernetes 不提供负载均衡或代理支持 DNS 的配置方式取决于该 Service 是否配置了 selector： 配置了 Selector Endpoints Controller 创建 Endpoints 记录，并修改 DNS 配置，使其直接返回指向 selector 选取的 Pod 的 IP 地址 没有配置 Selector Endpoints Controller 不创建 Endpoints 记录。DNS服务返回如下结果中的一种： 对 ExternalName 类型的 Service，返回 CNAME 记录 对于其他类型的 Service，返回与 Service 同名的 Endpoints 的 A 记录 虚拟 IP 的实现避免冲突 Kubernetes 的一个设计哲学是：尽量避免非人为错误产生的可能性 就设计 Service 而言，Kubernetes 应该将选择的端口号与其他人选择的端口号隔离开 为此 Kubernetes 为每一个 Service 分配一个该 Service 专属的 IP 地址 为了确保每个 Service 都有一个唯一的 IP 地址，Kubernetes 在创建 Service 之前，先更新 etcd 中的一个全局分配表，如果更新失败（例如 IP 地址已被其他 Service 占用），则 Service 不能成功创建 Kubernetes 使用一个后台控制器检查该全局分配表中的 IP 地址的分配是否仍然有效，并且自动清理不再被 Service 使用的 IP 地址 Service 的 IP 地址 Pod 的 IP 地址路由到一个确定的目标，然而 Service 的 IP 地址则不同，通常背后并不对应一个唯一的目标 kube-proxy 使用 iptables （Linux 中的报文处理逻辑）来定义虚拟 IP 地址 当客户端连接到该虚拟 IP 地址时，它们的网络请求将自动发送到一个合适的 Endpoint Service 对应的环境变量和 DNS 实际上反应的是 Service 的虚拟 IP 地址（和端口） Userspace 当后端 Service 被创建时，Kubernetes master 为其分配一个虚拟 IP 地址（假设是 10.0.0.1），并假设 Service 的端口是 1234 集群中所有的 kube-proxy 都实时监听着 Service 的创建和删除 Service 创建后，kube-proxy 将打开一个新的随机端口，并设定 iptables 的转发规则（以便将该 Service 虚拟 IP 的网络请求全都转发到这个新的随机端口上），并且 kube-proxy 将开始接受该端口上的连接 当一个客户端连接到该 Service 的虚拟 IP 地址时，iptables 的规则被触发，并且将网络报文重定向到 kube-proxy 自己的随机端口上 kube-proxy 接收到请求后，选择一个后端 Pod，再将请求以代理的形式转发到该后端 Pod 这意味着 Service 可以选择任意端口号，而无需担心端口冲突 客户端可以直接连接到一个 IP:port，无需关心最终在使用哪个 Pod 提供服务 iptables 当后端 Service 被创建时，Kubernetes master 为其分配一个虚拟 IP 地址（假设是 10.0.0.1），并假设 Service 的端口是 1234 集群中所有的 kube-proxy 都实时监听者 Service 的创建和删除 Service 创建后，kube-proxy 设定了一系列的 iptables 规则（这些规则可将虚拟 IP 地址映射到 per-Service 的规则） per-Service 规则进一步链接到 per-Endpoint 规则，并最终将网络请求重定向（使用 destination-NAT）到后端 Pod 当一个客户端连接到该 Service 的虚拟 IP 地址时，iptables 的规则被触发 一个后端 Pod 将被选中（基于 session affinity 或者随机选择），且网络报文被重定向到该后端 Pod 与 userspace proxy 不同，网络报文不再被复制到 userspace，kube-proxy 也无需处理这些报文，且报文被直接转发到后端 Pod。 在使用 node-port 或 load-balancer 类型的 Service 时，以上的代理处理过程是相同的 IPVS 在一个大型集群中（例如，存在 10000 个 Service）iptables 的操作将显著变慢 IPVS 的设计是基于 in-kernel hash table 执行负载均衡 因此使用 IPVS 的 kube-proxy 在 Service 数量较多的情况下仍然能够保持好的性能 同时基于 IPVS 的 kube-proxy 可以使用更复杂的负载均衡算法（最少连接数、基于地址的、基于权重的等） ConfigMapSecret Opaque kubernetes.io/dockerconfigjson kubernetes.io/service-account-token Horizontal Pod Autoscaling (HPA) Heapster Job CronJob RBAC Role 和 ClusterRole Rule Subject User Account Group Service Account RoleBinding 和 ClusterRoleBinding DaemonSet StatefulSet 服务的有状态和无状态 持久化存储 PV PVC StorageClass 服务发现 内部服务发现 通过 apiserver 查询 依赖 K8s 通过环境变量 依赖的服务必须在 Pod 启动之前就存在 DNS Server kubedns coredns 外部服务发现 ingress Traefik Helm 概念 chart config release Helm Client Tiller Server]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql 定时备份 (基于 Dokcer 运行)]]></title>
    <url>%2F2020%2F01%2F17%2FMysql-%E5%AE%9A%E6%97%B6%E5%A4%87%E4%BB%BD-(%E5%9F%BA%E4%BA%8E-Dokcer-%E8%BF%90%E8%A1%8C)%2F</url>
    <content type="text"><![CDATA[脚本编写 vim mysql_bak.sh 123456789101112131415161718192021222324252627282930#!/bin/sh# 数据库相关配置host=1.2.3.4port=3306user=rootpassword=123456db_name=db# 存放目录 (绝对路径)backupdir=/mysqlbackup# 保留天数time=1echo "开始备份数据库";# 生成脚本echo "mysqldump --host=$host --port=$port -u$user -p$password --extended-insert $db_name | gzip &gt; /mysqlbackup/$db_name`date +%Y-%m-%d_%H%M%S`.sql.gz" &gt; $backupdir/run.sh# 赋权sudo chmod 744 $backupdir/run.sh# 启动 docker 运行脚本docker run --rm -v $backupdir:/mysqlbackup mysql:5.7 /mysqlbackup/run.sh# 删除 time 天前的备份文件, 若要改为分钟, 使用 -mminfind $backupdir -name $db_name"*.sql.gz" -type f -mtime +$time -exec rm -rf &#123;&#125; \; echo "备份完成"; 赋权 sudo chmod 744 mysql_bak.sh 执行测试是否成功 sudo ./mysql_bak.sh 查看对应目录下是否生成备份文件 配置定时任务 用 crontab 定时执行备份脚本代码 sudo crontab -u root -e 以 root 身份打开编辑 crontab 的工作内容 加入内容, 若要每小时 30 分时备份, 输入以下内容 30 * * * * /(脚本所在路径)/mysql_bak.sh &gt;&gt; /(脚本所在路径)/mysql_bak.log Ubuntu 系统默认是不打开 cron 日志, 需要配置: 打开文件 sudo vi /etc/rsyslog.d/50-default.conf 在文件中找到 cron.*，把前面的 # 去掉 保存退出 重新加载配置 sudo service rsyslog restart 查看日志 tail -f /var/log/cron.log 其他 恢复数据时, 可能会因为 sql 文件过大报错, 需要修改 max_allowed_packet 的值, 默认为 4MB 12345678# 查看当前 max_allowed_packet 的值mysql&gt; show variables like &apos;max_allowed_packet&apos;;# 数据库中临时修改（重启数据库后失效）, 最大可以设置为 1Gmysql&gt; set global max_allowed_packet = 1 * 1024 * 1024 * 1024;# 需要退出保存mysql&gt; exit]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 基本使用]]></title>
    <url>%2F2019%2F08%2F02%2FDocker-%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[停止并删除所有容器1$ docker stop $(docker ps -q) docker rm $(docker ps -aq) 清理镜像12345# 清理 dangling 镜像$ docker rmi $(docker images -f "dangling=true" -q)# 根据通配符查找删除指定镜像$ docker rmi $(docker images -f "reference=*" --format "&#123;&#123;.Repository&#125;&#125;:&#123;&#123;.Tag&#125;&#125;") 创建 Swarm 集群以及其基本操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# 修改 node name￼$ sudo hostnamectl set-hostname &#123;$name&#125;￼$ sudo systemctl restart docker# ￼manager: 创建 swarm 集群￼$ ￼docker swarm init# 创建 swarm 集群时指定 ip 和通讯的 udp 端口 (针对云服务器上部分环境时需用到)$ docker swarm init --advertise-addr 1.2.3.4 --data-path-port 12345# ￼node: 加入 swarm 集群￼$ ￼docker swarm join ......# ￼获取加入token￼$ ￼docker swarm join-token [worker|manager]# ￼manager: 查看节点￼$ ￼docker node ls# 部署单个服务$ docker service create --replicas 3 -p 80:80 --name nginx nginx# 查看服务$ docker service ls # 服务伸缩$ docker service scale nginx=5# 删除服务$ docker service rm nginx# 根据 yaml 文件部署包含多个服务的 stack$ docker stack deploy -c xxxx.yml one-stack# 根据 yaml 文件部署 stack 时能读取同层 .env 文件内设置的环境变量$ docker stack deploy -c &lt;(docker-compose -f xxxx.yml config) one-stack# 根据 yaml 文件部署 stack 时能读取同层 .env 文件内设置的环境变量, 从私库拉取镜像$ docker stack deploy --with-registry-auth -c &lt;(docker-compose -f xxxx.yml config) one-stack# 查看 stack$ docker stack ls # 查看 stack 中的服务$ docker stack ps one-stack # 停止删除 stack (该命令不会移除服务所使用的数据卷，移除数据卷用 docker volume rm)$ docker stack down one-stack# 更新 service 镜像, 从私库拉取$ docker service update --with-registry-auth --image image-name:tag service-name# 批量删除 service$ docker service rm $(docker service ls -qf name=)# 指定 service 在某个 node 运行# 给 node 打标签$ docker node update --label-add run-a=1 xxxxxx# 给 service 加入约束条件$ docker service update --constraint-add "node.labels.run-a==1" swarm_a Docker Compose 文件相关配置 参考官方文档 yaml 中重用代码块 12345678910test: &amp;base a: 1 b: 2use1: *baseuse2: &lt;&lt;: *base b: 3 c: 4 123456789101112test: a: 1 b: 2 use1: a: 1 b: 2 use2: a: 1 b: 3 c: 4 Dockerfile ONBUILD 指令 格式：ONBUILD &lt;其它指令&gt;。 ONBUILD是一个特殊的指令，它后面跟的是其它指令，比如 RUN, COPY 等，而这些指令，在当前镜像构建时并不会被执行。只有当以当前镜像为基础镜像，去构建下一级镜像的时候才会被执行。 1234567FROM node:slimRUN mkdir /appWORKDIR /appONBUILD COPY ./package.json /appONBUILD RUN [ "npm", "install" ]ONBUILD COPY . /app/CMD [ "npm", "start" ] 这样各个子镜像在构建的时候就不需要重复加入上面三行命令 多阶段构建 12345678910111213FROM golang AS build-envADD . /go/src/appWORKDIR /go/src/appRUN go get -u -v github.com/kardianos/govendorRUN govendor syncRUN GOOS=linux GOARCH=386 go build -v -o /go/src/app/app-serverFROM alpineRUN apk add -U tzdataRUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimeCOPY --from=build-env /go/src/app/app-server /usr/local/bin/app-serverEXPOSE 8080CMD [ "app-server" ] 默认情况下，构建阶段是没有命令的，我们可以通过它们的索引来引用它们，第一个 FROM 指令从0开始，我们也可以用 AS 指令为阶段命令，比如我们这里的将第一阶段命名为 build-env，然后在其他阶段需要引用的时候使用 --from=build-env 参数即可]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>docker-swarm</tag>
        <tag>docker-compose</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在服务器上的相关运维操作]]></title>
    <url>%2F2019%2F08%2F02%2F%E5%9C%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E7%9A%84%E7%9B%B8%E5%85%B3%E8%BF%90%E7%BB%B4%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[安装 Docker 其他系统的安装可以参考官方文档 Ubuntu 12345678910111213141516171819202122232425262728# 安装 Docker 依赖库$ sudo apt-get update $ sudo apt-get install apt-transport-https ca-certificates \ gnupg-agent software-properties-common \ curl# 清理过去遗留的老版本 $ sudo apt-get remove docker docker-engine docker.io containerd runc# 添加 Docker 官方的 GPG key$ sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -# 添加 docker 仓库$ sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"# 安装 Docker CE$ sudo apt-get update $ sudo apt-get install docker-ce docker-ce-cli containerd.io# 验证$ sudo docker run hello-world# 配置镜像加速器$ echo '&#123; "registry-mirrors": ["https://umbmb1yu.mirror.aliyuncs.com"] &#125;' | sudo tee /etc/docker/daemon.json$ sudo systemctl daemon-reload$ sudo systemctl restart docker# 登录私库$ sudo docker login --username=abmatrix registry.cn-hangzhou.aliyuncs.com 12# 使用阿里官方安装脚本自动安装 （仅适用于公网环境）$ sudo curl -fsSL https://get.docker.com | sudo bash -s docker --mirror Aliyun 删除 Docker12345# 卸载 Docker CE 包$ sudo apt-get purge docker-ce# 删除 images、containers 和 volumes$ sudo rm -rf /var/lib/docker 修改 hostname12# 修改 hostname$ hostnamectl set-hostname --static abm-aliyun-x 开启 ssh 密码登录1234567$ vim /etc/ssh/sshd_config修改 PasswordAuthentication yes若无法使用 root 用户进行 SSH 登录, 还需修改 PermitRootLogin yes$ service sshd restart 挂载硬盘12345678910111213# 查看主机上的硬盘$ fdisk -l# 硬盘格式化$ mkfs.ext4 /dev/vdb# 挂载$ mount /dev/vdb /mnt# 开机自动挂载, 编辑 /etc/fstab 文件$ vim /etc/fstab# 追加/dev/vdb /mnt ext4 defaults 0 0 添加用户并并将用户加入 Docker 组12345678910111213141516171819202122# 添加用户$ adduser abm# 加入 sudo 权限$ chmod u+w /etc/sudoers$ echo "abm ALL=(ALL:ALL) ALL" &gt;&gt; /etc/sudoers$ chmod u-w /etc/sudoers# 查看是否存在 docker 组$ cat /etc/group | grep docker# 不存的话创建 docker 分组$ groupadd docker# 对应用户登录# 将用户添加 docker 分组$ sudo gpasswd -a $USER docker$ newgrp docker# 设置权限$ sudo chown "$USER":"$USER" /home/"$USER"/.docker -R$ sudo chmod g+rwx "/home/$USER/.docker" -R Docker 其他组件安装12345678# 安装 docker-compose$ sudo curl -L "https://github.com/docker/compose/releases/download/1.25.5/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose# 设置权限$ sudo chmod +x /usr/local/bin/docker-compose# 安装命令补全$ sudo curl -L https://raw.githubusercontent.com/docker/compose/1.25.5/contrib/completion/bash/docker-compose -o /etc/bash_completion.d/docker-compose 修改 Docker 储存位置1234567891011# 停止服务$ sudo service docker stop# 转移$ sudo mv /var/lib/docker /mnt/data/docker# 创建软链接$ sudo ln -s /mnt/data/docker /var/lib/docker# 启动服务$ sudo service docker start 设置防火墙规则123456789101112131415161718# 指定端口$ sudo ufw allow 22# Docker Swarm 相关端口$ sudo ufw allow 2376/tcp$ sudo ufw allow 2377/tcp$ sudo ufw allow 7946/tcp$ sudo ufw allow 7946/udp$ sudo ufw allow 4789/udp# 指定 ip$ sudo ufw allow from 123.123.123.123# 开启$ sudo ufw enable# 重新加载$ sudo ufw reload 测试 UPD 端口123456789# 服务端监听：$ nc -l -u 192.168.80.129 8001# 客户端：$ nc -u 192.168.80.129 8001# 在这里输入字符串， 服务端就会回显相同的字符串，表示 8001 端口上的 udp 服务是否启用.# 查看 udp 连接$ netstat -nua]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gitlab Devops 配置]]></title>
    <url>%2F2019%2F06%2F13%2FGitlab-Devops-%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[K8s 环境在 Kubernetes 集群上安装 Gitlab-Runnerhttps://docs.gitlab.com/runner/install/kubernetes.html 123kubectl patch deploy --namespace kube-system tiller-deploy -p '&#123;"spec":&#123;"template":&#123;"spec":&#123;"serviceAccount":"tiller"&#125;&#125;&#125;&#125;'helm install --namespace gitlab --name gitlab-runner -f values.yaml gitlab/gitlab-runner values.yaml 文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282## GitLab Runner Image#### By default it's using gitlab/gitlab-runner:alpine-v&#123;VERSION&#125;## where &#123;VERSION&#125; is taken from Chart.yaml from appVersion field#### ref: https://hub.docker.com/r/gitlab/gitlab-runner/tags/##image: gitlab/gitlab-runner:alpine-v12.0.0-rc1## Specify a imagePullPolicy## 'Always' if imageTag is 'latest', else set to 'IfNotPresent'## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images##imagePullPolicy: IfNotPresent## The GitLab Server URL (with protocol) that want to register the runner against## ref: https://docs.gitlab.com/runner/commands/README.html#gitlab-runner-register##gitlabUrl: https://gitlab.com/## The Registration Token for adding new Runners to the GitLab Server. This must## be retrieved from your GitLab Instance.## ref: https://docs.gitlab.com/ce/ci/runners/README.html##runnerRegistrationToken: "NkeYywTkYwWf14JZxtM1"## The Runner Token for adding new Runners to the GitLab Server. This must## be retrieved from your GitLab Instance. It is token of already registered runner.## ref: (we don't yet have docs for that, but we want to use existing token)### runnerToken: ""### Unregister all runners before termination#### Updating the runner's chart version or configuration will cause the runner container## to be terminated and created again. This may cause your Gitlab instance to reference## non-existant runners. Un-registering the runner before termination mitigates this issue.## ref: https://docs.gitlab.com/runner/commands/README.html#gitlab-runner-unregister##unregisterRunners: true## Set the certsSecretName in order to pass custom certficates for GitLab Runner to use## Provide resource name for a Kubernetes Secret Object in the same namespace,## this is used to populate the /etc/gitlab-runner/certs directory## ref: https://docs.gitlab.com/runner/configuration/tls-self-signed.html#supported-options-for-self-signed-certificates### certsSecretName:## Configure the maximum number of concurrent jobs## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-global-section##concurrent: 10## Defines in seconds how often to check GitLab for a new builds## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-global-section##checkInterval: 30## Configure GitLab Runner's logging level. Available values are: debug, info, warn, error, fatal, panic## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-global-section### logLevel:## For RBAC support:rbac: create: true ## Run the gitlab-bastion container with the ability to deploy/manage containers of jobs ## cluster-wide or only within namespace clusterWideAccess: false ## Use the following Kubernetes Service Account name if RBAC is disabled in this Helm chart (see rbac.create) ## # serviceAccountName: default## Configure integrated Prometheus metrics exporter## ref: https://docs.gitlab.com/runner/monitoring/#configuration-of-the-metrics-http-servermetrics: enabled: true## Configuration for the Pods that that the runner launches for each new job##runners: ## Default container image to use for builds when none is specified ## image: ubuntu:16.04 ## Specify one or more imagePullSecrets ## ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ ## # imagePullSecrets: [] ## Specify the image pull policy: never, if-not-present, always. The cluster default will be used if not set. ## # imagePullPolicy: "" ## Defines number of concurrent requests for new job from GitLab ## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-runners-section ## # requestConcurrency: 1 ## Specify whether the runner should be locked to a specific project: true, false. Defaults to true. ## # locked: true ## Specify the tags associated with the runner. Comma-separated list of tags. ## ## ref: https://docs.gitlab.com/ce/ci/runners/#using-tags ## tags: "k8s-runner" ## Run all containers with the privileged flag enabled ## This will allow the docker:dind image to run if you need to run Docker ## commands. Please read the docs before turning this on: ## ref: https://docs.gitlab.com/runner/executors/kubernetes.html#using-docker-dind ## privileged: true ## The name of the secret containing runner-token and runner-registration-token # secret: gitlab-runner ## Namespace to run Kubernetes jobs in (defaults to the same namespace of this release) ## namespace: gitlab ## Distributed runners caching ## ref: https://gitlab.com/gitlab-org/gitlab-runner/blob/master/docs/configuration/autoscale.md#distributed-runners-caching ## ## If you want to use s3 based distributing caching: ## First of all you need to uncomment General settings and S3 settings sections. ## ## Create a secret 's3access' containing 'accesskey' &amp; 'secretkey' ## ref: https://aws.amazon.com/blogs/security/wheres-my-secret-access-key/ ## ## $ kubectl create secret generic s3access \ ## --from-literal=accesskey="YourAccessKey" \ ## --from-literal=secretkey="YourSecretKey" ## ref: https://kubernetes.io/docs/concepts/configuration/secret/ ## ## If you want to use gcs based distributing caching: ## First of all you need to uncomment General settings and GCS settings sections. ## ## Access using credentials file: ## Create a secret 'google-application-credentials' containing your application credentials file. ## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-runners-cache-gcs-section ## You could configure ## $ kubectl create secret generic google-application-credentials \ ## --from-file=gcs-applicaton-credentials-file=./path-to-your-google-application-credentials-file.json ## ref: https://kubernetes.io/docs/concepts/configuration/secret/ ## ## Access using access-id and private-key: ## Create a secret 'gcsaccess' containing 'gcs-access-id' &amp; 'gcs-private-key'. ## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-runners-cache-gcs-section ## You could configure ## $ kubectl create secret generic gcsaccess \ ## --from-literal=gcs-access-id="YourAccessID" \ ## --from-literal=gcs-private-key="YourPrivateKey" ## ref: https://kubernetes.io/docs/concepts/configuration/secret/ cachePath: "/opt/gitlab_runner/cache" cache: &#123;&#125; ## General settings # cacheType: s3 # cachePath: "/opt/gitlab_runner/cache" # cacheShared: true ## S3 settings # s3ServerAddress: s3.amazonaws.com # s3BucketName: # s3BucketLocation: # s3CacheInsecure: false # secretName: s3access ## GCS settings # gcsBucketName: ## Use this line for access using access-id and private-key # secretName: gcsaccess ## Use this line for access using google-application-credentials file # secretName: google-application-credentials ## Build Container specific configuration ## builds: &#123;&#125; # cpuLimit: 200m # memoryLimit: 256Mi # cpuRequests: 100m # memoryRequests: 128Mi # image: gitlab/gitlab-runner-helper:x86_64-latest ## Service Account to be used for runners ## # serviceAccountName: ## If Gitlab is not reachable through $CI_SERVER_URL ## # cloneUrl: ## Specify node labels for CI job pods assignment ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ## # nodeSelector: &#123;&#125; ## Specify pod labels for CI job pods ## # podLabels: &#123;&#125; ## Specify annotations for job pods, useful for annotations such as iam.amazonaws.com/role # podAnnotations: &#123;&#125; ## Configure environment variables that will be injected to the pods that are created while ## the build is running. These variables are passed as parameters, i.e. `--env "NAME=VALUE"`, ## to `gitlab-runner register` command. ## ## Note that `envVars` (see below) are only present in the runner pod, not the pods that are ## created for each build. ## ## ref: https://docs.gitlab.com/runner/commands/#gitlab-runner-register ## # env: # NAME: VALUE## Configure resource requests and limits## ref: http://kubernetes.io/docs/user-guide/compute-resources/##resources: &#123;&#125; # limits: # memory: 256Mi # cpu: 200m # requests: # memory: 128Mi # cpu: 100m## Affinity for pod assignment## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity##affinity: &#123;&#125;## Node labels for pod assignment## Ref: https://kubernetes.io/docs/user-guide/node-selection/##nodeSelector: &#123;&#125; # Example: The gitlab runner manager should not run on spot instances so you can assign # them to the regular worker nodes only. # node-role.kubernetes.io/worker: "true"## List of node taints to tolerate (requires Kubernetes &gt;= 1.6)## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/##tolerations: [] # Example: Regular worker nodes may have a taint, thus you need to tolerate the taint # when you assign the gitlab runner manager with nodeSelector or affinity to the nodes. # - key: "node-role.kubernetes.io/worker" # operator: "Exists"## Configure environment variables that will be present when the registration command runs## This provides further control over the registration process and the config.toml file## ref: `gitlab-runner register --help`## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html### envVars:# - name: RUNNER_EXECUTOR# value: kubernetes## list of hosts and IPs that will be injected into the pod's hosts filehostAliases: [] # Example: # - ip: "127.0.0.1" # hostnames: # - "foo.local" # - "bar.local" # - ip: "10.1.2.3" # hostnames: # - "foo.remote" # - "bar.remote"## Annotations to be added to manager pod##podAnnotations: &#123;&#125; # Example: # iam.amazonaws.com/role: &lt;my_role_arn&gt; 123456gcr.io/kubernetes-helm/tiller:v2.12.3docker pull fishead/gcr.io.kubernetes-helm.tiller:v2.12.3docker tag fishead/gcr.io.kubernetes-helm.tiller:v2.12.3 gcr.io/kubernetes-helm/tiller:v2.12.3去 docker hub 找个镜像仓库 配置 Kubernetes cluster details 参考 官方文档 - Adding an existing Kubernetes cluster 的说明 安装 Helm Tiller 点击页面 Install 按钮 kubectl get pods -A 能看到 install-helm 的 pod 根据 pod/install-helm 生成 yaml 1kubectl get pod install-helm -n gitlab-managed-apps -o yaml &gt; install-helm.yaml 修改 yaml 中的 pod 启动命令, 加入阿里云仓库 1234567891011121314151617181920212223242526apiVersion: v1kind: Podmetadata:......spec: containers: - args: - -c - $(COMMAND_SCRIPT) command: - /bin/sh env: - name: HELM_VERSION value: 2.12.3 - name: TILLER_NAMESPACE value: gitlab-managed-apps - name: COMMAND_SCRIPT value: |- set -xeo pipefail helm init --tiller-tls --tiller-tls-verify --tls-ca-cert /data/helm/helm/config/ca.pem --tiller-tls-cert /data/helm/helm/config/cert.pem --tiller-tls-key /data/helm/helm/config/key.pem --service-account tiller --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.12.3 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts helm repo add stable https://burdenbear.github.io/kube-charts-mirror/ helm repo update...... 根据修改后的 yaml 重新创建 pod 12kubectl delete pod install-helm -n gitlab-managed-apps kubectl apply -f install-helm.yaml 删除 namespace 1kubectl delete namespaces gitlab-managed-apps 重新点击页面 Install 按钮 非 K8s 环境在 Docker 中部署 GitLab Runner运行 GitLab Runner 命令行 1234$ docker run -d --name gitlab-runner --restart always \ -v /path/to/gitlab-runner/config:/etc/gitlab-runner \ -v /var/run/docker.sock:/var/run/docker.sock \ gitlab/gitlab-runner:latest docke-compose 123456789version: '3.7'services: gitlab-runner: image: gitlab/gitlab-runner:latest restart: always volumes: - ./gitlab-runner/config:/etc/gitlab-runner - /var/run/docker.sock:/var/run/docker.sock 注册 GitLab Runner 执行命令注册 123456789docker exec gitlab-runner gitlab-runner register -n \ --url https://gitlab.com/ \ --registration-token xxxxxxxxxx \ --executor docker \ --tag-list runInDk \ --description "My Docker Runner" \ --docker-image "docker:latest" \ --docker-volumes /var/run/docker.sock:/var/run/docker.sock \ --docker-volumes /root/.m2:/root/.m2 优化工作 Runner 默认情况下每执行一个 Job 都会重新拉取一次所需镜像，我们可把策略改为：镜像不存在时才拉取，编辑 config.toml 文件，修改 [runners.docker] 栏中加入 pull_policy = &quot;if-not-present&quot; 重启 Runner 容器，使之生效 编写项目的 gitlab-ci.yml 文件 后端项目 12345678910111213141516171819202122232425262728293031323334353637stages: - package - build - deploymy_package: image: maven:3.6.0-jdk-8-alpine stage: package script: - mvn clean package -DskipTests - cp Dockerfile target/Dockerfile cache: key: $&#123;CI_PIPELINE_ID&#125; paths: - target/ only: - master tags: - lvTestmy_build: stage: build cache: key: $&#123;CI_PIPELINE_ID&#125; paths: - target/ script: - cd target - docker build -t 192.168.88.4:5000/$&#123;CI_PROJECT_NAME&#125;:$&#123;CI_PIPELINE_ID&#125; . - docker push 192.168.88.4:5000/$&#123;CI_PROJECT_NAME&#125;:$&#123;CI_PIPELINE_ID&#125; tags: - lvTestmy_deploy: stage: deploy script: - docker stop lxwtest &amp;&amp; docker rm lxwtest - docker run -d -p 8888:8080 --restart=always --name=lxwtest 192.168.88.4:5000/$&#123;CI_PROJECT_NAME&#125;:$&#123;CI_PIPELINE_ID&#125; tags: - lvTest 前端项目]]></content>
      <categories>
        <category>programming</category>
      </categories>
      <tags>
        <tag>gitlab</tag>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Effective Java 笔记]]></title>
    <url>%2F2019%2F06%2F08%2FEffective-Java-%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[1. 考虑使用静态工厂方法替代构造方法优点 通过方法名体现不同构造方式的差异 不需要每次调用时都创建一个新对象 ( 单例模式, 享元模式 ) 可以返回其返回类型的任何子类型的对象 在编写包含该方法的类时，返回的对象的类不需要存在 限制 没有公共或受保护构造方法的类不能被子类化 ( 因为大部分实现会将构造函数设为私有 ) 没有构造函数那么明显 2. 当构造方法参数过多时使用 builder 模式分析 可伸缩 (telescoping constructor) 构造方法模式, 多个不同参数数量的构造方法 当有很多参数时，很难编写代码，而且很难读懂它 JavaBeans 模式, 调用 setter 方法来设置参数 由于构造方法在多次调用中被分割，在过程中可能处于不一致的状态 Builder 模式, 构造方法为 private, 通过内嵌的 Builder 类调用 build() 返回本身 12NutritionFacts cocaCola = new NutritionFacts.Builder(240, 8) .calories(100).sodium(35).carbohydrate(27).build(); 可以在 build() 里加入参数检查 编写简单, 易读 能区分必须和可选字段 平行层次的 Builder, 抽象类有抽象的 builder, 具体的类有具体的 builder 12345NyPizza pizza = new NyPizza.Builder(SMALL) .addTopping(SAUSAGE).addTopping(ONION).build();Calzone calzone = new Calzone.Builder() .addTopping(HAM).sauceInside().build(); 3. 使用私有构造方法或枚类实现 Singleton 属性私有构造方法 通过公共静态成员提供访问 123456// Singleton with public final fieldpublic class Elvis &#123; public static final Elvis INSTANCE = new Elvis(); private Elvis() &#123; ... &#125; public void leaveTheBuilding() &#123; ... &#125;&#125; 通过静态的工厂方法提供访问 优点: 可以灵活地改变你的想法 优点: 可以编写一个泛型单例工厂 优点: 方法引用可以用 Supplier 1234567 // Singleton with static factorypublic class Elvis &#123; private static final Elvis INSTANCE = new Elvis(); private Elvis() &#123; ... &#125; public static Elvis getInstance() &#123; return INSTANCE; &#125; public void leaveTheBuilding() &#123; ... &#125;&#125; 警告 可以使用 AccessibleObject.setAccessible 方法，以反射方式调用私有构造方法 为了维护单例的保证，声明所有的实例属性为 transient，并提供一个 readResolve 方法 枚举 声明单一元素的枚举类 简洁 提供了免费的序列化机制 提供了针对多个实例化的保证 12345// Enum singleton - the preferred approachpublic enum Elvis &#123; INSTANCE; public void leaveTheBuilding() &#123; ... &#125;&#125; 4. 使用私有构造方法执行非实例化 如一些工具类, 只包含静态方法, 可以用来避免其实例化和被继承 12345678// Noninstantiable utility classpublic class UtilityClass &#123; // Suppress default constructor for noninstantiability private UtilityClass() &#123; throw new AssertionError(); &#125; ... // Remainder omitted&#125; 5. 使用依赖注入取代硬连接资源(hardwiringresources) 不要使用单例或静态的实用类来实现一个类，该类依赖于一个或多个底层资源，这些资源的行为会影响类的行为，并且不让类直接创建这些资源。相反，将资源或工厂传递给构造方法 (或静态工厂或 builder 模式)。这种称为依赖注入的实践将极大地增强类的灵活性、可重用性和可测试性。 6. 避免创建不必要的对象 如果对象是不可变的，它总是可以被重用 注意无意识的自动装箱 (autoboxing) 在现代 JVM 实现上, 使用构造方法创建和回收小的对象其实是非常廉价的, 创建额外的对象以增强程序的清晰度，简单性或功能性通常是件好事 除非池中的对象非常重量级，否则通过维护自己的对象池来避免对象创建是一个坏主意 7. 消除过期的对象引用 当一个类自己管理内存时，程序员应该警惕内存泄漏问题 每当一个元素被释放时，元素中包含的任何对象引用都应该被清除 另一个常见的内存泄漏来源是缓存 只要在缓存之外存在对某个项 (entry) 的键 (key) 引用，那么这项就是明确有关联的，就可以用 WeakHashMap 来表示缓存 可以通过一个后台线程或将新的项添加到缓存时顺便清理 第三个常见的内存泄漏来源是监听器和其他回调 客户端注册回调, 仅将它们保存在 WeakHashMap 的键 (key) 中 8. 避免使用 Finalizer 和 Cleaner 机制缺点 不能保证他们能够及时执行 不要相信 System.gc 和 System.runFinalization 方法, 可能会增加被执行的几率，但不能保证一定会执行 在执行 Finalizer 机制过程中，未捕获的异常会被忽略 导致严重的性能损失 严重的安全问题: 它们会打开你的类来进行 Finalizer 机制攻击 正确做法 实现 AutoCloseable 接口，并要求客户在不再需要时调用每个实例 close 方法，通常使用 try-with-resources 确保终止 合法用途 作为一个安全网 (safetynet)，以防资源的拥有者忽略了它的 close 方法 与本地对等类(native peers)有关。本地对等类是一个由普通对象委托的本地 (非 Java) 对象。由于本地对等类不是普通的 Java 对象，所以垃圾收集器并不知道它，当它的 Java 对等对象被回收时，本地对等类也不会回收。 9. 使用 try-with-resources 语句替代 try-finally 语句对比 try-finally 12345678910111213141516// try-finally is ugly when used with more than one resource!static void copy(String src, String dst) throws IOException &#123; InputStream in = new FileInputStream(src); try &#123; OutputStream out = new FileOutputStream(dst); try &#123; byte[] buf = new byte[BUFFER_SIZE]; int n; while ((n = in.read(buf)) &gt;= 0) out.write(buf, 0, n); &#125; finally &#123; out.close(); &#125; &#125; finally &#123; in.close();&#125; &#125; try-with-resources 12345678// try-with-resources on multiple resources - short and sweetstatic void copy(String src, String dst) throws IOException &#123; try (InputStream in = new FileInputStream(src); OutputStream out = new FileOutputStream(dst)) &#123; byte[] buf = new byte[BUFFER_SIZE]; int n; while ((n = in.read(buf)) &gt;= 0)&#125; &#125; 分析 用 try-finally 语句关闭资源, 有多个资源时容易犯错 try-with-resources 块和 finally 块中的代码都可能抛出异常 finally 块中close()时抛出的异常会冲掉前面的异常 try-with-resources 块中close()(不可见) 时抛出的异常会被抑制 (suppressed), 这些抑制的异常没 有被抛弃， 而是打印在堆栈跟踪中，并标注为被抑制了 10. 重写 equals 方法时遵守通用约定不覆盖 equals 方法的场景 每个类的实例都是固有唯一的 类不需要提供一个“逻辑相等(logical equality)”的测试功能 父类已经重写了 equals 方法，则父类行为完全适合于该子类 类是私有的或包级私有的 覆盖 equals 方法的场景 需要提供一个逻辑相等的判断, 且父类还没重写过equals 通用约定 自反性 (Reflexivity): 对于任何非空引用 x， x.equals(x) 必须返回 true 对称性 (Symmetry): 对于任何非空引用 x 和 y，如果且仅当 y.equals(x) == true, x.equals(y)必须为 true 传递性 (Transitivity): 对于任何非空引用 x、y、z，如果 x.equals(y) == true, y.equals(z) == true, 则x.equals(z)必须返回 true 一致性 (Consistent): 对于任何非空引用 x 和 y，如果在 equals 比较中使用的信息没有修改，则x.equals(y)的多次调用必须始终返回 true 或始终返回 false 非空性 (Non-nullity): 对于任何非空引用 x, x.equals(null) 必须返回 false 编写高质量 equals 方法 使用 instanceof 运算符来检查参数是否具有正确的类型, 再将参数转换为正确的类型 检查是否与类中的每个重要属性相匹配 不同类型的比较方式 对于类型为非 float 或 double 的基本类型，使用 == 运算符进行比较 对于对象引用属性，递归地调用 equals 方法 对于 float 基本类型的属性，使用静态方法 Float.compare(float, float) 对于 double 基本类型的属性，使用静态方法 Double.compare(double, double) 对于数组属性，将这些准则应用于每个元素 某些对象引用的属性可能合法地包含 null。 为避免出现 NullPointerException 异常，请使用静态方法Objects.equals(Object, Object) 检查这些属性是否相等 性能优化 用 == 运算符检查参数是否为该对象的引用, 如果是, 返回 true 首先比较最可能不同的属性, 开销比较小的属性 不要比较不属于对象逻辑状态的属性 不需要比较可以从“重要属性”计算出来的派生属性 注意 当重写 equals 方法时，同时也要重写 hashCode 方法 不要让 equals 方法试图太聪明, 例如File 类不应该试图将引用的符号链接等同于同一文件对象 在 equal 时方法声明中，不要将参数 Object 替换成其他类型 11. 重写 equals 方法时同时也要重写 hashcode 方法Object 规范 当在一个应用程序执行过程中，如果在 equals 方法比较中没有修改任何信息，在一个对象上重复调用 hashCode 方法时，它必须始终返回相同的值。从一个应用程序到另一个应用程序的每一次执行返回的值可以是不一致的。 如果两个对象根据 equals(Object) 方法比较是相等的，那么在两个对象上调用 hashCode 就必须产生的结果是相同的整数。 如果两个对象根据 equals(Object) 方法比较并不相等，则不要求在每个对象上调用 hashCode 都必须产生不同的结果。 但是，程序员应该意识到，为不相等的对象生成不同的结果可能会提高散列表(hash tables)的性能。 一个简单的配方12345678// Typical hashCode method@Overridepublic int hashCode() &#123; int result = Short.hashCode(areaCode); result = 31 * result + Short.hashCode(prefix); result = 31 * result + Short.hashCode(lineNum); return result;&#125; 基本类型，使用 Type.hashCode(f) 方法计算，其中 Type 类是对应属性 f 基本类型的包装类。 如果该属性是一个对象引用，并且该类的 equals 方法通过递归调用 equals 来比较该属性，并递归地调用 hashCode 方法。 如果需要更复杂的比较，则计算此字段的“范式(“canonical representation)”，并在范式上调用 hashCode。 如果该字段的值为空，则使用 0(也可以使用其他常数，但通常来使用 0 表示)。 如果属性 f 是一个数组，把它看作每个重要的元素都是一个独立的属性。 也就是说，通过递归地应用 这些规则计算每个重要元素的哈希码，并且将每个步骤的值合并。 如果数组没有重要的元素，则使用一个常量，最好不要为 0。如果所有元素都很重要，则使用 Arrays.hashCode 方法。 12. 始终重写 toString 方法 便于调试 13. 谨慎地重写 clone 方法缺陷 Cloneable 接口缺少 clone 方法, 而 Object 的 clone 方法是受保护的, 所以不能保证调用成功 clone 方法的通用规范 ( 非绝对 ) x.clone() != x x.clone().getClass() == x.getClass() x.clone().equals(x) == true 如果一个 final 类有一个不调用 super.clone 的 clone 方法, 那么这个类没有理由实现 Cloneable 接口，因为它不依赖于 Object 的 clone 实现的行为 复制构造方法及其静态工厂变体与 Cloneable/clone 相比有许多优点 不依赖风险很大的语言外的对象创建机制 不要求遵守那些不太明确的惯例 不会与 final 属性的正确使用相冲突 不会抛出不必要的检查异常 不需要类型转换 14. 考虑实现 Comparable 接口 如果你正在编写具有明显自然顺序(如字母顺序，数字顺序或时间顺序)的值类，则应该实现 Comparable 接口: 123public interface Comparable&lt;T&gt; &#123; int compareTo(T t);&#125; compareTo 方法的通用约定 将此对象与指定的对象按照排序进行比较, 返回值可能为负整数, 零或正整数, 因为此对象对应小于，等于或大于指定的对象。 如果指定对象的类型与此对象不能进行比较，则引发 ClassCastException 异常 实现类必须确保所有 x 和 y 都满足 sgn(x.compareTo(y)) == -sgn(y. compareTo(x))。 （这意味着 当且仅当 y.compareTo(x) 抛出异常时， x.compareTo(y) 必须抛出异常。） 实现类还必须确保该关系是可传递的：(x. compareTo(y) &gt; 0 &amp;&amp; y.compareTo(z) &gt; 0) 意味着x.compareTo(z) &gt; 0 。 对于所有的 z，实现类必须确保 x.compareTo(y) == 0 意味着 sgn(x.compareTo(z)) == sgn(y.compareTo(z)) 。 推荐 (x.compareTo(y) == 0) == (x.equals(y)) ，但不是必需的。 一般来说，任何实现了 Comparable 接口的类违反了这个条件都应该清楚地说明这个事实。 推荐的语言是 “注意：这个类有一个自然顺序，与 equals 不一致”。 使用包装类中的静态 compare 方法或 Comparator 接口中的构建方法 12345678910// Comparator based on static compare methodstatic Comparator&lt;Object&gt; hashCodeOrder = new Comparator&lt;&gt;() &#123; public int compare(Object o1, Object o2) &#123; return Integer.compare(o1.hashCode(), o2.hashCode()); &#125;&#125;;// Comparator based on Comparator construction methodstatic Comparator&lt;Object&gt; hashCodeOrder = Comparator.comparingInt(o -&gt; o.hashCode()); 请避免使用 “&lt;” 和 “&gt;” 运算符 15. 使类和成员的可访问性最小化 使用尽可能低的访问级别 类具有公共静态 final 数组属性，或返回这样一个属性的访问器是错误的 12// Potential security hole!public static final Thing[] VALUES = &#123; ... &#125;; 有两种方法可以解决这个问题 你可以使公共数组私有并添加一个公共的不可变列表： 123private static final Thing[] PRIVATE_VALUES = &#123; ... &#125;;public static final List&lt;Thing&gt; VALUES = Collections.unmodifiableList(Arrays.asList(PRIVATE_VALUES)); 可以将数组设置为 private，并添加一个返回私有数组拷贝的公共方法： 1234private static final Thing[] PRIVATE_VALUES = &#123; ... &#125;;public static final Thing[] values() &#123; return PRIVATE_VALUES.clone();&#125; 16. 在公共类中使用访问方法而不是公共属性 如果一个类在其包之外是可访问的，则提供访问方法来保留更 改类内部表示的灵活性 如果一个类是包级私有的，或者是一个私有的内部类，那么暴露它的数据属性就没有什么本质上的错误 17. 最小化可变性要使一个类不可变，请遵循以下五条规则 不要提供修改对象状态的方法（也称为 mutators） 确保这个类不能被继承 把所有属性设置为 final 把所有的属性设置为 private 确保对任何可变组件的互斥访问 ( 确保该类的客户端无法获得对这些对象的引用 ) 不可变对象本质上是线程安全的 不需要也不应该在一个不可变的类上提供一个 clone 方法或拷贝构造方法（copy constructor） 一个不可变的类可以提供静态的工厂来缓存经常被请求的实例 一个类不得允许子类化, 可以设置类为 final , 更灵活的方式是使其所有的构造方法私有或包级私有，并添加公共静态工厂，而不是公共构造方法 如果一个类不能设计为不可变类，那么也要尽可能地限制它的可变性 18. 组合优于继承 与方法调用不同，继承打破了封装 , 父类的实现可能会不断变化, 子类可能会被破坏，即使它的代码没有任何改变 包装类 ( 装饰器模式 ) 有时组合和转发的结合被不精确地地称为委托 (delegation). 从技术上讲，除非包装对象把自身传递给被包装对象，否则不是委托 包装类的缺点 包装类不适合在回调框架（callback frameworks）中使用，其中对象将自我引用传递给其他对象以用于后续调用（“回调”） 编写转发方法有些繁琐 只有在子类真的是父类的子类型的情况下，继承才是合适的 19. 如使用继承则设计，应当文档说明，否则不该使用 这个类必须准确地描述重写这个方法带来的影响 测试为继承而设计的类的唯一方法是编写子类, 经验表明，三个子类通常足以测试一个可继承的类。 这些子类应该由父类作者以外的人编写 构造方法绝不能直接或间接调用可重写的方法 如果你决定在为继承而设计的类中实现 Cloneable 或 Serializable 接口, 那么 clone 和 readObject 都不会直接或间接调用可重写的方法 20. 接口优于抽象类 Java 只允许单一继承 接口是定义混合类型（mixin）的理想选择, 允许构建非层级类型的框架 可以通过提供一个抽象的骨架实现类（abstract skeletal implementation class）来与接口一起使用，将接口和抽象类的优点结合起来。 接口定义了类型，可能提供了一些默认的方法，而骨架实现类在原始接口方法的顶层实现了剩余的非原始接口方法。 继承骨架实现需要大部分的工作来实现一个接口。 这就是模板方法设计模式 21. 为后代设计接口 编写一个默认方法并不总是可能的，它保留了每个可能的实现的所有不变量 在默认方法的情况下，接口的现有实现类可以在没有错误或警告的情况下编译，但在运行时会失败 22. 接口仅用来定义类型 常量接口模式是对接口的糟糕使用 23. 优先使用类层次而不是标签类24. 优先考虑静态成员类 非静态成员类的每个实例都隐含地与其包含的类的宿主实例相关联, 可以调用宿主实例上的方法, 不可能在没有宿主实例的情况下创建非静态成员类的实例 非静态成员类的一个常见用法 1234567891011// Typical use of a nonstatic member classpublic class MySet&lt;E&gt; extends AbstractSet&lt;E&gt; &#123; ... // Bulk of the class omitted @Override public Iterator&lt;E&gt; iterator() &#123; return new MyIterator(); &#125; private class MyIterator implements Iterator&lt;E&gt; &#123; ... &#125;&#125; 如果你声明了一个不需要访问宿主实例的成员类，总是把 static 修饰符放在它的声明中，使它成为一个静态成员类，而不是非静态的成员类 有四种不同的嵌套类，每个都有它的用途。 如果一个嵌套的类需要在一个方法之外可见，或者太长而不能很好地适应一个方法，使用一个成员类。 如果一个成员类的每个实例都需要一个对其宿主实例的引用，使其成为非静态的; 否则，使其静态。 假设这个类属于一个方法内部，如果你只需要从一个地方创建实例，并且存在一个预置类型来说明这个类的特征，那么把它作为一个匿名类; 否则，把它变成局部类。 25. 将源文件限制为单个顶级类26. 不要使用原始类型 如果你使用原始类型，则会丧失泛型的所有安全性和表达上的优势 以下是使用泛型类型的 instanceof 运算符的首选方法 12345// Legitimate use of raw type - instanceof operatorif (o instanceof Set) &#123; // Raw type Set&lt;?&gt; s = (Set&lt;?&gt;) o; // Wildcard type ...&#125; 27. 消除非检查警告 尽可能 地消除每一个未经检查的警告 如果你不能消除警告，但你可以证明引发警告的代码是类型安全的，那么（并且只能这样）用@SuppressWarnings(“unchecked”) 注解来抑制警告 每当使用 @SuppressWarnings(“unchecked”) 注解时，请添加注释，说明为什么是安全的 28. 列表优于数组29. 优先考虑泛型30. 优先使用泛型方法31. 使用限定通配符来增加 API 的灵活性12345// pushAll method without wildcard type - deficient!public void pushAll(Iterable&lt;E&gt; src) &#123; for (E e : src) push(e);&#125; 假设有一个 Stack&lt;Number&gt;，并调用 push(intVal)，其中 intVal 的类型是 Integer, 会得到错误消息 12345// Wildcard type for a parameter that serves as an E producerpublic void pushAll(Iterable&lt;? extends E&gt; src) &#123; for (E e : src) push(e);&#125; 为了获得最大的灵活性，对代表生产者或消费者的输入参数使用通配符类型 producer-extends, consumer-super（PECS） 如果一个参数化类型代表一个 T 生产者，使用 &lt;? extends T&gt; 如果它代表 T 消费者，则使用 &lt;? super T&gt; 改造 max 方法 123public static &lt;T extends Comparable&lt;T&gt;&gt; T max(List&lt;T&gt; list)public static &lt;T extends Comparable&lt;? super T&gt;&gt; T max(List&lt;? extends T&gt; list) max 方法返回 T , 所以 List&lt;? extends T&gt; list Comparable 的 T 消费 T 实例（并生成指示顺序关系的整数）, 所以 &lt;T extends Comparable&lt;? super T&gt;&gt; 32. 合理地结合泛型和可变参数 在 Java 7 中， @SafeVarargs 注解已添加到平台，以允许具有泛型可变参数的方法的作者自动禁止客户端警 告 如果可变参数数组仅用于从调用者向方法传递可变数量的参数, 那么该方法是安全的 33. 优先考虑类型安全的异构容器]]></content>
      <categories>
        <category>programming</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>effective-java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 关键字 transient]]></title>
    <url>%2F2019%2F05%2F31%2FJava-%E5%85%B3%E9%94%AE%E5%AD%97-transient%2F</url>
    <content type="text"><![CDATA[transient 一旦变量被 transient 修饰，变量将不再是对象持久化的一部分，该变量内容在序列化后无法获得访问。 transient 关键字只能修饰变量，而不能修饰方法和类。注意，本地变量是不能被 transient 关键字修饰的。变量如果是用户自定义类变量，则该类需要实现 Serializable 接口。 一个静态变量不管是否被 transient 修饰，均不能被序列化。 参考: https://www.cnblogs.com/lanxuezaipiao/p/3369962.html]]></content>
      <categories>
        <category>programming</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>transient</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字符串池、常量池（运行时常量池、Class常量池）、intern]]></title>
    <url>%2F2019%2F05%2F30%2F%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%B1%A0%E3%80%81%E5%B8%B8%E9%87%8F%E6%B1%A0%EF%BC%88%E8%BF%90%E8%A1%8C%E6%97%B6%E5%B8%B8%E9%87%8F%E6%B1%A0%E3%80%81Class%E5%B8%B8%E9%87%8F%E6%B1%A0%EF%BC%89%E3%80%81intern%2F</url>
    <content type="text"><![CDATA[重点： Class 常量池 是编译期生成的 .class 文件中的常量池 运行时常量池 是 Class 常量池 在运行时的表示形式 字符串常量池 是缓存字符串的，全局共享，它保存的是 String 实例对象的引用 Class 常量池 常量池中主要存放两大类常量：字面量（Literal） 和 符号引用（Symbolic Reference），字面量比较接近于 Java 语言层面的常量概念，如文本字符串 、声明为 final 的常量值等。而符号引用则属于编译原理方面的概念，包括了下面三类常量： 类和接口的全限定名（Fully Qualified Name） 字段的名称和描述符（Descriptor） 方法的名称和描述符 通过 javap 命令可以看到 .class 文件的常量池部分。 运行时常量池 运行时常量池（Runtime Constant Pool）是方法区的一部分，它是 Class 文件中每一个类或接口的常量池表的运行时表示形式。Class 常量池中存放的编译期生成的各种字面量和符号引用，将在类加载后进入方法区的运行时常量池中存放。 方法区与 Java 堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态常量、即时编译器编译后的代码等数据。虽然 Java 虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫 Non-Heap（非堆）。目的应该是与 Java 堆区分开来。 字符串常量池 字符串常量池是用来缓存字符串的。对于需要重复使用的字符串，每次都去 new 一个 String 实例，无疑是在浪费资源，降低效率。所以，JVM 一般会维护一个字符串常量池，它是全局共享的，你可是把它看成是一个 HashSet&lt;String&gt;。需要注意的是，它保存的是堆中字符串实例的引用，并不存储实例本身。 在 JDK 1.6, 常量池是在永久代中的，和 Java 堆是完全分开来的区域 在 JDK 1.7, 常量池在堆中 String.intern() 查找当前字符串常量池是否存在该字符串的引用，如果存在直接返回引用；如果不存在，则在堆中创建该字符串实例，并返回其引用。]]></content>
      <categories>
        <category>programming</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>string</tag>
      </tags>
  </entry>
</search>
